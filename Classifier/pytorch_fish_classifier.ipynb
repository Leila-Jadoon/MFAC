{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements\n",
        "In this secttion we import the required packages for ttraining our classifier."
      ],
      "metadata": {
        "id": "iwv87i7kt60v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kA_BNgzN7Zn",
        "outputId": "47232b02-d774-43d9-c48b-6fd52bc3b804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from skimage.filters import sobel\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_MSIOaV8fAk",
        "outputId": "fbf547fc-62d2-47c4-c5e9-7a27be2b8e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#We mount the google drive. If You are running this notebook locally do nott run this cell. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurations\n",
        "In this section we define the configs for training. "
      ],
      "metadata": {
        "id": "UxpvNK6It6PI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "62mzNuS-89r8"
      },
      "outputs": [],
      "source": [
        "DATASET = \"Allele B Cropped\"\n",
        "X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/x_train_b.npy\"\n",
        "Y_PATH= \"/content/drive/Shareddrives/Exploding Gradients/y_train_b.npy\"\n",
        "\n",
        "\n",
        "BACKBONE = \"resnet50\"\n",
        "MULTI_BACKBONE = True\n",
        "OPTIM = \"Adam\"\n",
        "LR =5e-5\n",
        "SCHEDULER = \"None\"\n",
        "EPOCHS = 40\n",
        "BATCHSIZE = 4\n",
        "AUGMENTATION = \"None\"\n",
        "\n",
        "#The following is a list of hyper parameters to test. All Permuttations will be\n",
        "#tested\n",
        "\n",
        "DROPOUT = [0,0.1,0.2,0.5]\n",
        "WEIGHT_DECAY = [0,1e-3,1e-5]\n",
        "FREEZE = [10,25,40,50,55]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing\n",
        "In this section, we read the dataset as a pre saved numpy array. After reading the datset. we divide it into train-testtt sets. We tthen create a pytorch dataset which we will then turn into a dataloader. "
      ],
      "metadata": {
        "id": "vk8VqKiOuv-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob7GdwdTOjia",
        "outputId": "c180ca80-f65d-42e5-b570-f048a3b00ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Tensor Shape:  (285, 4, 200, 1024, 3)\n",
            "y Tensor Shape:  (285, 1)\n"
          ]
        }
      ],
      "source": [
        "#We read the \n",
        "x = np.load(X_PATH)\n",
        "y = np.load(Y_PATH)\n",
        "print(\"X Tensor Shape: \",x.shape)\n",
        "print(\"y Tensor Shape: \",y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "qQq6Lhq3_E1-",
        "outputId": "ffe15940-11ab-48cc-a2ee-10c6ad2c9fad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# In this part we draw the pie graph for the distribution of the data with 3 labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "percentages= [np.count_nonzero(y==0),\n",
        "              np.count_nonzero(y==1),\n",
        "              np.count_nonzero(y==2)]\n",
        "              \n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(percentages,labels=[\"1\",\"2\",\"3\"], autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2tTpGUZPSgL",
        "outputId": "15a8de84-981e-44c6-d954-2fa7cad345a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x train shape:  torch.Size([228, 4, 3, 200, 1024])\n",
            "x test shape:  torch.Size([57, 4, 3, 200, 1024])\n"
          ]
        }
      ],
      "source": [
        "# We creatte tensors from the numpy arrays\n",
        "tensor_x = torch.Tensor(x) \n",
        "tensor_y = torch.Tensor(y).long()\n",
        "\n",
        "#initially the data is in the form (Batch, 4Images, Width, Height, Channels)\n",
        "#We then  change it to (Batch, 4Images,Channels, Width, Height )\n",
        "tensor_x = torch.swapaxes(tensor_x,2,4)\n",
        "tensor_x = torch.swapaxes(tensor_x,3,4)\n",
        "\n",
        "\n",
        "#We split the data into train and test sets\n",
        "x_train,x_test,y_train,y_test = train_test_split(tensor_x,tensor_y,test_size=0.2,random_state=1)\n",
        "\n",
        "print(\"x train shape: \", x_train.shape)\n",
        "print(\"x test shape: \",x_test.shape)\n",
        "\n",
        "#We craette the dataloader\n",
        "train_ds = TensorDataset(x_train,nn.functional.one_hot(y_train,3)) \n",
        "test_ds = TensorDataset(x_test,nn.functional.one_hot(y_test,3)) \n",
        "#We shuffle the training set and drop the last of the test set\n",
        "#This might not be necessary\n",
        "train_dl = DataLoader(train_ds,BATCHSIZE,shuffle = True)\n",
        "test_dl = DataLoader(test_ds,BATCHSIZE,drop_last = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The following is the distribution of the test set\n",
        "#We want this to be similar to the dataset as a whole\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "percentages= [np.count_nonzero(y_test==0),\n",
        "              np.count_nonzero(y_test==1),\n",
        "              np.count_nonzero(y_test==2)]\n",
        "              \n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(percentages,labels=[\"1\",\"2\",\"3\"], autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "9ien3JWFXYe3",
        "outputId": "7514d83a-9216-4b82-b0f2-089e9ce1b7d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUdb7/8df3TJ80UoAkEAhgiEAGUJSI3bXLisqud7u4q78td++Wn96919/dxt7tu269u7r3bjNus+yu90bx2nsBCwqREgGRTkIgPZmZU76/P2YAKWrKZM6Uz/PxiCBkJu/B4e3Jtx2ltUYIIUR6GG4HEEKIfCKlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaSSlK4QQaeR1O4AQB9XetGICMOWoj4mAH/AlP/xgq6JZX/YCNmAlP6JAO9CW/Nj7lp+3tSxr6UvvqxHi+KR0RdrU3rTCA5wENJAo1KkcLtcaIDSU59E4JokCHrJIU2QA2AqsAdYe/LFlWcvu4TyPEKOl5M4RYqzU3rQiBDQCZyU/FgGFo31erWNO8eyvp2porIPDJfwq8FjLspadKXpuIY4hpStSpvamFaXAGSRLVmu9QCnlT/XX0TruFM/+2ljOR2wAHkx+PNmyrGVwDL+WyDNSumJUam9aMQe4BrhUa92glFJj/TXTULpvFQOeBh4CHmxZ1rI2TV9X5CgpXTFsyQmvD2vHuVYZxrx0f/00l+7RNgG3A00ty1p2uJRBZDEpXTEktTetCAJLtHaWgbpIKeXaJKzLpXuQAzwO3Ab8vWVZy4C7cUS2kNIVb6v2phUKOFNrfQ3oDyhlFLmdCTKmdN+qB7gbuK1lWcszbocRmU1KVxwjuergeu04NyrDmOp2nqNlYOm+1Vrgu8BdLctaHLfDiMwjpSsOqb1pRZG2zM9hGDcqw1Pmdp63k+Gle9Am4HvAH1qWtZhuhxGZQ0pXUHvTihJtxf8Fw/N5ZXhGvY52rGVJ6R60A/gh8BtZeiZASjev1d60IuSY0S8pj+9L2VC2B2VZ6R7UDvwY+A+ZdMtvUrp5qPamFV4nPviPyvB+XXl9GTuM8HaytHQP2gHc2LKs5W63gwh3SOnmmSn/9+7FeLz/aXj9k9zOMlJZXroHPQp8rmVZy4ZUP7FSKgg8BQRInK/yV63111P9dcTISOnmierrbyn3hIqbPAWli93OMlo5UroAJvBz4Bsty1p6U/WkyV2BBVrrPqWUD3gG+ILWemWqvoYYuVx444p3UX39rcu8xRPfyIXCzTE+4EagNdIU+UiqnlQnHDzK8uCRmHJ1lSHkSjeHVX/iF+ONYNFfvMUV57udJZVy6Er3aI8B17Qsa9k12idSSnmAl4ETgF9qrf91tM8pUiMX37gCqL7+1o97S6u25Frh5rj3AGsiTZErRvtEWmtbaz0fmAwsVEo1jDqdSAm50s0x1df9coIRLLzTW1RxrttZxkoOX+m+1a3ADS3LWqKjfSKl1NeAAa31zaOPJUYr19+4eaX6+ls+7h1XtSWXCzePfAZ4MdIUGfYVqlJqvFJqXPLnIeBCYGOK84kRktLNAeG6xkD1x39+u698yu8MXyBrNjmId9VAonj/cZiPqwIeV0qtBV4EHtZa35fydGJEZHghyxWdcvmkonmX3OcfXzvf7SzpkifDC0f7O4lJtn63g4jRybc3bk4pPffaBSULl67Kp8LNY0uBZyJNkcluBxGjI6WbhcJ1jar8ks9dXTjvkse8xROydmeZGLb5wAuRpsgpbgcRIyelm2XCdY1Gwaxzvlww57w/ekJFxW7nEWlXBTwVaYpc7nYQMTJSulkkXNcYLDpp8W3hE8/6huELpPwuuyJrhIB7Ik2R690OIoZPSjdLFMw6u7Rk0QceDk1f8DFlGPLfTXiAX0eaIl9zO4gYHvnLmwXCdY1TSk67+qlAdf2ZbmcRGecbkabID90OIYZOSjfDhesaG4oXvu9e/8Tpso1TvJ1/jjRFlrsdQgyNlG4GC9c1nlh00uLbgjVz5rqdRWS8r0eaIv/idgjx7qR0U0Qp9TulVLtS6rVUPF+4rnFaQcP5vwlNX7AgFc8n8sL3I02Rf3I7hHhnUrqpcxtwSSqeKFzXOClcf+Z/hWeefkYqnk/klZ9HmiIfdzuEeHtSuimitX4KODDa5wnXNU4IzVh4S8Gcc9+TuAGAEMOigN9EmiIfdDuIOD4p3QwSrmssDU6d97PCuRcuVkqWhYkRM4A/RJoil7odRBxL/mJniHBdY1Fg0qwfFp102fuU4fG4nUdkPS/wl0hTpM7tIOJIUroZIFzXGPKV1ywvWrDkI8rj87mdR+SMEhI71+S4zwwipeuycF2jX3kDnys+5YprDF8g6HYekXPmAE2RpohMEGQIKd0UUUr9BXgeqFdK7VRKXfdujwnXNSrgI8ULr/qEp7CsYsxDiny1FPg3t0OIBCndFNFaf0hrXaW19mmtJ2utfzuEh50erj/jukDVzPoxDyjy3b9HmiKXuR1CSOm6JlzXOMVXMfWGgtnnLnQ7i8gLBvCnSFPkBLeD5DspXReE6xoLlDfwheJTrzpHGR6ZOBPpMg74W6QpIseCukhKN82S47jXFp2y5GJPuLjc7Twi78wFvuJ2iHwmpZt+ZwWnzL0yUH3iHLeDiLz1/yJNEbmvnkukdNMoXNdYaYSKry+cf8lpssVXuMgL3BZpisjQlgukdNMkXNfoAz5Z0vi+0w1fUBarC7fNQ5aRuUJKN30WB6ZEzvCV18xwO4gQSV+ONEXkrOY0k9JNg3BdYy1KXVHYcL6Mo4lM4iMxzOB1O0g+kdIdY+G6RgP4UMGc99R6QsUT3M4jxFFOAm50O0Q+kdIde/NUINwQmnHqqW4HEeJt/FukKSLb0NNEvq0YQ+G6xgDw0aKTFtcZXn+B23nE0MT3x9n1611YPRYApeeWUnFRBXvv2EvPqz0or8I/wc/k6ybjKTj2FM6OBzvofLITFAQnB5l03SQMv8GOX+0gujNK0fwiKt9fCUB7czvBSUGKFxSn9TUepZjE2t0vuhkiX8iV7tg611tSOSVQVX+y20HE0CmPovKDldR9p47pX53OgUcPEN0VpaChgLpv11H3rToClQH2rdh3zGPNTpP9D+9nxvIZ1H27Du1ould1E90RxfAb1H2rjsGtg9gDNmaXyeCWQbcL96DPRJoi09wOkQ+kdMdIuK5xHLC06OTFDcow5FDyLOIb5yNUGwLAE/IQqA5gdVoUNRShPIn11eEZYcwD5nEfrx2NE3fQtkbHNd5SL3hI/Jqj0ZYGA9r/3s6EqzJmmN8PfMvtEPlASnfsLAlMnl3tK5t0ottBxMjF98WJbosSmhE64tc7n+qkaG7RMZ/vK/VRcUkFr9/4Ohu/uBEjZFDUUESwOoi3yMuWr2+heH4x8bY4WutD5Z4hPhRpipzkdohcJ2O6YyBc11gDvKcwcqHcPj2L2VGb7b/YTuWHK/GEDn+z0t7cDh4oWVRy7GP6bXpf6WXmD2fiCXvY/svtdD3XxbjTx1H1kapDn7ftJ9uovraa9uZ2ojuiFM4ppOzcsrS8rneggO8DF7kdJJfJlW6KJQ+0+WD4xDMne8IlVe/6AJGRtKXZ8YsdjFs0jpJTDpdr59Od9K7ppeZTNRxvK3ffuj58FT68xV6UV1F8SjEDmweO+Jye1T0Ea4M4MYf4vjhTPjuFnpd6cGLOmL+uIbgw0hS5wO0QuUxKN/UiQENo2oLZbgcRI6O1ZtfvdhGoClBxyeGVVL1re+n43w6mfmEqRuD4f3V85T4GtwzixBy01vSv7ydQFTj83JZm/0P7GX/ZeJz44ZI9NNabGb7udoBcprTOmP/QWS+5EeLbgcmzp5U0vv8DbufJVVrHneLZXxuzC4b+1/vZ+p2tBCYHDl3NTnz/RPb8aQ+O5eAtSIzKhWaEmHTtJMxOk12/30XtDbUAtN3TRveqbpRHEZwSZNInJmH4EnE7HuzAE/ZQelYpWmt2/mon0V1RiuYWUfkPlWP1kkZiQcuyltVuh8hFUropFK5rPBH419LzPrHAVzY54naeXDXWpSsAaGpZ1nKt2yFykbxxU+tCT0EZ3nHVMrQgst0HI02RjFnPlktk9UKKhOsaxwMnF8w5tzad63Ktnn10rPgxTn8XoCicfzHFp1xBvP0N9j/4S3Q8irdkAhWXfwkjED7m8T0v/jd9ax4CBb7xtVRc9kWU18++e3+IuW8boRmnUnrOMgC6nrsDf8VUwjMXpevlCfcEgE8B33Q7SK6RK93UOQPD4/grT0jvMjHDQ+l511F9/a1UfuxmelevIN6xnf3/+x+UnnMt1df9kvDMRfSs+tsxD7V6O+h5+V4ql/2E6utuAcehf8NTxNu3YngDVH/iF8T3bMKJ9WP1HSC+u1UKN798Rg46Tz0p3RRInrFwUXjm6aWGL3jsivkx5C0sI1CZuMGrEQjjK6/B7t2PeWAXgZoGAIK1JzHw+nPHfwLHRltxtGOjrRiewjKU4cWxYmjtoB0LlEH303+k5MyPpOtlicxQBVztdohcI6WbGvOAUHDqPFfPWLC624i3vUGguh5/xRQGN60EYGDjM1i9Hcd8vreoguKFV7Hr1o+z8xcfQwXChKadjK+iBk+ohD23fYHwCQuxOvegtT5U7iKvfN7tALlGSneUkpsh3uufMN3jLSyb6lYOJz7Ivnu+Q9n5/wcjEKb8si/Q+8r97LntCzjxQZRx7PC9He1jYNMqJn36t0z+7O1oM0bfuscBKLvgk1R//D8oXriUrqf/wLizPkr3c3ey77+/R++rD6T75Qn3NEaaInIT1RSS0h29aUBN+MQzXVuxoG2Lffd8h4LZ5xKuPx0AX3kNEz/wTaqu/RkFs8/BW3rsGtDom6/iLZmIJ1yC8ngJz1xEbNeGIz5nYNNK/JUnoM0oZtcexl95EwOtz+KY0bS8NpER3u92gFwipTt652N44r6ySQ1ufHGtNfv/92f4ymsoXnjVoV+3+7uSv+/Q/dwdFM2/9JjHeovHE9/dimNG0VoT3bYGX3nN4ee2LXpe+h+KG9+HtmIktuYD2gHbGtPXJTKKjOumkCwZG4Xk8Y2nBafM9SqPL/CuDxgDsV3r6V/3OL7xtez+/ecAKD37GszO3fSuXpHIOfN0CiIXAmD17mf/Az9n4tXfIFBdT7j+DPbc9kWUYeCfOIOieZcceu7e1SsobDgfwxfEN34a2oqx+7efJTTjFAy5oXE+mRNpipzYsqxlo9tBcoHsSBuFcF3jacCnShZ9oC5QXX+G23nyhexIc8VXW5a1yHm7KSBv3NFpBPq9ZZNmuh1EiDEm47opIqU7QuG6Rj/Q4C2tsj3BwvFu5xFijM2LNEVkzWAKSOmO3HTACE6ZN8PtIEKkiVztpoCU7sg1AI5//NQ6t4MIkSZXuB0gF0jpjkByQ8Rpyhvo9hRVyB1URb5YEGmKZNRN3bKRlO7ITATKgrXzK5XhkQNBRL7wAQvdDpHtpHRHZiaAv/IEGVoQ+eZMtwNkOyndkWkE+nzjqqR0Rb6R9eijJKU7TOG6xhBQ7ykebxqBcKnbeYRIs0WRpoj0xijIH97wzQCUf8L0iW4HEcIF4wA5dWwUpHSHbxqgfeOqMurWrUKkkYzrjoKU7vDNBPo9ReVSuiJfybjuKEjpDkNyfe50oM8THifDCyJfzXM7QDaT0h2eEiCkfEGtAuFyt8MI4ZITZDJt5OQPbngqAe0rrylTSim3wwjhkiBQ63aIbCWlOzyVgOEdVylXuSLf1bsdIFvJnSOGpwaIeYveck8bIXKY1hpl6r5w1OovH7TsadrqP81n75sTi092O1u2ktIdnhpgwAiPkytdkTO01tqIOb2FUbt/QsyMTzNN6i3TG3HMYANWcYmiEDh8f6ZB6oBVrgXOclK6Q5RcuTAJ6PIEi6R0RVbRjra9UbunOGYPVMVMa5ppMssyfXO1GZ6FVRxUqhgoPuJB7zxrMXUM4+Y0Kd2hCwFhYJ/yB4vf7ZOFSDdt6bg/ZveOi1qDk+KmPcM0mW2bgbnaLDgBu9CrVClw5NZ1degfw1U76sB5Skp36MoBG0B5vK7c+VcITGcwGLV7y2JWrCZu2nWm6WmwzcBczMIapcMk3qdHGnmxvhO50h0hKd2hK+bgO9fw+N2NInKV1hoj7vSFY3Z/RcyKTY2bTr1leufYZnAeZlGFIkTiu67D3Fm8OM6Vr5oDpHSHzg+gfAGvUoYstRMjph3teOJOb2HU6p8Ys8zaxPiqt8ExQxHM4kKljpy4AreK9Z14WF4SZHl31O0g2UZKd+gCgDICBXKVK96VtrXli9k9xTF7sCpmWtMT46u+iGMWnIhVFFCqhMQOx8PGZhhgLBUAUrrDJKU7dH5AKX9YSlcAoC0nFojavaUxO5qcuFJzbNM/T5uF07ELDaXKjnlQ9hXrOykA9rsdIttI6Q5dENCGPySlm0/izkAwZveVx6zYlJhp11mmp8Exg3Mds3CSoUMkvgM6Um4V6zspcDtANpLSHboCwDb8QSndHKK11kbc6SuI2v3jY1ZsqmnqejMxvjpPm0WlBmESSwWPJKP6cPS4sxgSKd2hCwO28gWP/QsoMpp2tOOJOT1FMWugMmaZ0+ImJybGV0MN2iwOG6oIKDriQYo8uVgdFbnSHQEp3aELAbbyBuRKNwNpW5u+qN1TErOi1XHLmhE31Szb9M3VZkE9VpFPqXEcvcxJAXJY3GhI6Y6AlO7QJUrXF5CNES4ynLg+w3xyr2d3r1lnmWqObQbmOmbhdMMpIH0bA0SClO4ISOkOXWJ4weOTP7M0Czp95kXxx9su1k8VnF24q6goTDWxt3yCjK+6xXI7QDaSAhm6EGBrK266HSQflNkdA5ebD3Zc7n3BMze8r9IfRo4SzDy9bgfIRlK6Q+cHHCc+MOh2kFxVY27tXmo92HVZ4NVQXbhnvKGY4nYm8Y763A6QjaR0hy4KlDjRfindFJobW9NxlX6478Lg+pLJRdFSjt6lJTKZXOmOgJTu0PUA5U60V0p3FAxtOqfHVrYvVY/Hzg1tLi8rsSqACrdziRGRK90RkNIdul7A5wz2yhttmALOgHlR/LH2K41nnNPCOycUhJxKtzOJlJAr3RGQ0h26bsBr93cOaq2RmwG/s1KrY+C91kMdS7yrPPNC+yr9YSa5nUmknFyAjICU7tD1AF60o3HsGHKQ+TEmm9t6ltoPdF7mfyVYV9AzwSMTYbnMZHl37N0/TRxNSnfo+kmuste2GZW7RyRE4ms6rnIe6b8ouK5oclG0jKPvsyVylQwtjJCU7tANAg6Ats1BCOXlLLvStj4j9lzbVeqJ2LmhTeXlxTIRlqd2uR0gW0npDt0goAG0Fc+rFQwBZ8C8MP54W2IibMeEQpkIE7DJ7QDZSkp36A4VbT6UbqnVMbDYerhjiXeVZ36oXXaEiaNtdjtAtpLSHbpDResM9nYedSPrnDDJ3J6cCFsdnCkTYeKdyZXuCEnpDt0gyYk0q7ttb6C63uU4qTEn3rL/Kufh3ouD64prigZlIkwMlVzpjpCU7tD1ATbgiXds35utZ9opbetF8ZXtS3ksem5oc1lFsVnO8Y5EFOKdyZXuCEnpDtHAplVOuK5xO1Bm7tu6Xzu2qQyPz+1cQxFwBq0L4o+3XWk87SwK7xhfGHImup1JZLUBYLfbIbKVlO7wvA5cgNY9zmBvu6dgXMbushpn7R9cbD7cscS30pgf2jcxENYZm1VknS0s79Zuh8hWUrrD8ybJPzO7v3NvppVutbm95yr7oc7FvpcD9QXdEz2KGrcziZwkQwujIKU7PHtJbpCwevbt9U+Y5nIcmB1/LTERFnitaErRYDkyESbG3gtuB8hmUrrD00bi5jDKPLBzDyxMewClbX1afGXbUh6PnRfcJBNhwg1Pux0gm0npDsPAplWxcF1jGxCO793cprXWKg3HjQWcQev8+BNtVxpP26eHtsuOMOEarXVUKfWS2zmymZTu8G0CTtVmtF/HBvarYMGYnDtQYnUOLrYe2neFZ6VnfrhdJsJERlBKrWJ5d9ztHNlMSnf4NgFnAdgDXXuNFJZutbmj50r7wc7FvtWBE8NdEz2G7AgTGUeGFkZJSnf4Dk+mde3d4Sub1DCaJ5sVX3cgORFWOLVoQCbCRKaT0h0lKd3h20tyO3B0e8um0PQFlw7r0drWp8VXtS/lseh5wU2l44vNMqBsDHIKkVJaa1sp9bzbObKdlO7w9QEdQNjcv73TifXvNwIF77h6wO8MWufHn2y70njKPj20fUKR7AgTWUgp9SrLu+Xw8lGS0h2mgU2rdLiucRVwKTBgdu7eFKisO6Z0i63O6GLr4X1XeJ43TpKJMJEbnnA7QC6Q0h2Z14DLAOJ7Nm0KVNadBlBl7uq90n7gwGLfy4FZiYkw2REmcsnf3A6QC6R0R2YriRPHvNFta7Z9pf7NLVeFVo+rTUyEFbmcTYiUc7TeYSi10u0cucBwO0A2Gti0Kg6s9WBXltr76xcMriyuDQ3IrjCRswyl7pJDblJDSneEqtjfGlFbF52gdlW/ujsmBzqLXHen2wFyhZTuCM0xtr1QTs8LxWrw0ZXbY4+ZtjbdziTEWHC0fpPl3S+6nSNXSOmOUHOr2a0UG4HSvjjWm13O625nEmIsGErd4XaGXCKlOzpPAYUAT26zX3U5ixBjRYYWUkhKd3Q2JH9U97Zam3ti+oCraYRIMUfrTSzvlguKFJLSHYXmVrMLWAtUaGDVTluOvBM5xVDqd25nyDVSuqP3MBAGuHOd+YrlyISayA2O1oPAf7qdI9dI6Y7eRmA/UNDer6MbO5wWtwMJkQqWw+0s7+50O0eukdIdpeZW0wbuJ3nLnOZWS5bWiKyntXb8HnWz2zlykZRuarxI4oxd78qd9t69fc4OtwMJMRpxmwdY3i2bfsaAlG4KNLeavSSWj00EeGqbLVe7IqsFvOp7bmfIVVK6qfMk4AO4a525ri+ue1zOI8SIxCy9luXdcoeIMSKlmzrbgS1AadzGeeQN6wmX8wgxIj4PcpU7hqR0U6S51dTACmAcwO1rzFe7orrD3VRCDE/c1jsMpe52O0cuk9JNrTUkrnhLLQd9b6v5qNuBhBgOBTewvNtyO0cuk9JNoeTysTuAEoC711sb2/qcne6mEmJoemJ6je+bPX91O0euk9JNvfUkNkyMB7h7vfmIu3GEeHdaaxR8yu0c+UBKN8WSY7t3AwWAemiLvW17tyPrHUVG64pyb9F3e1a5nSMfSOmOgeZWcwvwEsl1u39caz6qtdzpRGQmy9FmgZ9/dDtHvpDSHTv3AAHAWLnT3rt+nyPH44mM1B3Vt/i/2SNzD2kipTtGmlvNXSQ2TFQB/GRl/MEBU/e5m0qII0Ut3VUeNr7sdo58IqU7tu4FNBBo79fRu9eZ97kdSIi36ovrf2F5d7/bOfKJlO4Yam419wN/AaoB/rbBan19v/2au6myn+1oTvrPPt775wEAzvp9P/N/1cf8X/VR/aNerrxj4G0f2xPTTP5xL/90/yAAMUtzyR/7abilj1tejB/6vE/eO8jqPfbYvhCXtfc7z1X8oPfXbufIN1K6Y+8pErf1mQhw83Px+wdNLVcWo/CzVXFmVRx+6z798QJe/XQhr366kEU1HpbO8r7tY7/6WIyzp3oO/fuDWyzOnOJl7WcK+MPaxPnza/ba2A6cXOV5u6fJelFLD+7r1x9yO0c+ktIdY8kNE7eROAzHv7dPD96z0bzf3VTZa2ePw4pNFtef7D/m93pimse2Wlx5ou+4j315t01bv8NFMw6Xss+AAVNj2nBwgclXH4/xzfcExiR/ptja6Xxlzi19293OkY+kdNOgudXcS2KnWjXAHa9Z67cccDa886PE8XzxgSg/uCCIoY79vf/eaHL+NC/FgWN/09GaGx+KcvNFwSN+/cIZXt7scjjtt/18vtFPc6vJyVUG1UW5+1djR7fz3Kxf9v3Y7Rz5KnffWZnncWAzMAHgR8/HVkQtPehupOxy3+smEwoUC6qP/23/X14z+VDD8a9yb3nR5LI6L5OLj3zLew3Fn98X5pVPFXL1bC8/XRnnxkUBbngwyvvvGqC5NbduedcT0z2bDzhL3c6Rz6R00yQ5zPA7Emt3/Tt7dP8f1ph/c2TXxJA9u92mudWi9qe9fPCvgzy21eKjf0/8f6tjwOGFXQ6LZx5/PPf5nRa/eCFO7U97+eeHYty+xuSmR6JHfM4tL8a5Zp6PlTttSgKKO98f4kfPx4/7fNnI0Vq/1m5/8rym/ja3s+Szt59xECnX3GruXlLvuxv4MPDGva9bW2aUGY++Z5r3ArezZYPvXhDkuxckhgeeeNPi5ufi/HFpCIC/rrd470wvQe9xxh2APy0NH/r5ba/GeWm3zfcuODzU0DmouW+TxYMfDXNvq4WhQCkYNHPn/4kb9jl3nP7b/jvdzpHv5Eo3/R4B1gKTAH66Mv7spv32OncjZb87jjO08NJum+ubhzaC8+9PxvjyWQEMpbj4BC9Pb7eI3NrPx+YeO2GXjbZ2Ouvve91a5nYOAUq+u02/JfW+YuDrJFY0HCgO4Pv5pcHrykLGRJejiRzU1ud03L7GPPVLD0ffdDuLkCtdVzS3mj3Az4EwEOqJYX7/mfidMZlYEynWF9eDd683PySFmzmkdF3S3GpuA/6LxNkMng0dTmeTTKyJFDJtbd+zwfzSP90flTOdM4iUrrteJHE+wxSA+163tjzxpi23+BGjprXmgc3WrXevt25xO4s4kpSui5IHnt/DURNrq/fYcpi0GJVnd9gP/nq1eUPyPSYyiJSuy5pbTQv4NdANVAAsfyL2wNo2+2VXg4msta7dfu0Hz8avbm41c2tnR46Q0s0AyYm1nwAKKAX46mOx+9bvs9e4Gkxkndfa7U03Pxe/pLnV7HU7izg+Kd0MkTz0/PuAHxinga88FvsfWcMrhurVvfaWf38ydsXvX43vcjuLeHtSuhmkudXcDvwQCAHFloO+6ZHY39/odDa6HE1kuNV77M3feir2wbvWmV9yTK8AAAg8SURBVHKQUoaT0s0wza3mG8DNQDFQaDo4Nz0S/eu2LrmjsDi+l3bbr3/zydjVf11vvuR2FvHupHQzUHOruYlE8ZYCBVEL+18fid75ZpfzusvRRIZ5cZfd+u2nYlffs9GUG59mCdkGnMGW1PsagBuBfcCA10B99ezAJSdVeRa6HE1kgJU7rQ3ffyb+D/dsNOUWUFlESjfDLan3zQO+AHQBPQCfPdXfeOEMz8WGUsc/UkvkNK01D22xX/7li/Flza2mTLRmGSndLLCk3jcT+CLgAB0A75vlrf9wxPc+n0cd/9RukZPito7/7hXzqfs3WZ9vbpVJs2wkpZslltT7KoEbgHHAboAzajxVn2v0fzjsU4WuhhNp0RXVPT94NrbitXZneXOrKeP7WUpKN4skj4T8LDAT2A7oE8qM4q+cHfhIWUhNcDedGEtbO53d33469pf2fv2D5laz3e08YuSkdLPMknpfAFgGnEmieK3SIP4vnx24fGa5p8HddGIsPLvd2vCj5+O3Wg6/bW41B9zOI0ZHSjcLLan3GcASYCmJoYYowDXzfHOX1Hsv83tUbt8/PE9YjrbuWme+cMdr1g+Ae5tbTcftTGL0pHSz1JJ6nwIWAZ8gUbodACdWGONuWORfWllo1LiZT4zOnl5n709Wxp/e2OF8v7nVlMOPcoiUbpZbUu+rBj5F4kzenYDtNVA3LPKffXqN5xxZVpZdbEfbD26xVv/6ZfMZW/PT5NZwkUOkdHPAknqfn8Rww+XAfpLrec+Z6pn8fxb4lxYHVKmb+cTQtPc7bT95Pr5q3T7nfuDPclJYbpLSzSFL6n0nAp8GCoFdgC4N4v/n0wMXN0wwTpaL3sxkO9p5dKv9yq9eir9gOfwaeFUOH89dUro5Zkm9rwj4KInx3kOTbGdP9UxaNs+3eHyBUeVmPnGkff1O289WxV9c23bo6rbb7UxibEnp5qDkJNvpwDUkDjXaDWhDoa4/2bfgwune9wS8KuRqyDzXH9c9/9NqvnLXOmu9o/kd8LJc3eYHKd0ctqTeVwZcTaKAu4BOgAkFKviZU/znzK80TvUYyuNmxnwTt3XsiTftF36zOr4tarEK+GNzq9nldi6RPlK6OS551VtPYkNFJdBGcshh9nij9PqT/RecUGbMdjFiXrAdba/e47xy60vxzR0DegfwZ2CNXN3mHyndPLGk3ucDziJx5esH9gAWJM5wWDrLe8aMMmO2LDFLLa01rfud9b96Kb7+jU7dBtwFPC83jcxfUrp5JjnRdjFwKWCTuPK1AWZVGKUfivgWNUww5nsNOb1sNCxHWxs7nNfufM18Y02bcwBoBh6VbbxCSjdPLan3TQQWA2cAmkT5mgCVhSp0zTzfwlOqPQuDXhV2MWbW6Y/rnhd22S/9ca25Y9+A1sDjJLbwdrqdTWQGKd08t6TeVw6cB1wEeEncpWIQoNCP96NzfSedOcV7WnFAlbkYM+Pt7XN2PPqG9fLfNlidloMGVgErknd5FuIQKV0BwJJ6XyGJtb1LgCLgAMmdbQo4t9ZTc940b6S+3Jgd8qkC95Jmjrit45sPOBv+vsFa98IuOw4MAA8Azza3mgdcjicylJSuOEJyS/HJwJXARCBG4jAdC8BroC49wTv9jCmeyAllxon5dqJZzNLRLZ3OxpU77Y0PbLZ6oxY+Emde3EtiNULM5Ygiw0npiuNKHh85i8Qa34Ukhh4GSJzt4ACEfXgvn+mdedpkb8PUcarOayiva4HH0KCp+zcfcDY+v9Pe8MBmq9NyOHinjpeAh4HNsvRLDJWUrnhXS+p9IRIFfBYwj8SIQw+JDRcaIOjFc0aNZ9JJVZ7a6aVGbWWhmpytKyAcrfX+Ab13W7d+8/kd1qZHt9rdjqaIxGt9HXgWWCdDCGIkpHTFsCSXnM0BziVx2yBIbLboAuIHPy/oxXN6jaf6pEpP7Ywyo7ayUNVkagnHbR1r79e7d3Q7O19rd7Y9vd3a1RWlEDi4cmM98AywQc5GEKMlpStGLLnyYSYQSX4cnGA7poT9HoyGCUb5jFKjfHKxUTGxUJWXh1TFuKAqT9c5EFFLD/bGdFd3jM4Dg7prW5ezd02bvbulzenSUELidDad/GgBngM2yhGLIpWkdEVKJLcbTwRqSRTwXBJXiorEJNxA8iN+9GOrClV41nijvHacUVESUAUhH4GQVwWDXgIBrwoGPIkf/R4Cfg9BrdG2xrQcLNvBshxtWQ6mrbEsB8u0tdkTo7djQHft7XO6dvTork377c7OKPFkngKgmMQ4tUNic0grsBbYCuyUCTExVqR0xZhIlvAEEiVcC0wFJpO4mnRIlJ8isSZ4kEQZW8nfGw0FBIBg8sPP4avXg1ucdwIbgC0kzh1ua2417VF+XSGGREpXpFVyUq4CKCdRylOBGhLf3odJXH1qji3fg4X5dm9Y9ZYfO0nssNtDolQ7OXzKWo/c4FG4SUpXZIzk1bGXw1eqb71iDZAoXCv5Yb7Nz/ubW00r7eGFGCIpXSGOQylVA9xOYpxaA/+ltf6Zu6lELpDSFeI4lFJVQJXWerVSqgh4GbhSa73e5WgiyxluBxAiE2mt92itVyd/3kti4m2Su6lELpDSFeJdKKVqgZNInBwmxKhI6QrxDpRShcDfgC9qrXvcziOyn5SuEG9DKeUjUbh/0lr/3e08IjfIRJoQx6ES94prAg5orb/odh6RO6R0hTgOpdSZwNMkzmA4uJni37TW97uXSuQCKV0hhEgjGdMVQog0ktIVQog0ktIVQog0ktIVQog0ktIVQog0ktIVQog0ktIVQog0ktIVQog0+v/T3Ul1lQiNqAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier\n",
        "The following section includes the code for the classifier.\n",
        "We have support for muliple backbones. \n"
      ],
      "metadata": {
        "id": "6qxyFVTvyJ9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Og6k2dvk8Wni"
      },
      "outputs": [],
      "source": [
        "#This function takes in a model and replaces inplace relu layers to an independent relu layer\n",
        "def reluToInplaceFalse(model):\n",
        "  for name, child in model.named_children():\n",
        "    if isinstance(child, nn.ReLU):\n",
        "      setattr(child, 'inplace', False)\n",
        "    else:\n",
        "      reluToInplaceFalse(child)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VHTLP0NoRnLm"
      },
      "outputs": [],
      "source": [
        "#This is the classifier Class.\n",
        "from torchvision.transforms.transforms import RandomRotation, RandomAdjustSharpness, RandomGrayscale\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, backbone='resnet', multi_backbone = False, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False):\n",
        "    super().__init__()\n",
        "    self.multi_backbone = multi_backbone # Bool: Indicates if we use multibackbone\n",
        "\n",
        "    #In the following secttion we download the appropriatee prettrained model\n",
        "    if backbone == \"vgg19\":\n",
        "      backbone = torchvision.models.vgg19(pretrained=True)\n",
        "      self.out_channels = 25088\n",
        "      \n",
        "    elif backbone == \"resnet18\":\n",
        "      backbone = torchvision.models.resnet18(pretrained=True)\n",
        "      self.out_channels = 512\n",
        "\n",
        "    elif backbone == \"resnet50\":\n",
        "      backbone = torchvision.models.resnet50(pretrained=True)\n",
        "      self.out_channels = 2048\n",
        "\n",
        "    elif backbone == \"Efficientnet b1\":\n",
        "      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n",
        "      self.out_channels = 1280\n",
        "\n",
        "    elif backbone == \"Efficientnet b3\":\n",
        "      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "      self.out_channels = 1536\n",
        "\n",
        "    elif backbone == \"Efficientnet b5\":\n",
        "      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n",
        "      self.out_channels = 2048\n",
        "\n",
        "    elif backbone == \"Efficientnet b7\":\n",
        "      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n",
        "      self.out_channels = 2560\n",
        "      \n",
        "    # Disabling inplace ReLu becasuse GradCam doesn't work it enabled\n",
        "    reluToInplaceFalse(backbone)\n",
        "     \n",
        "    modules = list(backbone.children())[:-1]\n",
        "    self.do_augmentation = do_augmentation\n",
        "\n",
        "    if self.do_augmentation: #If augmentation is enabled we  init tthe layer\n",
        "      self.augmentation = nn.Sequential(transforms.RandomHorizontalFlip(0.2),\n",
        "                                        # transforms.RandomVerticalFlip(0.2),\n",
        "                                        # transforms.RandomPerspective(0.2),\n",
        "                                        RandomRotation(20),\n",
        "                                        # RandomAdjustSharpness(,),\n",
        "                                        # RandomGrayscale(),\n",
        "                                        # transforms.RandomAutocontrast()\n",
        "      )\n",
        "\n",
        "    if self.multi_backbone: #We create the backbones and put them on the device\n",
        "      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "\n",
        "    else:\n",
        "      self.backbone =  nn.Sequential(*modules).to(device)\n",
        "\n",
        "\n",
        "     \n",
        "    #This is the first fully connected layer\n",
        "    #The output of the backbone goes through this layer before being concatnenated\n",
        "    self.fc1 = nn.Sequential(nn.Dropout(dropout_rate),\n",
        "                              nn.Linear(self.out_channels, 256), \n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(dropout_rate)) #TODO: Experiment with BN and Dropout\n",
        "    #This is the final classification layer\n",
        "    self.fc = nn.Sequential(nn.Linear(1024, 256),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout_rate),\n",
        "                            nn.Linear(256, 3))                \n",
        "     \n",
        "  def forward(self, x, is_training = True): \n",
        "    if self.do_augmentation and is_training:\n",
        "      imgs = [self.augmentation(x[:,i]) for i in range(4)] #list of 4 images\n",
        "    else:\n",
        "      imgs = [x[:,i] for i in range(4)] #list of 4 images\n",
        "\n",
        "    if self.multi_backbone:\n",
        "      encodings = [self.fc1(self.backbone1(imgs[0]).squeeze()), \n",
        "                   self.fc1(self.backbone2(imgs[1]).squeeze()),\n",
        "                   self.fc1(self.backbone3(imgs[2]).squeeze()),\n",
        "                   self.fc1(self.backbone4(imgs[3]).squeeze())]\n",
        "    else:\n",
        "      encodings = [self.fc1(self.backbone(img).squeeze()) for img in imgs]\n",
        "\n",
        "    return self.fc(torch.cat(encodings,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vfYTWTfpHT9E"
      },
      "outputs": [],
      "source": [
        "#This functtion is responsibl for freezing the early layers of the backbone(s)\n",
        "def freeze_layers(model,layers = 5):\n",
        "  if model.multi_backbone:\n",
        "    for i,param in enumerate(model.backbone1.parameters()):\n",
        "      pass\n",
        "    print(\"This Bacbone has {} parameter layers.\".format(i))\n",
        "    print(\"Freezing {} Layers.\".format(layers))\n",
        "    for i,param in enumerate(model.backbone1.parameters()):\n",
        "      if i < layers:\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for i,param in enumerate(model.backbone2.parameters()):\n",
        "      if i < layers:\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for i,param in enumerate(model.backbone3.parameters()):\n",
        "      if i < layers:\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for i,param in enumerate(model.backbone4.parameters()):\n",
        "      if i < layers:\n",
        "        param.requires_grad = False\n",
        "  else:\n",
        "     for i,param in enumerate(model.backbone.parameters()):\n",
        "      if i < layers:\n",
        "        param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "This secttion containes the code for training the model."
      ],
      "metadata": {
        "id": "7LPQIAFx5jfC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "H6wrm2SBPmc9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dl, test_dl, optimizer, evaluation_steps = 20, epochs =1, savefile='model.pt'):\n",
        "  writer = SummaryWriter()\n",
        "\n",
        "  EPOCHS = epochs\n",
        "  DATA_SIZE =  {\"train\":len(train_dl.dataset),\"eval\":len(test_dl.dataset)}\n",
        "\n",
        "  best_eval_acc = 0\n",
        "  best_eval_acc_step = 0\n",
        "  \n",
        "  best_eval_loss = float('inf')\n",
        "  best_eval_loss_step = 0\n",
        "\n",
        "  final_eval_loss = 0\n",
        "  final_eval_acc = 0\n",
        "\n",
        "  final_train_loss = 0\n",
        "  final_train_acc = 0\n",
        "\n",
        "\n",
        "  # exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "  criterion = nn.BCEWithLogitsLoss() #This is the loss function we use\n",
        " \n",
        "  step =0\n",
        "  model.train()\n",
        "  for epoch in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        model.train()\n",
        "        for inputs, labels in train_dl: #Basic Training Loop\n",
        "          optimizer.zero_grad() \n",
        "          step += 1\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1) \n",
        "          loss = criterion(outputs, labels.squeeze(1).float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == torch.argmax(labels,2).squeeze().data)\n",
        "          # if phase == 'train':\n",
        "          #     scheduler.step()\n",
        "          if step % evaluation_steps == 0:\n",
        "            test_running_loss = 0.0\n",
        "            test_running_corrects = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "              for inputs, labels in test_dl:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs,is_training=False)\n",
        "                loss = criterion(outputs, labels.squeeze(1).float())\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "                test_running_loss += loss.item() * inputs.size(0)\n",
        "                test_running_corrects += torch.sum(preds == torch.argmax(labels,2).squeeze().data)\n",
        "              #Put model back in training mode\n",
        "              model.train()\n",
        "\n",
        "            eval_loss = test_running_loss / DATA_SIZE[\"eval\"]\n",
        "            eval_acc = test_running_corrects.double() / DATA_SIZE[\"eval\"]\n",
        "\n",
        "            #We add the mettrics tto tensorboard\n",
        "            writer.add_scalar('Loss/'+\"eval\", eval_loss, step)\n",
        "            writer.add_scalar('Accuracy/'+\"eval\", eval_acc, step)\n",
        "\n",
        "            final_eval_loss = eval_loss\n",
        "            final_eval_acc = eval_acc\n",
        "\n",
        "            if eval_loss < best_eval_loss:\n",
        "              best_eval_loss = eval_loss\n",
        "              best_eval_loss_step = step\n",
        "              torch.save(model, savefile)\n",
        "\n",
        "            if eval_acc > best_eval_acc:\n",
        "              best_eval_acc = eval_acc\n",
        "              best_eval_acc_step = step\n",
        "\n",
        "            print(\"Step {} \".format(step),'{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            \"Eval\", eval_loss, eval_acc))\n",
        "\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / DATA_SIZE[\"train\"]\n",
        "        epoch_acc = running_corrects.double() / DATA_SIZE[\"train\"]\n",
        "\n",
        "\n",
        "        final_train_loss = epoch_loss\n",
        "        final_train_acc = epoch_acc\n",
        "\n",
        "        print('Epoch {}/{} '.format(epoch+1, EPOCHS ),'{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "              \"Train\", epoch_loss, epoch_acc))\n",
        "          \n",
        "        writer.add_scalar('Loss/'+\"Train\", epoch_loss, step)\n",
        "        writer.add_scalar('Accuracy/'+\"Train\", epoch_acc, step)\n",
        "\n",
        "  print('Best val Acc: {:4f}\\n\\n'.format(best_eval_acc))\n",
        "\n",
        "  return {  \"Best Eval Acc\": best_eval_acc.cpu().item(),\n",
        "            \"Best Eval Loss\": best_eval_loss,\n",
        "            \"Best Eval Step\": best_eval_loss_step,\n",
        "            \"Final Train Acc\": final_train_acc.cpu().item(),\n",
        "            \"Final Eval Acc\": final_eval_acc.cpu().item(),\n",
        "            \"Final Train Loss\": final_train_loss,\n",
        "            \"Final Eval Loss\": final_eval_loss,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is an example for training a model\n",
        "model = Classifier(dropout_rate=0,backbone=BACKBONE,multi_backbone=True,do_augmentation=True).to(device)\n",
        "freeze_layers(model,80)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "train_model(model,train_dl,test_dl,optimizer,epochs=10,savefile='model{}{}{}.pt'.format(0.001,50,0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtbflhviLsA-",
        "outputId": "d36ed628-7947-4b62-c8a1-ccf634560b1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This Bacbone has 158 parameter layers.\n",
            "Freezing 80 Layers.\n",
            "Step 20  Eval Loss: 0.6268 Acc: 0.4561\n",
            "Step 40  Eval Loss: 0.6077 Acc: 0.4561\n",
            "Epoch 1/10  Train Loss: 0.6160 Acc: 0.5439\n",
            "Step 60  Eval Loss: 0.6102 Acc: 0.4561\n",
            "Step 80  Eval Loss: 0.6315 Acc: 0.4561\n",
            "Step 100  Eval Loss: 0.6145 Acc: 0.4561\n",
            "Epoch 2/10  Train Loss: 0.5784 Acc: 0.5702\n",
            "Step 120  Eval Loss: 0.6065 Acc: 0.4561\n",
            "Step 140  Eval Loss: 0.6104 Acc: 0.4561\n",
            "Step 160  Eval Loss: 0.6109 Acc: 0.4561\n",
            "Epoch 3/10  Train Loss: 0.5705 Acc: 0.5702\n",
            "Step 180  Eval Loss: 0.6087 Acc: 0.4561\n",
            "Step 200  Eval Loss: 0.6114 Acc: 0.4561\n",
            "Step 220  Eval Loss: 0.5946 Acc: 0.4561\n",
            "Epoch 4/10  Train Loss: 0.5594 Acc: 0.5702\n",
            "Step 240  Eval Loss: 0.5985 Acc: 0.4561\n",
            "Step 260  Eval Loss: 0.6153 Acc: 0.4561\n",
            "Step 280  Eval Loss: 0.6050 Acc: 0.4561\n",
            "Epoch 5/10  Train Loss: 0.5580 Acc: 0.5702\n",
            "Step 300  Eval Loss: 0.6045 Acc: 0.4561\n",
            "Step 320  Eval Loss: 0.6035 Acc: 0.4737\n",
            "Step 340  Eval Loss: 0.5996 Acc: 0.4561\n",
            "Epoch 6/10  Train Loss: 0.5341 Acc: 0.5702\n",
            "Step 360  Eval Loss: 0.6007 Acc: 0.4912\n",
            "Step 380  Eval Loss: 0.5930 Acc: 0.4737\n",
            "Epoch 7/10  Train Loss: 0.5132 Acc: 0.6009\n",
            "Step 400  Eval Loss: 0.5985 Acc: 0.4737\n",
            "Step 420  Eval Loss: 0.6105 Acc: 0.4737\n",
            "Step 440  Eval Loss: 0.5828 Acc: 0.4561\n",
            "Epoch 8/10  Train Loss: 0.5048 Acc: 0.6272\n",
            "Step 460  Eval Loss: 0.5869 Acc: 0.4386\n",
            "Step 480  Eval Loss: 0.5707 Acc: 0.5439\n",
            "Step 500  Eval Loss: 0.5609 Acc: 0.5439\n",
            "Epoch 9/10  Train Loss: 0.4707 Acc: 0.6272\n",
            "Step 520  Eval Loss: 0.5798 Acc: 0.5263\n",
            "Step 540  Eval Loss: 0.5951 Acc: 0.5088\n",
            "Step 560  Eval Loss: 0.6217 Acc: 0.4737\n",
            "Epoch 10/10  Train Loss: 0.4548 Acc: 0.6535\n",
            "Best val Acc: 0.543860\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Best Eval Acc': 0.5438596491228069,\n",
              " 'Best Eval Loss': 0.5609051177376195,\n",
              " 'Best Eval Step': 500,\n",
              " 'Final Eval Acc': 0.47368421052631576,\n",
              " 'Final Eval Loss': 0.6217203307570073,\n",
              " 'Final Train Acc': 0.6535087719298245,\n",
              " 'Final Train Loss': 0.4547569307318905}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "D7OdEIJLkzu8",
        "outputId": "0d9255d3-4115-4ed9-8a9b-a2ad803b9d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Decay 0 \n",
            "Freeze       10 \n",
            "Dropout Rate 0 \n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d42f7e0f9f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mfreeze_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model{}{}{}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mSweep_Summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Backbone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-80d654f79364>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, test_dl, optimizer, evaluation_steps, epochs, savefile)\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#This part of the code runs multiple trainings on multiple combination on \n",
        "#specified hyperparameters for hyper-parameter tuning \n",
        "\n",
        "Sweep_Summary = {\n",
        "    \"Backbone\" : [],\n",
        "    \"Multi Backbone\": [],\n",
        "    \"Optim\" : [],\n",
        "    \"LR\" : [],\n",
        "    \"Scheduler\" : [],\n",
        "    \"Epochs\" : [],\n",
        "    \"Batch Size\" : [],\n",
        "    \"Dropout\": [],\n",
        "    \"Augmentation\": [],\n",
        "    \"Weight Decay\": [],\n",
        "    \"Freeze Layers\": [],\n",
        "    \"Best Eval Loss\": [],\n",
        "    \"Best Eval Step\": [],\n",
        "    \"Best Eval Acc\": [],\n",
        "    \"Final Train Acc\": [],\n",
        "    \"Final Eval Acc\": [],\n",
        "    \"Final Train Loss\": [],\n",
        "    \"Final Eval Loss\": [],\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "for weight_decay in WEIGHT_DECAY:\n",
        "  for freeze in FREEZE:\n",
        "    for dropout_rate in DROPOUT:\n",
        "      print(\"Weight Decay {} \".format(weight_decay))\n",
        "      print(\"Freeze       {} \".format(freeze))\n",
        "      print(\"Dropout Rate {} \\n------------------------------------------------------\".format(dropout_rate))\n",
        "      model = Classifier(dropout_rate=dropout_rate,backbone=BACKBONE,multi_backbone=MULTI_BACKBONE).to(device)\n",
        "      freeze_layers(model,freeze)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
        "      summary = train_model(model,train_dl,test_dl,optimizer,epochs=EPOCHS,savefile='model{}{}{}.pt'.format(weight_decay,freeze,dropout_rate))\n",
        "\n",
        "      Sweep_Summary[\"Backbone\"].append(BACKBONE)\n",
        "      Sweep_Summary[\"Optim\"].append(OPTIM)\n",
        "      Sweep_Summary[\"LR\"].append(LR)\n",
        "      Sweep_Summary[\"Scheduler\"].append(SCHEDULER)      \n",
        "      Sweep_Summary[\"Epochs\"].append(EPOCHS)\n",
        "      Sweep_Summary[\"Batch Size\"].append(BATCHSIZE)\n",
        "      Sweep_Summary[\"Dropout\"].append(dropout_rate)\n",
        "      Sweep_Summary[\"Augmentation\"].append(AUGMENTATION)\n",
        "      Sweep_Summary[\"Weight Decay\"].append(weight_decay)\n",
        "      Sweep_Summary[\"Freeze Layers\"].append(freeze)\n",
        "      Sweep_Summary[\"Multi Backbone\"].append(MULTI_BACKBONE)\n",
        "      for key,value in summary.items():\n",
        "        Sweep_Summary[key].append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "brB-bKaP08Hb",
        "outputId": "f284e00a-39c7-4f48-e9dd-2bb84f82d6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cd47f918-8105-404d-aaf0-58a242646fd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Backbone</th>\n",
              "      <th>Multi Backbone</th>\n",
              "      <th>Optim</th>\n",
              "      <th>LR</th>\n",
              "      <th>Scheduler</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Dropout</th>\n",
              "      <th>Augmentation</th>\n",
              "      <th>Weight Decay</th>\n",
              "      <th>Freeze Layers</th>\n",
              "      <th>Best Eval Loss</th>\n",
              "      <th>Best Eval Step</th>\n",
              "      <th>Best Eval Acc</th>\n",
              "      <th>Final Train Acc</th>\n",
              "      <th>Final Eval Acc</th>\n",
              "      <th>Final Train Loss</th>\n",
              "      <th>Final Eval Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.524519</td>\n",
              "      <td>340</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.957031</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.071135</td>\n",
              "      <td>0.911740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.523889</td>\n",
              "      <td>540</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.027901</td>\n",
              "      <td>1.073272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.506876</td>\n",
              "      <td>340</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.042024</td>\n",
              "      <td>0.697343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.516705</td>\n",
              "      <td>600</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.957031</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.116061</td>\n",
              "      <td>0.976772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>25</td>\n",
              "      <td>0.576749</td>\n",
              "      <td>60</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.022749</td>\n",
              "      <td>1.270029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>25</td>\n",
              "      <td>0.534919</td>\n",
              "      <td>280</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.061553</td>\n",
              "      <td>0.981142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>25</td>\n",
              "      <td>0.561822</td>\n",
              "      <td>220</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.980469</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.056390</td>\n",
              "      <td>0.835536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>25</td>\n",
              "      <td>0.520022</td>\n",
              "      <td>840</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.054673</td>\n",
              "      <td>0.836594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.549737</td>\n",
              "      <td>340</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.033070</td>\n",
              "      <td>0.922413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.570477</td>\n",
              "      <td>360</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.022764</td>\n",
              "      <td>1.286257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.565983</td>\n",
              "      <td>340</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.024311</td>\n",
              "      <td>0.862905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.524916</td>\n",
              "      <td>820</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.972656</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.077674</td>\n",
              "      <td>0.798949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>50</td>\n",
              "      <td>0.572738</td>\n",
              "      <td>340</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.020041</td>\n",
              "      <td>1.126971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>50</td>\n",
              "      <td>0.521608</td>\n",
              "      <td>680</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.036462</td>\n",
              "      <td>0.826011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>50</td>\n",
              "      <td>0.564970</td>\n",
              "      <td>780</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.059667</td>\n",
              "      <td>0.950224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>50</td>\n",
              "      <td>0.556237</td>\n",
              "      <td>880</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.087918</td>\n",
              "      <td>0.758252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.550112</td>\n",
              "      <td>1340</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.960938</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.160574</td>\n",
              "      <td>0.613796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.562151</td>\n",
              "      <td>1560</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.960938</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.177153</td>\n",
              "      <td>0.627600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.558726</td>\n",
              "      <td>1800</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.917969</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.243365</td>\n",
              "      <td>0.572039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.555775</td>\n",
              "      <td>2100</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.691406</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.456001</td>\n",
              "      <td>0.565197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>10</td>\n",
              "      <td>0.537286</td>\n",
              "      <td>200</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.023849</td>\n",
              "      <td>1.020833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>10</td>\n",
              "      <td>0.571937</td>\n",
              "      <td>200</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.026305</td>\n",
              "      <td>1.228819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>10</td>\n",
              "      <td>0.547095</td>\n",
              "      <td>340</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.049095</td>\n",
              "      <td>0.910337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>10</td>\n",
              "      <td>0.527522</td>\n",
              "      <td>700</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.080304</td>\n",
              "      <td>0.896574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>25</td>\n",
              "      <td>0.572477</td>\n",
              "      <td>280</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.022526</td>\n",
              "      <td>0.933137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>25</td>\n",
              "      <td>0.546448</td>\n",
              "      <td>440</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.056658</td>\n",
              "      <td>1.003697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>25</td>\n",
              "      <td>0.534575</td>\n",
              "      <td>260</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.980469</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.063868</td>\n",
              "      <td>0.970195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>25</td>\n",
              "      <td>0.515918</td>\n",
              "      <td>840</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.980469</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.056825</td>\n",
              "      <td>0.832078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>40</td>\n",
              "      <td>0.540080</td>\n",
              "      <td>280</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.022619</td>\n",
              "      <td>1.088005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>40</td>\n",
              "      <td>0.546334</td>\n",
              "      <td>420</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.976562</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.046562</td>\n",
              "      <td>0.920055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>40</td>\n",
              "      <td>0.560520</td>\n",
              "      <td>360</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.065104</td>\n",
              "      <td>0.843046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>40</td>\n",
              "      <td>0.562240</td>\n",
              "      <td>680</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.059261</td>\n",
              "      <td>0.966263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>50</td>\n",
              "      <td>0.547530</td>\n",
              "      <td>600</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.053464</td>\n",
              "      <td>1.035585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>50</td>\n",
              "      <td>0.527534</td>\n",
              "      <td>800</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.972656</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.087738</td>\n",
              "      <td>0.867286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>50</td>\n",
              "      <td>0.542973</td>\n",
              "      <td>620</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.980469</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.067168</td>\n",
              "      <td>0.923764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>50</td>\n",
              "      <td>0.529097</td>\n",
              "      <td>1500</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.071405</td>\n",
              "      <td>0.677758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>55</td>\n",
              "      <td>0.550217</td>\n",
              "      <td>1320</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.135316</td>\n",
              "      <td>0.696440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>55</td>\n",
              "      <td>0.555826</td>\n",
              "      <td>1200</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.941406</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.179101</td>\n",
              "      <td>0.624284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>55</td>\n",
              "      <td>0.552272</td>\n",
              "      <td>1680</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.224622</td>\n",
              "      <td>0.608710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>55</td>\n",
              "      <td>0.565162</td>\n",
              "      <td>1740</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.410912</td>\n",
              "      <td>0.576544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>10</td>\n",
              "      <td>0.539851</td>\n",
              "      <td>360</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.036772</td>\n",
              "      <td>0.934315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>10</td>\n",
              "      <td>0.538967</td>\n",
              "      <td>460</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.062382</td>\n",
              "      <td>0.751192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>10</td>\n",
              "      <td>0.567504</td>\n",
              "      <td>160</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.021666</td>\n",
              "      <td>1.025139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>10</td>\n",
              "      <td>0.568039</td>\n",
              "      <td>360</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.042243</td>\n",
              "      <td>1.580782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.559682</td>\n",
              "      <td>180</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.018215</td>\n",
              "      <td>1.043107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.493861</td>\n",
              "      <td>420</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.976562</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.057422</td>\n",
              "      <td>0.679152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.564331</td>\n",
              "      <td>440</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.036226</td>\n",
              "      <td>0.861008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.561832</td>\n",
              "      <td>540</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.040948</td>\n",
              "      <td>1.052402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>40</td>\n",
              "      <td>0.519186</td>\n",
              "      <td>340</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.007934</td>\n",
              "      <td>1.166759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>40</td>\n",
              "      <td>0.561629</td>\n",
              "      <td>320</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.044840</td>\n",
              "      <td>0.934024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>40</td>\n",
              "      <td>0.510254</td>\n",
              "      <td>360</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.976562</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.061820</td>\n",
              "      <td>0.748724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>40</td>\n",
              "      <td>0.541542</td>\n",
              "      <td>620</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.976562</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.071857</td>\n",
              "      <td>1.069821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.533205</td>\n",
              "      <td>480</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.992188</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.037844</td>\n",
              "      <td>0.913108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.556747</td>\n",
              "      <td>500</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.029529</td>\n",
              "      <td>0.970438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.539955</td>\n",
              "      <td>720</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.996094</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.030664</td>\n",
              "      <td>0.846921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.530088</td>\n",
              "      <td>1620</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.988281</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.079480</td>\n",
              "      <td>0.642136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>55</td>\n",
              "      <td>0.560379</td>\n",
              "      <td>1220</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.177037</td>\n",
              "      <td>0.676117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>55</td>\n",
              "      <td>0.525810</td>\n",
              "      <td>1560</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.205349</td>\n",
              "      <td>0.558281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>55</td>\n",
              "      <td>0.542304</td>\n",
              "      <td>1620</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.251045</td>\n",
              "      <td>0.576916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>False</td>\n",
              "      <td>Adam</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>None</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>55</td>\n",
              "      <td>0.573028</td>\n",
              "      <td>2320</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.425186</td>\n",
              "      <td>0.582550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd47f918-8105-404d-aaf0-58a242646fd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd47f918-8105-404d-aaf0-58a242646fd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd47f918-8105-404d-aaf0-58a242646fd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Backbone  Multi Backbone Optim       LR Scheduler  Epochs  Batch Size  \\\n",
              "0   resnet18           False  Adam  0.00005      None      40           4   \n",
              "1   resnet18           False  Adam  0.00005      None      40           4   \n",
              "2   resnet18           False  Adam  0.00005      None      40           4   \n",
              "3   resnet18           False  Adam  0.00005      None      40           4   \n",
              "4   resnet18           False  Adam  0.00005      None      40           4   \n",
              "5   resnet18           False  Adam  0.00005      None      40           4   \n",
              "6   resnet18           False  Adam  0.00005      None      40           4   \n",
              "7   resnet18           False  Adam  0.00005      None      40           4   \n",
              "8   resnet18           False  Adam  0.00005      None      40           4   \n",
              "9   resnet18           False  Adam  0.00005      None      40           4   \n",
              "10  resnet18           False  Adam  0.00005      None      40           4   \n",
              "11  resnet18           False  Adam  0.00005      None      40           4   \n",
              "12  resnet18           False  Adam  0.00005      None      40           4   \n",
              "13  resnet18           False  Adam  0.00005      None      40           4   \n",
              "14  resnet18           False  Adam  0.00005      None      40           4   \n",
              "15  resnet18           False  Adam  0.00005      None      40           4   \n",
              "16  resnet18           False  Adam  0.00005      None      40           4   \n",
              "17  resnet18           False  Adam  0.00005      None      40           4   \n",
              "18  resnet18           False  Adam  0.00005      None      40           4   \n",
              "19  resnet18           False  Adam  0.00005      None      40           4   \n",
              "20  resnet18           False  Adam  0.00005      None      40           4   \n",
              "21  resnet18           False  Adam  0.00005      None      40           4   \n",
              "22  resnet18           False  Adam  0.00005      None      40           4   \n",
              "23  resnet18           False  Adam  0.00005      None      40           4   \n",
              "24  resnet18           False  Adam  0.00005      None      40           4   \n",
              "25  resnet18           False  Adam  0.00005      None      40           4   \n",
              "26  resnet18           False  Adam  0.00005      None      40           4   \n",
              "27  resnet18           False  Adam  0.00005      None      40           4   \n",
              "28  resnet18           False  Adam  0.00005      None      40           4   \n",
              "29  resnet18           False  Adam  0.00005      None      40           4   \n",
              "30  resnet18           False  Adam  0.00005      None      40           4   \n",
              "31  resnet18           False  Adam  0.00005      None      40           4   \n",
              "32  resnet18           False  Adam  0.00005      None      40           4   \n",
              "33  resnet18           False  Adam  0.00005      None      40           4   \n",
              "34  resnet18           False  Adam  0.00005      None      40           4   \n",
              "35  resnet18           False  Adam  0.00005      None      40           4   \n",
              "36  resnet18           False  Adam  0.00005      None      40           4   \n",
              "37  resnet18           False  Adam  0.00005      None      40           4   \n",
              "38  resnet18           False  Adam  0.00005      None      40           4   \n",
              "39  resnet18           False  Adam  0.00005      None      40           4   \n",
              "40  resnet18           False  Adam  0.00005      None      40           4   \n",
              "41  resnet18           False  Adam  0.00005      None      40           4   \n",
              "42  resnet18           False  Adam  0.00005      None      40           4   \n",
              "43  resnet18           False  Adam  0.00005      None      40           4   \n",
              "44  resnet18           False  Adam  0.00005      None      40           4   \n",
              "45  resnet18           False  Adam  0.00005      None      40           4   \n",
              "46  resnet18           False  Adam  0.00005      None      40           4   \n",
              "47  resnet18           False  Adam  0.00005      None      40           4   \n",
              "48  resnet18           False  Adam  0.00005      None      40           4   \n",
              "49  resnet18           False  Adam  0.00005      None      40           4   \n",
              "50  resnet18           False  Adam  0.00005      None      40           4   \n",
              "51  resnet18           False  Adam  0.00005      None      40           4   \n",
              "52  resnet18           False  Adam  0.00005      None      40           4   \n",
              "53  resnet18           False  Adam  0.00005      None      40           4   \n",
              "54  resnet18           False  Adam  0.00005      None      40           4   \n",
              "55  resnet18           False  Adam  0.00005      None      40           4   \n",
              "56  resnet18           False  Adam  0.00005      None      40           4   \n",
              "57  resnet18           False  Adam  0.00005      None      40           4   \n",
              "58  resnet18           False  Adam  0.00005      None      40           4   \n",
              "59  resnet18           False  Adam  0.00005      None      40           4   \n",
              "\n",
              "    Dropout Augmentation  Weight Decay  Freeze Layers  Best Eval Loss  \\\n",
              "0       0.0         None       0.00000             10        0.524519   \n",
              "1       0.1         None       0.00000             10        0.523889   \n",
              "2       0.2         None       0.00000             10        0.506876   \n",
              "3       0.5         None       0.00000             10        0.516705   \n",
              "4       0.0         None       0.00000             25        0.576749   \n",
              "5       0.1         None       0.00000             25        0.534919   \n",
              "6       0.2         None       0.00000             25        0.561822   \n",
              "7       0.5         None       0.00000             25        0.520022   \n",
              "8       0.0         None       0.00000             40        0.549737   \n",
              "9       0.1         None       0.00000             40        0.570477   \n",
              "10      0.2         None       0.00000             40        0.565983   \n",
              "11      0.5         None       0.00000             40        0.524916   \n",
              "12      0.0         None       0.00000             50        0.572738   \n",
              "13      0.1         None       0.00000             50        0.521608   \n",
              "14      0.2         None       0.00000             50        0.564970   \n",
              "15      0.5         None       0.00000             50        0.556237   \n",
              "16      0.0         None       0.00000             55        0.550112   \n",
              "17      0.1         None       0.00000             55        0.562151   \n",
              "18      0.2         None       0.00000             55        0.558726   \n",
              "19      0.5         None       0.00000             55        0.555775   \n",
              "20      0.0         None       0.00100             10        0.537286   \n",
              "21      0.1         None       0.00100             10        0.571937   \n",
              "22      0.2         None       0.00100             10        0.547095   \n",
              "23      0.5         None       0.00100             10        0.527522   \n",
              "24      0.0         None       0.00100             25        0.572477   \n",
              "25      0.1         None       0.00100             25        0.546448   \n",
              "26      0.2         None       0.00100             25        0.534575   \n",
              "27      0.5         None       0.00100             25        0.515918   \n",
              "28      0.0         None       0.00100             40        0.540080   \n",
              "29      0.1         None       0.00100             40        0.546334   \n",
              "30      0.2         None       0.00100             40        0.560520   \n",
              "31      0.5         None       0.00100             40        0.562240   \n",
              "32      0.0         None       0.00100             50        0.547530   \n",
              "33      0.1         None       0.00100             50        0.527534   \n",
              "34      0.2         None       0.00100             50        0.542973   \n",
              "35      0.5         None       0.00100             50        0.529097   \n",
              "36      0.0         None       0.00100             55        0.550217   \n",
              "37      0.1         None       0.00100             55        0.555826   \n",
              "38      0.2         None       0.00100             55        0.552272   \n",
              "39      0.5         None       0.00100             55        0.565162   \n",
              "40      0.0         None       0.00001             10        0.539851   \n",
              "41      0.1         None       0.00001             10        0.538967   \n",
              "42      0.2         None       0.00001             10        0.567504   \n",
              "43      0.5         None       0.00001             10        0.568039   \n",
              "44      0.0         None       0.00001             25        0.559682   \n",
              "45      0.1         None       0.00001             25        0.493861   \n",
              "46      0.2         None       0.00001             25        0.564331   \n",
              "47      0.5         None       0.00001             25        0.561832   \n",
              "48      0.0         None       0.00001             40        0.519186   \n",
              "49      0.1         None       0.00001             40        0.561629   \n",
              "50      0.2         None       0.00001             40        0.510254   \n",
              "51      0.5         None       0.00001             40        0.541542   \n",
              "52      0.0         None       0.00001             50        0.533205   \n",
              "53      0.1         None       0.00001             50        0.556747   \n",
              "54      0.2         None       0.00001             50        0.539955   \n",
              "55      0.5         None       0.00001             50        0.530088   \n",
              "56      0.0         None       0.00001             55        0.560379   \n",
              "57      0.1         None       0.00001             55        0.525810   \n",
              "58      0.2         None       0.00001             55        0.542304   \n",
              "59      0.5         None       0.00001             55        0.573028   \n",
              "\n",
              "    Best Eval Step  Best Eval Acc  Final Train Acc  Final Eval Acc  \\\n",
              "0              340       0.586207         0.957031        0.551724   \n",
              "1              540       0.655172         0.988281        0.448276   \n",
              "2              340       0.620690         0.988281        0.482759   \n",
              "3              600       0.620690         0.957031        0.379310   \n",
              "4               60       0.551724         1.000000        0.482759   \n",
              "5              280       0.586207         0.968750        0.482759   \n",
              "6              220       0.586207         0.980469        0.551724   \n",
              "7              840       0.586207         0.984375        0.482759   \n",
              "8              340       0.586207         0.992188        0.448276   \n",
              "9              360       0.586207         0.996094        0.517241   \n",
              "10             340       0.620690         0.996094        0.482759   \n",
              "11             820       0.655172         0.972656        0.482759   \n",
              "12             340       0.551724         1.000000        0.517241   \n",
              "13             680       0.620690         0.992188        0.551724   \n",
              "14             780       0.586207         0.984375        0.551724   \n",
              "15             880       0.586207         0.984375        0.448276   \n",
              "16            1340       0.586207         0.960938        0.517241   \n",
              "17            1560       0.517241         0.960938        0.413793   \n",
              "18            1800       0.586207         0.917969        0.482759   \n",
              "19            2100       0.551724         0.691406        0.551724   \n",
              "20             200       0.655172         0.996094        0.517241   \n",
              "21             200       0.586207         0.992188        0.482759   \n",
              "22             340       0.586207         0.984375        0.482759   \n",
              "23             700       0.586207         0.968750        0.448276   \n",
              "24             280       0.586207         0.996094        0.517241   \n",
              "25             440       0.620690         0.988281        0.517241   \n",
              "26             260       0.620690         0.980469        0.517241   \n",
              "27             840       0.620690         0.980469        0.517241   \n",
              "28             280       0.586207         0.992188        0.448276   \n",
              "29             420       0.620690         0.976562        0.482759   \n",
              "30             360       0.586207         0.968750        0.517241   \n",
              "31             680       0.586207         0.984375        0.379310   \n",
              "32             600       0.586207         0.988281        0.517241   \n",
              "33             800       0.620690         0.972656        0.551724   \n",
              "34             620       0.586207         0.980469        0.517241   \n",
              "35            1500       0.655172         0.992188        0.586207   \n",
              "36            1320       0.551724         0.988281        0.448276   \n",
              "37            1200       0.551724         0.941406        0.482759   \n",
              "38            1680       0.551724         0.937500        0.482759   \n",
              "39            1740       0.551724         0.753906        0.551724   \n",
              "40             360       0.586207         0.984375        0.482759   \n",
              "41             460       0.620690         0.984375        0.517241   \n",
              "42             160       0.586207         0.996094        0.517241   \n",
              "43             360       0.551724         0.988281        0.482759   \n",
              "44             180       0.551724         1.000000        0.517241   \n",
              "45             420       0.586207         0.976562        0.413793   \n",
              "46             440       0.551724         0.988281        0.517241   \n",
              "47             540       0.586207         0.984375        0.448276   \n",
              "48             340       0.620690         1.000000        0.482759   \n",
              "49             320       0.551724         0.984375        0.482759   \n",
              "50             360       0.655172         0.976562        0.586207   \n",
              "51             620       0.586207         0.976562        0.482759   \n",
              "52             480       0.586207         0.992188        0.517241   \n",
              "53             500       0.586207         0.996094        0.586207   \n",
              "54             720       0.586207         0.996094        0.517241   \n",
              "55            1620       0.620690         0.988281        0.517241   \n",
              "56            1220       0.551724         0.937500        0.482759   \n",
              "57            1560       0.586207         0.949219        0.551724   \n",
              "58            1620       0.586207         0.937500        0.586207   \n",
              "59            2320       0.517241         0.734375        0.482759   \n",
              "\n",
              "    Final Train Loss  Final Eval Loss  \n",
              "0           0.071135         0.911740  \n",
              "1           0.027901         1.073272  \n",
              "2           0.042024         0.697343  \n",
              "3           0.116061         0.976772  \n",
              "4           0.022749         1.270029  \n",
              "5           0.061553         0.981142  \n",
              "6           0.056390         0.835536  \n",
              "7           0.054673         0.836594  \n",
              "8           0.033070         0.922413  \n",
              "9           0.022764         1.286257  \n",
              "10          0.024311         0.862905  \n",
              "11          0.077674         0.798949  \n",
              "12          0.020041         1.126971  \n",
              "13          0.036462         0.826011  \n",
              "14          0.059667         0.950224  \n",
              "15          0.087918         0.758252  \n",
              "16          0.160574         0.613796  \n",
              "17          0.177153         0.627600  \n",
              "18          0.243365         0.572039  \n",
              "19          0.456001         0.565197  \n",
              "20          0.023849         1.020833  \n",
              "21          0.026305         1.228819  \n",
              "22          0.049095         0.910337  \n",
              "23          0.080304         0.896574  \n",
              "24          0.022526         0.933137  \n",
              "25          0.056658         1.003697  \n",
              "26          0.063868         0.970195  \n",
              "27          0.056825         0.832078  \n",
              "28          0.022619         1.088005  \n",
              "29          0.046562         0.920055  \n",
              "30          0.065104         0.843046  \n",
              "31          0.059261         0.966263  \n",
              "32          0.053464         1.035585  \n",
              "33          0.087738         0.867286  \n",
              "34          0.067168         0.923764  \n",
              "35          0.071405         0.677758  \n",
              "36          0.135316         0.696440  \n",
              "37          0.179101         0.624284  \n",
              "38          0.224622         0.608710  \n",
              "39          0.410912         0.576544  \n",
              "40          0.036772         0.934315  \n",
              "41          0.062382         0.751192  \n",
              "42          0.021666         1.025139  \n",
              "43          0.042243         1.580782  \n",
              "44          0.018215         1.043107  \n",
              "45          0.057422         0.679152  \n",
              "46          0.036226         0.861008  \n",
              "47          0.040948         1.052402  \n",
              "48          0.007934         1.166759  \n",
              "49          0.044840         0.934024  \n",
              "50          0.061820         0.748724  \n",
              "51          0.071857         1.069821  \n",
              "52          0.037844         0.913108  \n",
              "53          0.029529         0.970438  \n",
              "54          0.030664         0.846921  \n",
              "55          0.079480         0.642136  \n",
              "56          0.177037         0.676117  \n",
              "57          0.205349         0.558281  \n",
              "58          0.251045         0.576916  \n",
              "59          0.425186         0.582550  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#We save the results of the training sweep \n",
        "df = pd.DataFrame(Sweep_Summary)\n",
        "df.to_csv(\"Fish_Classifier_Sweep_Results_resnet18_singleabckbone.csv\")\n",
        "!cp \"/content/Fish_Classifier_Sweep_Results_resnet18_singleabckbone.csv\" \"/content/drive/MyDrive\"\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "In this section we load the ttrained model and test it."
      ],
      "metadata": {
        "id": "JN197toL8YdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3RdeuEJ8Wnm",
        "outputId": "ec91b89d-52c0-46d9-c219-88803269a33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: 0.5953 Acc: 0.5357\n",
            "Pred: [1 1 1 2 1 1 1 2 1 1 1 2 2 1 1 1 0 1 1 1 1 1 1 2 1 2 1 1 1 2 1 0 1 2 1 1 1\n",
            " 2 2 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1]\n",
            "True: [1 0 0 2 1 1 0 2 1 0 0 1 2 0 0 2 1 1 1 2 0 1 1 2 2 2 0 1 1 1 2 0 2 1 1 1 1\n",
            " 2 0 1 1 0 1 1 2 1 0 1 0 0 1 0 0 1 1 2]\n",
            "Confusion Matrix:\n",
            "[[ 1 14  2]\n",
            " [ 1 22  3]\n",
            " [ 0  6  7]]\n"
          ]
        }
      ],
      "source": [
        "# Test model loading\n",
        "TRAINED_MODEL_PATH = '/content/model0.001500.001.pt'\n",
        "\n",
        "saved_model = torch.load(TRAINED_MODEL_PATH)\n",
        "saved_model = model\n",
        "saved_model.eval()\n",
        "test_running_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "dl = test_dl\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in dl:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = saved_model(inputs,is_training=False)\n",
        "    loss = criterion(outputs, labels.squeeze(1).float())\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    all_preds.extend(preds.cpu().tolist())\n",
        "    all_labels.extend([label[0] for label in labels.cpu().tolist()])\n",
        "\n",
        "\n",
        "    test_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.argmax(all_labels, axis = 1)\n",
        "\n",
        "eval_loss = test_running_loss / len(dl.dataset)\n",
        "eval_acc = accuracy_score(all_labels, all_preds)\n",
        "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Eval\", eval_loss, eval_acc))\n",
        "print(f'Pred: {all_preds}')\n",
        "print(f'True: {all_labels}')\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SnSMTbvaKRI7"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "torch.cuda.empty_cache()\n",
        "model = Classifier(dropout_rate=dropout_rate,backbone=BACKBONE,multi_backbone=True).to(device)\n",
        "summary(model, (4, 3, 200, 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WsFblgnUKAGx"
      },
      "outputs": [],
      "source": [
        "!tensorboard dev upload --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WeISDPwmagUp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pytorch_fish_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}