{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwv87i7kt60v"
   },
   "source": [
    "#Requirements\n",
    "In this secttion we import the required packages for ttraining our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kA_BNgzN7Zn",
    "outputId": "c0ec5295-44e9-439c-e819-f6618efd8650"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_MSIOaV8fAk",
    "outputId": "fbf547fc-62d2-47c4-c5e9-7a27be2b8e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#We mount the google drive. If You are running this notebook locally do nott run this cell. \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxpvNK6It6PI"
   },
   "source": [
    "#Configurations\n",
    "In this section we define the configs for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_MSIOaV8fAk"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62mzNuS-89r8"
   },
   "outputs": [],
   "source": [
    "DATASET = \"Allele B Cropped\"\n",
    "X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/x_train_b_cropped.npy\"\n",
    "Y_PATH= \"/content/drive/Shareddrives/Exploding Gradients/y_train_b.npy\"\n",
    "\n",
    "\n",
    "BACKBONE = \"resnet50\"\n",
    "MULTI_BACKBONE = False\n",
    "OPTIM = \"Adam\"\n",
    "LR =5e-5\n",
    "SCHEDULER = \"None\"\n",
    "EPOCHS = 40\n",
    "BATCHSIZE = 4\n",
    "# DROPOUT = [0,0.1,0.2,0.5]\n",
    "DROPOUT = [0,0.1,0.2,0.5]\n",
    "AUGMENTATION = \"None\"\n",
    "\n",
    "#The following is a list of hyper parameters to test. All Permuttations will be\n",
    "#tested\n",
    "\n",
    "WEIGHT_DECAY = [1e-5]\n",
    "# FREEZE = [10,25,40,50,55]\n",
    "FREEZE = [50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk8VqKiOuv-U"
   },
   "source": [
    "#Data Processing\n",
    "In this section, we read the dataset as a pre saved numpy array. After reading the datset. we divide it into train-testtt sets. We tthen create a pytorch dataset which we will then turn into a dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ob7GdwdTOjia",
    "outputId": "5bead4f7-1e17-4456-961c-563e144e68b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Tensor Shape:  (285, 4, 200, 1024, 3)\n",
      "y Tensor Shape:  (285, 1)\n"
     ]
    }
   ],
   "source": [
    "#We read the \n",
    "x = np.load(X_PATH)\n",
    "y = np.load(Y_PATH)\n",
    "print(\"X Tensor Shape: \",x.shape)\n",
    "print(\"y Tensor Shape: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQq6Lhq3_E1-",
    "outputId": "4ba55fc8-f798-479d-8654-2fb8e2b5c129"
   },
   "outputs": [],
   "source": [
    "# In this part we draw the pie graph for the distribution of the data with 3 labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentages= [np.count_nonzero(y==0),\n",
    "              np.count_nonzero(y==1),\n",
    "              np.count_nonzero(y==2)]\n",
    "              \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(percentages,labels=[\"1\",\"2\",\"3\"], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2tTpGUZPSgL",
    "outputId": "c2958609-b86f-4d83-c085-de7d9930eb7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape:  torch.Size([256, 4, 3, 200, 1024]) x test shape:  torch.Size([29, 4, 3, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "# We creatte tensors from the numpy arrays\n",
    "tensor_x = torch.Tensor(x) \n",
    "tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "#initially the data is in the form (Batch, 4Images, Width, Height, Channels)\n",
    "#We then  change it to (Batch, 4Images,Channels, Width, Height )\n",
    "tensor_x = torch.swapaxes(tensor_x,2,4)\n",
    "tensor_x = torch.swapaxes(tensor_x,3,4)\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(tensor_x,tensor_y,test_size=0.1)\n",
    "\n",
    "print(\"x train shape: \", x_train.shape,\"x test shape: \",x_test.shape)\n",
    "print(\"x test shape: \",x_test.shape)\n",
    "\n",
    "validation_dl = TensorDataset(x_validate, nn.functional.one_hot(y_validate,3))\n",
    "test_ds = TensorDataset(x_test,nn.functional.one_hot(y_test,3)) \n",
    "#We shuffle the training set and drop the last of the test set\n",
    "#This might not be necessary\n",
    "train_dl = DataLoader(train_ds,BATCHSIZE,shuffle = True)\n",
    "test_dl = DataLoader(test_ds,BATCHSIZE,drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "9ien3JWFXYe3",
    "outputId": "7514d83a-9216-4b82-b0f2-089e9ce1b7d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The following is the distribution of the test set\n",
    "#We want this to be similar to the dataset as a whole\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentages= [np.count_nonzero(y_test==0),\n",
    "              np.count_nonzero(y_test==1),\n",
    "              np.count_nonzero(y_test==2)]\n",
    "              \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(percentages,labels=[\"1\",\"2\",\"3\"], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qxyFVTvyJ9G"
   },
   "source": [
    "#Classifier\n",
    "The following section includes the code for the classifier.\n",
    "We have support for muliple backbones. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2tTpGUZPSgL"
   },
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(x) \n",
    "tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "tensor_x = torch.swapaxes(tensor_x,2,4)\n",
    "tensor_x = torch.swapaxes(tensor_x,3,4)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tensor_x, tensor_y, test_size=0.1, stratify=tensor_y)\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.1, stratify=y_train)\n",
    "\n",
    "print(\"x train shape: \", x_train.shape, \"x validate shape: \", x_validate.shape, \"x test shape: \",x_test.shape)\n",
    "\n",
    "train_ds = TensorDataset(x_train,nn.functional.one_hot(y_train,3))\n",
    "validation_ds = TensorDataset(x_validate, nn.functional.one_hot(y_validate,3))\n",
    "test_ds = TensorDataset(x_test,nn.functional.one_hot(y_test,3)) \n",
    "\n",
    "train_dl = DataLoader(train_ds, BATCHSIZE, shuffle = True, drop_last = True)\n",
    "validation_dl = DataLoader(validation_ds, BATCHSIZE)\n",
    "test_dl = DataLoader(test_ds, BATCHSIZE)\n",
    "del x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og6k2dvk8Wni"
   },
   "outputs": [],
   "source": [
    "#This function takes in a model and replaces inplace relu layers to an independent relu layer\n",
    "def reluToInplaceFalse(model):\n",
    "  for name, child in model.named_children():\n",
    "    if isinstance(child, nn.ReLU):\n",
    "      setattr(child, 'inplace', False)\n",
    "    else:\n",
    "      reluToInplaceFalse(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHTLP0NoRnLm"
   },
   "outputs": [],
   "source": [
    "#This is the classifier Class.\n",
    "from torchvision.transforms.transforms import RandomRotation, RandomAdjustSharpness, RandomGrayscale\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, backbone='resnet', multi_backbone = False, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False):\n",
    "    super().__init__()\n",
    "    self.multi_backbone = multi_backbone # Bool: Indicates if we use multibackbone\n",
    "\n",
    "    #In the following secttion we download the appropriatee prettrained model\n",
    "    if backbone == \"vgg19\":\n",
    "      backbone = torchvision.models.vgg19(pretrained=True)\n",
    "      self.out_channels = 25088\n",
    "      \n",
    "    elif backbone == \"resnet18\":\n",
    "      backbone = torchvision.models.resnet18(pretrained=True)\n",
    "      self.out_channels = 512\n",
    "\n",
    "    elif backbone == \"resnet50\":\n",
    "      backbone = torchvision.models.resnet50(pretrained=True)\n",
    "      self.out_channels = 2048\n",
    "\n",
    "    elif backbone == \"Efficientnet b1\":\n",
    "      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n",
    "      self.out_channels = 1280\n",
    "\n",
    "    elif backbone == \"Efficientnet b3\":\n",
    "      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n",
    "      self.out_channels = 1536\n",
    "\n",
    "    elif backbone == \"Efficientnet b5\":\n",
    "      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n",
    "      self.out_channels = 2048\n",
    "\n",
    "    elif backbone == \"Efficientnet b7\":\n",
    "      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n",
    "      self.out_channels = 2560\n",
    "      \n",
    "    # Disabling inplace ReLu becasuse GradCam doesn't work it enabled\n",
    "    reluToInplaceFalse(backbone)\n",
    "     \n",
    "    modules = list(backbone.children())[:-1]\n",
    "    self.do_augmentation = do_augmentation\n",
    "\n",
    "    if self.do_augmentation: #If augmentation is enabled we  init tthe layer\n",
    "      self.augmentation = nn.Sequential(transforms.RandomHorizontalFlip(0.2),\n",
    "                                        # transforms.RandomVerticalFlip(0.2),\n",
    "                                        # transforms.RandomPerspective(0.2),\n",
    "                                        RandomRotation(20),\n",
    "                                        # RandomAdjustSharpness(,),\n",
    "                                        # RandomGrayscale(),\n",
    "                                        # transforms.RandomAutocontrast()\n",
    "      )\n",
    "\n",
    "    if self.multi_backbone: #We create the backbones and put them on the device\n",
    "      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "\n",
    "    else:\n",
    "      self.backbone =  nn.Sequential(*modules).to(device)\n",
    "\n",
    "\n",
    "     \n",
    "    #This is the first fully connected layer\n",
    "    #The output of the backbone goes through this layer before being concatnenated\n",
    "    self.fc1 = nn.Sequential(nn.Dropout(dropout_rate),\n",
    "                              nn.Linear(self.out_channels, 256), \n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(dropout_rate)) #TODO: Experiment with BN and Dropout\n",
    "    #This is the final classification layer\n",
    "    self.fc = nn.Sequential(nn.Linear(1024, 256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Dropout(dropout_rate),\n",
    "                            nn.Linear(256, 3))                \n",
    "     \n",
    "  def forward(self, x, is_training = True): \n",
    "    if self.do_augmentation and is_training:\n",
    "      imgs = [self.augmentation(x[:,i]) for i in range(4)] #list of 4 images\n",
    "    else:\n",
    "      imgs = [x[:,i] for i in range(4)] #list of 4 images\n",
    "\n",
    "    if self.multi_backbone:\n",
    "      encodings = [self.fc1(self.backbone1(imgs[0]).squeeze()), \n",
    "                   self.fc1(self.backbone2(imgs[1]).squeeze()),\n",
    "                   self.fc1(self.backbone3(imgs[2]).squeeze()),\n",
    "                   self.fc1(self.backbone4(imgs[3]).squeeze())]\n",
    "    else:\n",
    "      encodings = [self.fc1(self.backbone(img).squeeze()) for img in imgs]\n",
    "\n",
    "    return self.fc(torch.cat(encodings,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfYTWTfpHT9E"
   },
   "outputs": [],
   "source": [
    "#This functtion is responsibl for freezing the early layers of the backbone(s)\n",
    "def freeze_layers(model,layers = 5):\n",
    "  if model.multi_backbone:\n",
    "    for i,param in enumerate(model.backbone1.parameters()):\n",
    "      pass\n",
    "    print(\"This Bacbone has {} parameter layers.\".format(i))\n",
    "    print(\"Freezing {} Layers.\".format(layers))\n",
    "    for i,param in enumerate(model.backbone1.parameters()):\n",
    "      if i < layers:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for i,param in enumerate(model.backbone2.parameters()):\n",
    "      if i < layers:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for i,param in enumerate(model.backbone3.parameters()):\n",
    "      if i < layers:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for i,param in enumerate(model.backbone4.parameters()):\n",
    "      if i < layers:\n",
    "        param.requires_grad = False\n",
    "  else:\n",
    "     for i,param in enumerate(model.backbone.parameters()):\n",
    "      if i < layers:\n",
    "        param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LPQIAFx5jfC"
   },
   "source": [
    "#Training\n",
    "This secttion containes the code for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6wrm2SBPmc9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model, test_dl, criterion):\n",
    "  test_running_loss = 0.0\n",
    "  test_running_corrects = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs,is_training=False)\n",
    "      loss = criterion(outputs, labels.squeeze(1).float())\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "      test_running_loss += loss.item() * inputs.size(0)\n",
    "      test_running_corrects += torch.sum(preds == torch.argmax(labels,2).squeeze().data)\n",
    "  \n",
    "  eval_loss = test_running_loss / len(test_dl.dataset)\n",
    "  eval_acc = test_running_corrects.double() / len(test_dl.dataset)\n",
    "  return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_dl, validation_dl, test_dl, optimizer, evaluation_steps = 20, epochs =1, savefile='model.pt'):\n",
    "  writer = SummaryWriter()\n",
    "\n",
    "  EPOCHS = epochs\n",
    "  DATA_SIZE =  {\"train\":len(train_dl.dataset),\"eval\":len(test_dl.dataset)}\n",
    "\n",
    "  best_validate_acc = 0\n",
    "  \n",
    "  best_validate_loss = float('inf')\n",
    "  best_validate_loss_step = 0\n",
    "\n",
    "  final_validate_loss = 0\n",
    "  final_validate_acc = 0\n",
    "\n",
    "  # exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "  criterion = nn.BCEWithLogitsLoss() #This is the loss function we use\n",
    " \n",
    "  step =0\n",
    "  model.train()\n",
    "  for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "        model.train()\n",
    "        for inputs, labels in train_dl:\n",
    "          model.train()\n",
    "          optimizer.zero_grad()\n",
    "      step += 1\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "      loss = criterion(outputs, labels.squeeze(1).float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # statistics\n",
    "      running_loss += loss.item() * inputs.size(0)\n",
    "      running_corrects += torch.sum(preds == torch.argmax(labels,2).squeeze().data)\n",
    "      # if phase == 'train':\n",
    "      #     scheduler.step()\n",
    "      if step % evaluation_steps == 0:\n",
    "        validate_loss, validate_acc = test_model(model, validation_dl, criterion)\n",
    "        writer.add_scalar('Loss/'+\"validate\", validate_loss, step)\n",
    "        writer.add_scalar('Accuracy/'+\"validate\", validate_acc, step)\n",
    "\n",
    "        if validate_loss < best_validate_loss:\n",
    "          best_validate_loss = validate_loss\n",
    "          best_validate_loss_step = step\n",
    "          best_validate_acc = validate_acc\n",
    "          torch.save(model, savefile)\n",
    "\n",
    "        final_validate_acc = validate_acc\n",
    "        final_validate_loss = validate_loss\n",
    "\n",
    "            writer.add_scalar('Loss/'+\"eval\", eval_loss, step)\n",
    "            writer.add_scalar('Accuracy/'+\"eval\", eval_acc, step)\n",
    "\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / DATA_SIZE[\"train\"]\n",
    "    epoch_acc = running_corrects.double() / DATA_SIZE[\"train\"]\n",
    "\n",
    "            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            # \"Eval\", eval_loss, eval_acc))\n",
    "\n",
    "    final_train_loss = epoch_loss\n",
    "    final_train_acc = epoch_acc\n",
    "\n",
    "        # print('Epoch {}/{} '.format(epoch+1, EPOCHS ),'{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        #       \"Train\", epoch_loss, epoch_acc))\n",
    "          \n",
    "    writer.add_scalar('Loss/'+\"Train\", epoch_loss, step)\n",
    "    writer.add_scalar('Accuracy/'+\"Train\", epoch_acc, step)\n",
    "\n",
    "  print('Best val Acc: {:4f}\\n\\n'.format(best_eval_acc))\n",
    "  test_loss, test_acc = test_model(model, test_dl, criterion)\n",
    "  return {  \"Test Acc\": test_acc.cpu().item(),\n",
    "            \"Test Loss\": test_loss,\n",
    "            \"Best Validate Acc\": best_validate_acc.cpu().item(),\n",
    "            \"Best Validate Loss\": best_validate_loss,\n",
    "            \"Best Validate Step\": best_validate_loss_step,\n",
    "            \"Final Train Acc\": final_train_acc.cpu().item(),\n",
    "            \"Final Eval Acc\": final_validate_acc.cpu().item(),\n",
    "            \"Final Train Loss\": final_train_loss,\n",
    "            \"Final Eval Loss\": final_validate_loss,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtbflhviLsA-",
    "outputId": "d36ed628-7947-4b62-c8a1-ccf634560b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Bacbone has 158 parameter layers.\n",
      "Freezing 80 Layers.\n",
      "Step 20  Eval Loss: 0.6268 Acc: 0.4561\n",
      "Step 40  Eval Loss: 0.6077 Acc: 0.4561\n",
      "Epoch 1/10  Train Loss: 0.6160 Acc: 0.5439\n",
      "Step 60  Eval Loss: 0.6102 Acc: 0.4561\n",
      "Step 80  Eval Loss: 0.6315 Acc: 0.4561\n",
      "Step 100  Eval Loss: 0.6145 Acc: 0.4561\n",
      "Epoch 2/10  Train Loss: 0.5784 Acc: 0.5702\n",
      "Step 120  Eval Loss: 0.6065 Acc: 0.4561\n",
      "Step 140  Eval Loss: 0.6104 Acc: 0.4561\n",
      "Step 160  Eval Loss: 0.6109 Acc: 0.4561\n",
      "Epoch 3/10  Train Loss: 0.5705 Acc: 0.5702\n",
      "Step 180  Eval Loss: 0.6087 Acc: 0.4561\n",
      "Step 200  Eval Loss: 0.6114 Acc: 0.4561\n",
      "Step 220  Eval Loss: 0.5946 Acc: 0.4561\n",
      "Epoch 4/10  Train Loss: 0.5594 Acc: 0.5702\n",
      "Step 240  Eval Loss: 0.5985 Acc: 0.4561\n",
      "Step 260  Eval Loss: 0.6153 Acc: 0.4561\n",
      "Step 280  Eval Loss: 0.6050 Acc: 0.4561\n",
      "Epoch 5/10  Train Loss: 0.5580 Acc: 0.5702\n",
      "Step 300  Eval Loss: 0.6045 Acc: 0.4561\n",
      "Step 320  Eval Loss: 0.6035 Acc: 0.4737\n",
      "Step 340  Eval Loss: 0.5996 Acc: 0.4561\n",
      "Epoch 6/10  Train Loss: 0.5341 Acc: 0.5702\n",
      "Step 360  Eval Loss: 0.6007 Acc: 0.4912\n",
      "Step 380  Eval Loss: 0.5930 Acc: 0.4737\n",
      "Epoch 7/10  Train Loss: 0.5132 Acc: 0.6009\n",
      "Step 400  Eval Loss: 0.5985 Acc: 0.4737\n",
      "Step 420  Eval Loss: 0.6105 Acc: 0.4737\n",
      "Step 440  Eval Loss: 0.5828 Acc: 0.4561\n",
      "Epoch 8/10  Train Loss: 0.5048 Acc: 0.6272\n",
      "Step 460  Eval Loss: 0.5869 Acc: 0.4386\n",
      "Step 480  Eval Loss: 0.5707 Acc: 0.5439\n",
      "Step 500  Eval Loss: 0.5609 Acc: 0.5439\n",
      "Epoch 9/10  Train Loss: 0.4707 Acc: 0.6272\n",
      "Step 520  Eval Loss: 0.5798 Acc: 0.5263\n",
      "Step 540  Eval Loss: 0.5951 Acc: 0.5088\n",
      "Step 560  Eval Loss: 0.6217 Acc: 0.4737\n",
      "Epoch 10/10  Train Loss: 0.4548 Acc: 0.6535\n",
      "Best val Acc: 0.543860\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Best Eval Acc': 0.5438596491228069,\n",
       " 'Best Eval Loss': 0.5609051177376195,\n",
       " 'Best Eval Step': 500,\n",
       " 'Final Eval Acc': 0.47368421052631576,\n",
       " 'Final Eval Loss': 0.6217203307570073,\n",
       " 'Final Train Acc': 0.6535087719298245,\n",
       " 'Final Train Loss': 0.4547569307318905}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example for training a model\n",
    "model = Classifier(dropout_rate=0,backbone=BACKBONE,multi_backbone=True,do_augmentation=True).to(device)\n",
    "freeze_layers(model,80)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "train_model(model,train_dl,test_dl,optimizer,epochs=10,savefile='model{}{}{}.pt'.format(0.001,50,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "86de9c116eff4a168a16e22dce464605",
      "fb24e23d12b74328a4daa17c3c4efa86",
      "804bcd615e714b15842821f9fd094bf1",
      "bb0ea55cd85741f690a2134d1c86f6d7",
      "bfed81409515492291064175a989a480",
      "6bf2defe3d4a48519bad5cc1f08e4cfd",
      "d89fc1b9997247028be0224da4008808",
      "63bb28b5f4524ebebe91e70c77caeb3f",
      "0e0054f459e0463785be1eb671c9dddc",
      "51c8aecceddd42748e8f5c7e91d1d8b4",
      "e3257cb24bc54dcab908227aa35c478c"
     ]
    },
    "id": "D7OdEIJLkzu8",
    "outputId": "43fc36d2-bbc1-4e83-b60b-9deba7a6fdf1"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d42f7e0f9f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mfreeze_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model{}{}{}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mSweep_Summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Backbone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-80d654f79364>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, test_dl, optimizer, evaluation_steps, epochs, savefile)\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This part of the code runs multiple trainings on multiple combination on \n",
    "#specified hyperparameters for hyper-parameter tuning \n",
    "\n",
    "Sweep_Summary = {\n",
    "    \"Backbone\" : [],\n",
    "    \"Multi Backbone\": [],\n",
    "    \"Optim\" : [],\n",
    "    \"LR\" : [],\n",
    "    \"Scheduler\" : [],\n",
    "    \"Epochs\" : [],\n",
    "    \"Batch Size\" : [],\n",
    "    \"Dropout\": [],\n",
    "    \"Augmentation\": [],\n",
    "    \"Weight Decay\": [],\n",
    "    \"Freeze Layers\": [],\n",
    "    \"Test Loss\": [],\n",
    "    \"Test Acc\": [],\n",
    "    \"Best Validate Loss\": [],\n",
    "    \"Best Validate Step\": [],\n",
    "    \"Best Validate Acc\": [],\n",
    "    \"Final Train Acc\": [],\n",
    "    \"Final Eval Acc\": [],\n",
    "    \"Final Train Loss\": [],\n",
    "    \"Final Eval Loss\": [],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for weight_decay in WEIGHT_DECAY:\n",
    "  for freeze in FREEZE:\n",
    "    for dropout_rate in DROPOUT:\n",
    "      print(\"Weight Decay {} \".format(weight_decay))\n",
    "      print(\"Freeze       {} \".format(freeze))\n",
    "      print(\"Dropout Rate {} \\n------------------------------------------------------\".format(dropout_rate))\n",
    "      model = Classifier(dropout_rate=dropout_rate,backbone=BACKBONE,multi_backbone=MULTI_BACKBONE).to(device)\n",
    "      freeze_layers(model,freeze)\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
    "      summary = train_model(model,train_dl,validation_dl,test_dl,optimizer,epochs=EPOCHS,savefile='model{}{}{}.pt'.format(weight_decay,freeze,dropout_rate))\n",
    "\n",
    "      Sweep_Summary[\"Backbone\"].append(BACKBONE)\n",
    "      Sweep_Summary[\"Optim\"].append(OPTIM)\n",
    "      Sweep_Summary[\"LR\"].append(LR)\n",
    "      Sweep_Summary[\"Scheduler\"].append(SCHEDULER)      \n",
    "      Sweep_Summary[\"Epochs\"].append(EPOCHS)\n",
    "      Sweep_Summary[\"Batch Size\"].append(BATCHSIZE)\n",
    "      Sweep_Summary[\"Dropout\"].append(dropout_rate)\n",
    "      Sweep_Summary[\"Augmentation\"].append(AUGMENTATION)\n",
    "      Sweep_Summary[\"Weight Decay\"].append(weight_decay)\n",
    "      Sweep_Summary[\"Freeze Layers\"].append(freeze)\n",
    "      Sweep_Summary[\"Multi Backbone\"].append(MULTI_BACKBONE)\n",
    "      for key,value in summary.items():\n",
    "        Sweep_Summary[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "brB-bKaP08Hb",
    "outputId": "f284e00a-39c7-4f48-e9dd-2bb84f82d6d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cd47f918-8105-404d-aaf0-58a242646fd8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Backbone</th>\n",
       "      <th>Multi Backbone</th>\n",
       "      <th>Optim</th>\n",
       "      <th>LR</th>\n",
       "      <th>Scheduler</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Freeze Layers</th>\n",
       "      <th>Best Eval Loss</th>\n",
       "      <th>Best Eval Step</th>\n",
       "      <th>Best Eval Acc</th>\n",
       "      <th>Final Train Acc</th>\n",
       "      <th>Final Eval Acc</th>\n",
       "      <th>Final Train Loss</th>\n",
       "      <th>Final Eval Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.524519</td>\n",
       "      <td>340</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.071135</td>\n",
       "      <td>0.911740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.523889</td>\n",
       "      <td>540</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>1.073272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.506876</td>\n",
       "      <td>340</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.042024</td>\n",
       "      <td>0.697343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.516705</td>\n",
       "      <td>600</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.116061</td>\n",
       "      <td>0.976772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.576749</td>\n",
       "      <td>60</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.022749</td>\n",
       "      <td>1.270029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.534919</td>\n",
       "      <td>280</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.061553</td>\n",
       "      <td>0.981142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.561822</td>\n",
       "      <td>220</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.835536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.520022</td>\n",
       "      <td>840</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.836594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.549737</td>\n",
       "      <td>340</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.033070</td>\n",
       "      <td>0.922413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>360</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>1.286257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.565983</td>\n",
       "      <td>340</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.024311</td>\n",
       "      <td>0.862905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.524916</td>\n",
       "      <td>820</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.972656</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.798949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.572738</td>\n",
       "      <td>340</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>1.126971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.521608</td>\n",
       "      <td>680</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.036462</td>\n",
       "      <td>0.826011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.564970</td>\n",
       "      <td>780</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.950224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.556237</td>\n",
       "      <td>880</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.087918</td>\n",
       "      <td>0.758252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.550112</td>\n",
       "      <td>1340</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.160574</td>\n",
       "      <td>0.613796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.562151</td>\n",
       "      <td>1560</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.177153</td>\n",
       "      <td>0.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.558726</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.917969</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.243365</td>\n",
       "      <td>0.572039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.555775</td>\n",
       "      <td>2100</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.691406</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.456001</td>\n",
       "      <td>0.565197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.537286</td>\n",
       "      <td>200</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>1.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.571937</td>\n",
       "      <td>200</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.026305</td>\n",
       "      <td>1.228819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.547095</td>\n",
       "      <td>340</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.049095</td>\n",
       "      <td>0.910337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.527522</td>\n",
       "      <td>700</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.080304</td>\n",
       "      <td>0.896574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>280</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.933137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.546448</td>\n",
       "      <td>440</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.056658</td>\n",
       "      <td>1.003697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.534575</td>\n",
       "      <td>260</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.063868</td>\n",
       "      <td>0.970195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.515918</td>\n",
       "      <td>840</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>0.832078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.540080</td>\n",
       "      <td>280</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>1.088005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.546334</td>\n",
       "      <td>420</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.920055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.560520</td>\n",
       "      <td>360</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.843046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.562240</td>\n",
       "      <td>680</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.966263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.547530</td>\n",
       "      <td>600</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>1.035585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.527534</td>\n",
       "      <td>800</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.972656</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.087738</td>\n",
       "      <td>0.867286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.542973</td>\n",
       "      <td>620</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>0.923764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.071405</td>\n",
       "      <td>0.677758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.550217</td>\n",
       "      <td>1320</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.135316</td>\n",
       "      <td>0.696440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.555826</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.179101</td>\n",
       "      <td>0.624284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.552272</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.224622</td>\n",
       "      <td>0.608710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.565162</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.410912</td>\n",
       "      <td>0.576544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.539851</td>\n",
       "      <td>360</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.036772</td>\n",
       "      <td>0.934315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.538967</td>\n",
       "      <td>460</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.062382</td>\n",
       "      <td>0.751192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.567504</td>\n",
       "      <td>160</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.021666</td>\n",
       "      <td>1.025139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.568039</td>\n",
       "      <td>360</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.042243</td>\n",
       "      <td>1.580782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>25</td>\n",
       "      <td>0.559682</td>\n",
       "      <td>180</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.018215</td>\n",
       "      <td>1.043107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>25</td>\n",
       "      <td>0.493861</td>\n",
       "      <td>420</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.057422</td>\n",
       "      <td>0.679152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>25</td>\n",
       "      <td>0.564331</td>\n",
       "      <td>440</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.036226</td>\n",
       "      <td>0.861008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>25</td>\n",
       "      <td>0.561832</td>\n",
       "      <td>540</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.040948</td>\n",
       "      <td>1.052402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>40</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>340</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>1.166759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>40</td>\n",
       "      <td>0.561629</td>\n",
       "      <td>320</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.044840</td>\n",
       "      <td>0.934024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>40</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>360</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.061820</td>\n",
       "      <td>0.748724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>40</td>\n",
       "      <td>0.541542</td>\n",
       "      <td>620</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.071857</td>\n",
       "      <td>1.069821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.533205</td>\n",
       "      <td>480</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.037844</td>\n",
       "      <td>0.913108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.556747</td>\n",
       "      <td>500</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.029529</td>\n",
       "      <td>0.970438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.539955</td>\n",
       "      <td>720</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.846921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.530088</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.079480</td>\n",
       "      <td>0.642136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>55</td>\n",
       "      <td>0.560379</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.177037</td>\n",
       "      <td>0.676117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>55</td>\n",
       "      <td>0.525810</td>\n",
       "      <td>1560</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.205349</td>\n",
       "      <td>0.558281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>55</td>\n",
       "      <td>0.542304</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.251045</td>\n",
       "      <td>0.576916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>55</td>\n",
       "      <td>0.573028</td>\n",
       "      <td>2320</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.425186</td>\n",
       "      <td>0.582550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd47f918-8105-404d-aaf0-58a242646fd8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cd47f918-8105-404d-aaf0-58a242646fd8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cd47f918-8105-404d-aaf0-58a242646fd8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Backbone  Multi Backbone Optim       LR Scheduler  Epochs  Batch Size  \\\n",
       "0   resnet18           False  Adam  0.00005      None      40           4   \n",
       "1   resnet18           False  Adam  0.00005      None      40           4   \n",
       "2   resnet18           False  Adam  0.00005      None      40           4   \n",
       "3   resnet18           False  Adam  0.00005      None      40           4   \n",
       "4   resnet18           False  Adam  0.00005      None      40           4   \n",
       "5   resnet18           False  Adam  0.00005      None      40           4   \n",
       "6   resnet18           False  Adam  0.00005      None      40           4   \n",
       "7   resnet18           False  Adam  0.00005      None      40           4   \n",
       "8   resnet18           False  Adam  0.00005      None      40           4   \n",
       "9   resnet18           False  Adam  0.00005      None      40           4   \n",
       "10  resnet18           False  Adam  0.00005      None      40           4   \n",
       "11  resnet18           False  Adam  0.00005      None      40           4   \n",
       "12  resnet18           False  Adam  0.00005      None      40           4   \n",
       "13  resnet18           False  Adam  0.00005      None      40           4   \n",
       "14  resnet18           False  Adam  0.00005      None      40           4   \n",
       "15  resnet18           False  Adam  0.00005      None      40           4   \n",
       "16  resnet18           False  Adam  0.00005      None      40           4   \n",
       "17  resnet18           False  Adam  0.00005      None      40           4   \n",
       "18  resnet18           False  Adam  0.00005      None      40           4   \n",
       "19  resnet18           False  Adam  0.00005      None      40           4   \n",
       "20  resnet18           False  Adam  0.00005      None      40           4   \n",
       "21  resnet18           False  Adam  0.00005      None      40           4   \n",
       "22  resnet18           False  Adam  0.00005      None      40           4   \n",
       "23  resnet18           False  Adam  0.00005      None      40           4   \n",
       "24  resnet18           False  Adam  0.00005      None      40           4   \n",
       "25  resnet18           False  Adam  0.00005      None      40           4   \n",
       "26  resnet18           False  Adam  0.00005      None      40           4   \n",
       "27  resnet18           False  Adam  0.00005      None      40           4   \n",
       "28  resnet18           False  Adam  0.00005      None      40           4   \n",
       "29  resnet18           False  Adam  0.00005      None      40           4   \n",
       "30  resnet18           False  Adam  0.00005      None      40           4   \n",
       "31  resnet18           False  Adam  0.00005      None      40           4   \n",
       "32  resnet18           False  Adam  0.00005      None      40           4   \n",
       "33  resnet18           False  Adam  0.00005      None      40           4   \n",
       "34  resnet18           False  Adam  0.00005      None      40           4   \n",
       "35  resnet18           False  Adam  0.00005      None      40           4   \n",
       "36  resnet18           False  Adam  0.00005      None      40           4   \n",
       "37  resnet18           False  Adam  0.00005      None      40           4   \n",
       "38  resnet18           False  Adam  0.00005      None      40           4   \n",
       "39  resnet18           False  Adam  0.00005      None      40           4   \n",
       "40  resnet18           False  Adam  0.00005      None      40           4   \n",
       "41  resnet18           False  Adam  0.00005      None      40           4   \n",
       "42  resnet18           False  Adam  0.00005      None      40           4   \n",
       "43  resnet18           False  Adam  0.00005      None      40           4   \n",
       "44  resnet18           False  Adam  0.00005      None      40           4   \n",
       "45  resnet18           False  Adam  0.00005      None      40           4   \n",
       "46  resnet18           False  Adam  0.00005      None      40           4   \n",
       "47  resnet18           False  Adam  0.00005      None      40           4   \n",
       "48  resnet18           False  Adam  0.00005      None      40           4   \n",
       "49  resnet18           False  Adam  0.00005      None      40           4   \n",
       "50  resnet18           False  Adam  0.00005      None      40           4   \n",
       "51  resnet18           False  Adam  0.00005      None      40           4   \n",
       "52  resnet18           False  Adam  0.00005      None      40           4   \n",
       "53  resnet18           False  Adam  0.00005      None      40           4   \n",
       "54  resnet18           False  Adam  0.00005      None      40           4   \n",
       "55  resnet18           False  Adam  0.00005      None      40           4   \n",
       "56  resnet18           False  Adam  0.00005      None      40           4   \n",
       "57  resnet18           False  Adam  0.00005      None      40           4   \n",
       "58  resnet18           False  Adam  0.00005      None      40           4   \n",
       "59  resnet18           False  Adam  0.00005      None      40           4   \n",
       "\n",
       "    Dropout Augmentation  Weight Decay  Freeze Layers  Best Eval Loss  \\\n",
       "0       0.0         None       0.00000             10        0.524519   \n",
       "1       0.1         None       0.00000             10        0.523889   \n",
       "2       0.2         None       0.00000             10        0.506876   \n",
       "3       0.5         None       0.00000             10        0.516705   \n",
       "4       0.0         None       0.00000             25        0.576749   \n",
       "5       0.1         None       0.00000             25        0.534919   \n",
       "6       0.2         None       0.00000             25        0.561822   \n",
       "7       0.5         None       0.00000             25        0.520022   \n",
       "8       0.0         None       0.00000             40        0.549737   \n",
       "9       0.1         None       0.00000             40        0.570477   \n",
       "10      0.2         None       0.00000             40        0.565983   \n",
       "11      0.5         None       0.00000             40        0.524916   \n",
       "12      0.0         None       0.00000             50        0.572738   \n",
       "13      0.1         None       0.00000             50        0.521608   \n",
       "14      0.2         None       0.00000             50        0.564970   \n",
       "15      0.5         None       0.00000             50        0.556237   \n",
       "16      0.0         None       0.00000             55        0.550112   \n",
       "17      0.1         None       0.00000             55        0.562151   \n",
       "18      0.2         None       0.00000             55        0.558726   \n",
       "19      0.5         None       0.00000             55        0.555775   \n",
       "20      0.0         None       0.00100             10        0.537286   \n",
       "21      0.1         None       0.00100             10        0.571937   \n",
       "22      0.2         None       0.00100             10        0.547095   \n",
       "23      0.5         None       0.00100             10        0.527522   \n",
       "24      0.0         None       0.00100             25        0.572477   \n",
       "25      0.1         None       0.00100             25        0.546448   \n",
       "26      0.2         None       0.00100             25        0.534575   \n",
       "27      0.5         None       0.00100             25        0.515918   \n",
       "28      0.0         None       0.00100             40        0.540080   \n",
       "29      0.1         None       0.00100             40        0.546334   \n",
       "30      0.2         None       0.00100             40        0.560520   \n",
       "31      0.5         None       0.00100             40        0.562240   \n",
       "32      0.0         None       0.00100             50        0.547530   \n",
       "33      0.1         None       0.00100             50        0.527534   \n",
       "34      0.2         None       0.00100             50        0.542973   \n",
       "35      0.5         None       0.00100             50        0.529097   \n",
       "36      0.0         None       0.00100             55        0.550217   \n",
       "37      0.1         None       0.00100             55        0.555826   \n",
       "38      0.2         None       0.00100             55        0.552272   \n",
       "39      0.5         None       0.00100             55        0.565162   \n",
       "40      0.0         None       0.00001             10        0.539851   \n",
       "41      0.1         None       0.00001             10        0.538967   \n",
       "42      0.2         None       0.00001             10        0.567504   \n",
       "43      0.5         None       0.00001             10        0.568039   \n",
       "44      0.0         None       0.00001             25        0.559682   \n",
       "45      0.1         None       0.00001             25        0.493861   \n",
       "46      0.2         None       0.00001             25        0.564331   \n",
       "47      0.5         None       0.00001             25        0.561832   \n",
       "48      0.0         None       0.00001             40        0.519186   \n",
       "49      0.1         None       0.00001             40        0.561629   \n",
       "50      0.2         None       0.00001             40        0.510254   \n",
       "51      0.5         None       0.00001             40        0.541542   \n",
       "52      0.0         None       0.00001             50        0.533205   \n",
       "53      0.1         None       0.00001             50        0.556747   \n",
       "54      0.2         None       0.00001             50        0.539955   \n",
       "55      0.5         None       0.00001             50        0.530088   \n",
       "56      0.0         None       0.00001             55        0.560379   \n",
       "57      0.1         None       0.00001             55        0.525810   \n",
       "58      0.2         None       0.00001             55        0.542304   \n",
       "59      0.5         None       0.00001             55        0.573028   \n",
       "\n",
       "    Best Eval Step  Best Eval Acc  Final Train Acc  Final Eval Acc  \\\n",
       "0              340       0.586207         0.957031        0.551724   \n",
       "1              540       0.655172         0.988281        0.448276   \n",
       "2              340       0.620690         0.988281        0.482759   \n",
       "3              600       0.620690         0.957031        0.379310   \n",
       "4               60       0.551724         1.000000        0.482759   \n",
       "5              280       0.586207         0.968750        0.482759   \n",
       "6              220       0.586207         0.980469        0.551724   \n",
       "7              840       0.586207         0.984375        0.482759   \n",
       "8              340       0.586207         0.992188        0.448276   \n",
       "9              360       0.586207         0.996094        0.517241   \n",
       "10             340       0.620690         0.996094        0.482759   \n",
       "11             820       0.655172         0.972656        0.482759   \n",
       "12             340       0.551724         1.000000        0.517241   \n",
       "13             680       0.620690         0.992188        0.551724   \n",
       "14             780       0.586207         0.984375        0.551724   \n",
       "15             880       0.586207         0.984375        0.448276   \n",
       "16            1340       0.586207         0.960938        0.517241   \n",
       "17            1560       0.517241         0.960938        0.413793   \n",
       "18            1800       0.586207         0.917969        0.482759   \n",
       "19            2100       0.551724         0.691406        0.551724   \n",
       "20             200       0.655172         0.996094        0.517241   \n",
       "21             200       0.586207         0.992188        0.482759   \n",
       "22             340       0.586207         0.984375        0.482759   \n",
       "23             700       0.586207         0.968750        0.448276   \n",
       "24             280       0.586207         0.996094        0.517241   \n",
       "25             440       0.620690         0.988281        0.517241   \n",
       "26             260       0.620690         0.980469        0.517241   \n",
       "27             840       0.620690         0.980469        0.517241   \n",
       "28             280       0.586207         0.992188        0.448276   \n",
       "29             420       0.620690         0.976562        0.482759   \n",
       "30             360       0.586207         0.968750        0.517241   \n",
       "31             680       0.586207         0.984375        0.379310   \n",
       "32             600       0.586207         0.988281        0.517241   \n",
       "33             800       0.620690         0.972656        0.551724   \n",
       "34             620       0.586207         0.980469        0.517241   \n",
       "35            1500       0.655172         0.992188        0.586207   \n",
       "36            1320       0.551724         0.988281        0.448276   \n",
       "37            1200       0.551724         0.941406        0.482759   \n",
       "38            1680       0.551724         0.937500        0.482759   \n",
       "39            1740       0.551724         0.753906        0.551724   \n",
       "40             360       0.586207         0.984375        0.482759   \n",
       "41             460       0.620690         0.984375        0.517241   \n",
       "42             160       0.586207         0.996094        0.517241   \n",
       "43             360       0.551724         0.988281        0.482759   \n",
       "44             180       0.551724         1.000000        0.517241   \n",
       "45             420       0.586207         0.976562        0.413793   \n",
       "46             440       0.551724         0.988281        0.517241   \n",
       "47             540       0.586207         0.984375        0.448276   \n",
       "48             340       0.620690         1.000000        0.482759   \n",
       "49             320       0.551724         0.984375        0.482759   \n",
       "50             360       0.655172         0.976562        0.586207   \n",
       "51             620       0.586207         0.976562        0.482759   \n",
       "52             480       0.586207         0.992188        0.517241   \n",
       "53             500       0.586207         0.996094        0.586207   \n",
       "54             720       0.586207         0.996094        0.517241   \n",
       "55            1620       0.620690         0.988281        0.517241   \n",
       "56            1220       0.551724         0.937500        0.482759   \n",
       "57            1560       0.586207         0.949219        0.551724   \n",
       "58            1620       0.586207         0.937500        0.586207   \n",
       "59            2320       0.517241         0.734375        0.482759   \n",
       "\n",
       "    Final Train Loss  Final Eval Loss  \n",
       "0           0.071135         0.911740  \n",
       "1           0.027901         1.073272  \n",
       "2           0.042024         0.697343  \n",
       "3           0.116061         0.976772  \n",
       "4           0.022749         1.270029  \n",
       "5           0.061553         0.981142  \n",
       "6           0.056390         0.835536  \n",
       "7           0.054673         0.836594  \n",
       "8           0.033070         0.922413  \n",
       "9           0.022764         1.286257  \n",
       "10          0.024311         0.862905  \n",
       "11          0.077674         0.798949  \n",
       "12          0.020041         1.126971  \n",
       "13          0.036462         0.826011  \n",
       "14          0.059667         0.950224  \n",
       "15          0.087918         0.758252  \n",
       "16          0.160574         0.613796  \n",
       "17          0.177153         0.627600  \n",
       "18          0.243365         0.572039  \n",
       "19          0.456001         0.565197  \n",
       "20          0.023849         1.020833  \n",
       "21          0.026305         1.228819  \n",
       "22          0.049095         0.910337  \n",
       "23          0.080304         0.896574  \n",
       "24          0.022526         0.933137  \n",
       "25          0.056658         1.003697  \n",
       "26          0.063868         0.970195  \n",
       "27          0.056825         0.832078  \n",
       "28          0.022619         1.088005  \n",
       "29          0.046562         0.920055  \n",
       "30          0.065104         0.843046  \n",
       "31          0.059261         0.966263  \n",
       "32          0.053464         1.035585  \n",
       "33          0.087738         0.867286  \n",
       "34          0.067168         0.923764  \n",
       "35          0.071405         0.677758  \n",
       "36          0.135316         0.696440  \n",
       "37          0.179101         0.624284  \n",
       "38          0.224622         0.608710  \n",
       "39          0.410912         0.576544  \n",
       "40          0.036772         0.934315  \n",
       "41          0.062382         0.751192  \n",
       "42          0.021666         1.025139  \n",
       "43          0.042243         1.580782  \n",
       "44          0.018215         1.043107  \n",
       "45          0.057422         0.679152  \n",
       "46          0.036226         0.861008  \n",
       "47          0.040948         1.052402  \n",
       "48          0.007934         1.166759  \n",
       "49          0.044840         0.934024  \n",
       "50          0.061820         0.748724  \n",
       "51          0.071857         1.069821  \n",
       "52          0.037844         0.913108  \n",
       "53          0.029529         0.970438  \n",
       "54          0.030664         0.846921  \n",
       "55          0.079480         0.642136  \n",
       "56          0.177037         0.676117  \n",
       "57          0.205349         0.558281  \n",
       "58          0.251045         0.576916  \n",
       "59          0.425186         0.582550  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We save the results of the training sweep \n",
    "df = pd.DataFrame(Sweep_Summary)\n",
    "df.to_csv(\"Fish_Classifier_Sweep_Results.csv\")\n",
    "!cp \"/content/Fish_Classifier_Sweep_Results.csv\" \"/content/drive/MyDrive\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN197toL8YdE"
   },
   "source": [
    "#Evaluation\n",
    "In this section we load the ttrained model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3RdeuEJ8Wnm",
    "outputId": "ec91b89d-52c0-46d9-c219-88803269a33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5953 Acc: 0.5357\n",
      "Pred: [1 1 1 2 1 1 1 2 1 1 1 2 2 1 1 1 0 1 1 1 1 1 1 2 1 2 1 1 1 2 1 0 1 2 1 1 1\n",
      " 2 2 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1]\n",
      "True: [1 0 0 2 1 1 0 2 1 0 0 1 2 0 0 2 1 1 1 2 0 1 1 2 2 2 0 1 1 1 2 0 2 1 1 1 1\n",
      " 2 0 1 1 0 1 1 2 1 0 1 0 0 1 0 0 1 1 2]\n",
      "Confusion Matrix:\n",
      "[[ 1 14  2]\n",
      " [ 1 22  3]\n",
      " [ 0  6  7]]\n"
     ]
    }
   ],
   "source": [
    "# Test model loading\n",
    "TRAINED_MODEL_PATH = '/content/model0.001500.001.pt'\n",
    "\n",
    "saved_model = torch.load(TRAINED_MODEL_PATH)\n",
    "saved_model = torch.load('model.pt')\n",
    "saved_model.eval()\n",
    "test_running_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "dl = test_dl\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in dl:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = saved_model(inputs,is_training=False)\n",
    "    loss = criterion(outputs, labels.squeeze(1).float())\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    all_preds.extend(preds.cpu().tolist())\n",
    "    all_labels.extend([label[0] for label in labels.cpu().tolist()])\n",
    "\n",
    "\n",
    "    test_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.argmax(all_labels, axis = 1)\n",
    "\n",
    "eval_loss = test_running_loss / len(dl.dataset)\n",
    "eval_acc = accuracy_score(all_labels, all_preds)\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Eval\", eval_loss, eval_acc))\n",
    "print(f'Pred: {all_preds}')\n",
    "print(f'True: {all_labels}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SnSMTbvaKRI7"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "torch.cuda.empty_cache()\n",
    "model = Classifier(dropout_rate=dropout_rate,backbone=BACKBONE,multi_backbone=True).to(device)\n",
    "summary(model, (4, 3, 200, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43P_oEKSFDqH"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WsFblgnUKAGx"
   },
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WeISDPwmagUp"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pytorch_fish_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
