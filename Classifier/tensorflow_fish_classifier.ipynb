{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_fish_classifier.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Requirements\n",
        "In this secttion we import the required packages for training our classifier.\n"
      ],
      "metadata": {
        "id": "0M52LGzqdszw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt"
      ],
      "metadata": {
        "id": "e5jz8UxbdxSw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We mount the google drive. If You are running this notebook locally do nott run this cell. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4WHaUIxd_LO",
        "outputId": "6e156dac-0106-4312-8068-d2787ed1fc03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurations\n",
        "In this section we define the configs for training. "
      ],
      "metadata": {
        "id": "HVJblvMveHI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"Allele B Cropped\"\n",
        "X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_b.npy\"\n",
        "Y_PATH= \"/content/drive/Shareddrives/Exploding Gradients/y_b.npy\"\n",
        "\n",
        "\n",
        "BACKBONE = \"resnet50\"\n",
        "MULTI_BACKBONE = True\n",
        "OPTIM = \"Adam\"\n",
        "LR =5e-5\n",
        "SCHEDULER = \"None\"\n",
        "EPOCHS = 40\n",
        "BATCHSIZE = 4\n",
        "AUGMENTATION = \"None\"\n",
        "\n",
        "#The following is a list of hyper parameters to test. All Permuttations will be\n",
        "#tested\n",
        "\n",
        "DROPOUT = [0,0.1,0.2,0.5]\n",
        "WEIGHT_DECAY = [0,1e-3,1e-5]\n",
        "FREEZE = [10,25,40,50,55]"
      ],
      "metadata": {
        "id": "cj3DWZdseID4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing\n",
        "In this section, we read the dataset as a pre saved numpy array. After reading the datset. we divide it into train-testtt sets. We tthen create a pytorch dataset which we will then turn into a dataloader.\n"
      ],
      "metadata": {
        "id": "t6dc7L3zd3co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We read the \n",
        "x = np.load(X_PATH)\n",
        "y = np.load(Y_PATH)\n",
        "y = np.squeeze(y.astype(np.int16))\n",
        "b = np.zeros((y.size, y.max()+1))\n",
        "b[np.arange(y.size),y] = 1\n",
        "y = b\n",
        "print(\"X Tensor Shape: \",x.shape)\n",
        "print(\"y Tensor Shape: \",y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB7nRPaCdzdw",
        "outputId": "c11f8c70-fc08-4a39-8cb4-d612fe4c3877"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Tensor Shape:  (285, 4, 200, 1024, 3)\n",
            "y Tensor Shape:  (285, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "Code for tensorflow model."
      ],
      "metadata": {
        "id": "oOYIWLCceuf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (200, 1024, 3)\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.backbone = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
        "                                                       include_top=False,\n",
        "                                                       weights='imagenet')\n",
        "        self.classifier= tf.keras.Sequential([\n",
        "                                                  \n",
        "                                                   tf.keras.layers.Dense(3)\n",
        "        ])\n",
        "       \n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        images = [inputs[:,i,:,:,:] for i in range(4)]\n",
        "        encodings = [tf.reshape(self.backbone(img),[inputs.shape[0],-1]) for img in images]\n",
        "        encodings = tf.concat(encodings,1)\n",
        "\n",
        "        return self.classifier(encodings)\n",
        "\n"
      ],
      "metadata": {
        "id": "YypYfUibez3H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "8SdJVSMAuOIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classifier()\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001,),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(from_logits = True))\n",
        "\n",
        "model.fit(x[0:232],y[0:232],4,epochs=4,validation_data=(x[232:264],y[232:264]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwi6kbByoKtF",
        "outputId": "097d710b-d35d-464a-8820-5a3e4b029f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "58/58 [==============================] - 42s 500ms/step - loss: 4.8105 - val_loss: 3.7167\n",
            "Epoch 2/4\n",
            "12/58 [=====>........................] - ETA: 19s - loss: 0.9276"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gg89KqDkv4jX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}