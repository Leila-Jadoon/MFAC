{"cells":[{"cell_type":"markdown","source":["## Label Generation\n","\n","The Label Generation notebook is responsible for creating the classifier predicted labels and saving them to a specified directory in *.npy form. It is assumed you ALREADY HAVE THE TRUE LABELS of the dataset in *.npy form.\n","\n","The notebook can also convert the classifier and predicted labels to text files, where the first column indicates the index of the instance of data that the label is associated with,  and the second column is the actual label (0, 1, or 2). The instance data is assumed to be in *.npy form as well, although all notebooks autoconvert from the saved numpy form to a PyTorch tensor. The index can then be used in the raw tensor or a PyTorch dataset to obtain the actual instance.\n","\n","## Inputs\n","\n"],"metadata":{"id":"Dh49CErdW606"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8meGkiDcGcH4"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","\n","%matplotlib inline \n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import random\n","import math\n","import string\n","\n","import tqdm\n","from tqdm.auto import tqdm\n","\n","import scipy.stats as stats\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.mixture import GaussianMixture\n","\n","from skimage.filters import sobel\n","from skimage.color import rgb2gray\n","\n","# TPU support\n","## Insufficient testing has been done to see what methods support using the TPU but the ability to enable the TPU has been provided here.\n","## Ensure that the Runtime for the Colab notebook has \"TPU\" selected prior to use!\n","#!pip install -q cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-Qmf0zGnPmI"},"outputs":[],"source":["from enum import Enum\n","\n","class Backend(Enum):\n","  CPU = \"cpu\"\n","  GPU = \"cuda:0\"\n","  TPU = \"tpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1653757250683,"user":{"displayName":"John Long","userId":"00584768646093006387"},"user_tz":420},"id":"PMJitkpOG0KH","outputId":"bad01c63-22a2-4fc5-85b3-5714b2cb82e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Selected, and confirmed available!\n"]}],"source":["## Control Flow Variables\n","BACKEND = Backend.GPU\n","BATCH_SIZE = 4\n","TEXT_LABELS_NEEDED = False\n","\n","MODEL_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/model1e-050.5.2022-05-22 12_13_10.pt\"\n","X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_cropped_b.npy\"\n","TRUE_LABELS_PATH = \"/content/drive/Shareddrives/Exploding Gradients/y_b.npy\"\n","CLASSIFIER_GENERATED_LABELS_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/predicted_labels.npy\"\n","TRUE_LABELS_TEXT_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/true-labels.txt\"\n","CLASSIFIER_GENERATED_LABELS_TEXT_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/predicted-labels.txt\"\n","\n","# Check that GPU is available, if it isn't then BACKEND will automatically toggle to CPU\n","if(BACKEND == Backend.GPU):\n","  if(torch.cuda.is_available()):\n","    print(\"GPU Selected, and confirmed available!\")\n","    device = torch.device(\"cuda:0\")\n","  else:\n","    print(\"GPU Selected, but not found! Switching to CPU for backend\")\n","    BACKEND = Backend.CPU\n","    device = torch.device(\"cpu\")\n","elif(BACKEND == Backend.TPU):\n","  assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n","  print(\"TPU Selected\")\n","  device = xm.xla_device()\n","elif(BACKEND == Backend.CPU):\n","  print(\"CPU Selected\")\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","source":["#This function takes in a model and replaces inplace relu layers to an independent relu layer\n","def reluToInplaceFalse(model):\n","  for name, child in model.named_children():\n","    if isinstance(child, nn.ReLU):\n","      setattr(child, 'inplace', False)\n","    else:\n","      reluToInplaceFalse(child)"],"metadata":{"id":"nTxKyFplltHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Model Class\n","TARGET_WIDTH = 750\n","TARGET_HEIGHT = 130\n","\n","from torchvision.transforms.transforms import RandomRotation, RandomAdjustSharpness, RandomGrayscale\n","import torchvision.transforms.functional as tf\n","\n","def init_weights(m):\n","  if isinstance(m, nn.Linear):\n","    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n","\n","\n","class Classifier(torch.nn.Module):\n","\n","  def __init__(self, backbone='resnet', multi_backbone = False, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH):\n","    super().__init__()\n","    self.multi_backbone = multi_backbone # Bool: Indicates if we use multibackbone\n","\n","    #In the following section we download the appropriate prettrained model\n","    if backbone == \"vgg19\":\n","      backbone = torchvision.models.vgg19(pretrained=True)\n","      self.out_channels = 25088\n","      \n","    elif backbone == \"resnet18\":\n","      backbone = torchvision.models.resnet18(pretrained=True)\n","      self.out_channels = 512\n","\n","    elif backbone == \"resnet50\":\n","      backbone = torchvision.models.resnet50(pretrained=True)\n","      self.out_channels = 2048\n","\n","    elif backbone == \"Efficientnet b1\":\n","      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n","      self.out_channels = 1280\n","\n","    elif backbone == \"Efficientnet b3\":\n","      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n","      self.out_channels = 1536\n","\n","    elif backbone == \"Efficientnet b5\":\n","      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n","      self.out_channels = 2048\n","\n","    elif backbone == \"Efficientnet b7\":\n","      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n","      self.out_channels = 2560\n","    else:\n","      raise ValueError(f'Invalid backbone \"{backbone}\"')\n","      \n","    # Disabling inplace ReLu becasuse GradCam doesn't work it enabled\n","    reluToInplaceFalse(backbone)\n","     \n","    modules = list(backbone.children())[:-1]\n","\n","    if self.multi_backbone: #We create the backbones and put them on the device\n","      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","\n","    else:\n","      self.backbone =  nn.Sequential(*modules).to(device)\n","\n","    self.do_augmentation = do_augmentation\n","\n","    # Note: These are not all of the augmnetations performed, see custom_augmentation()\n","    self.unlabeled_augmentation = nn.Sequential(transforms.RandomVerticalFlip(0.5),\n","                                      transforms.RandomCrop(size=(target_height,target_width)),\n","                                      transforms.RandomRotation(10, interpolation=transforms.InterpolationMode.BILINEAR, fill=1),\n","                                      transforms.Normalize(0, 1)\n","    )\n","\n","    self.bottleneck_dim = 256\n","\n","    # This is the linear layer to compress each backbone\n","    self.fc_bb = nn.Sequential(nn.BatchNorm1d(self.out_channels),\n","                               nn.Dropout(dropout_rate),\n","                               nn.Linear(self.out_channels, self.bottleneck_dim),\n","                               nn.BatchNorm1d(self.bottleneck_dim),\n","                               nn.ReLU())\n","    self.fc_bb.apply(init_weights)\n","\n","    self.fc_hflip1 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip1.apply(init_weights)\n","\n","    self.fc_hflip2 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip2.apply(init_weights)\n","\n","    self.fc_hflip3 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip3.apply(init_weights)\n","\n","    self.fc_hflip4 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip4.apply(init_weights)\n","\n","    #This is the final classification layer\n","    self.fc = nn.Sequential(nn.Dropout(dropout_rate),\n","                            nn.Linear(self.bottleneck_dim * 4, 3))\n","    self.fc.apply(init_weights)\n","\n","    # A softmax is applied in eval mode\n","    self.softmax = nn.Softmax(dim=1)              \n","     \n","  def forward(self, x):\n","    if self.do_augmentation and self.training:\n","      imgs, hflip_labels = map(list, zip(*[self.custom_augmentation(x[:,i]) for i in range(4)])) #list of 4 images\n","      hflip_labels = [torch.Tensor(hflip_label).float().unsqueeze(1) for hflip_label in hflip_labels]\n","    else:\n","      imgs = [x[:,i] for i in range(4)] #list of 4 images\n","      hflip_labels = None\n","    \n","    if self.multi_backbone:\n","      encodings = [self.fc_bb(self.backbone1(imgs[0]).flatten(1)), \n","                   self.fc_bb(self.backbone2(imgs[1]).flatten(1)),\n","                   self.fc_bb(self.backbone3(imgs[2]).flatten(1)),\n","                   self.fc_bb(self.backbone4(imgs[3]).flatten(1))]\n","    else:\n","      encodings = [self.fc_bb(self.backbone(img).flatten(1)) for img in imgs]\n","\n","    logits = self.fc(torch.cat(encodings,1))\n","    if self.training:\n","      # get hflip predictions\n","      hflip_preds = [self.fc_hflip1(encodings[0]),\n","                     self.fc_hflip2(encodings[1]),\n","                     self.fc_hflip3(encodings[2]),\n","                     self.fc_hflip4(encodings[3])]\n","      hflip_preds = torch.cat(hflip_preds, 1)\n","      hflip_labels = torch.cat(hflip_labels, 1).to(device)\n","      return logits, hflip_preds, hflip_labels\n","    else:\n","      return self.softmax(logits)\n","\n","  def custom_augmentation(self, images):\n","    hflip_labels = np.random.choice([0, 1], size = images.size(0))\n","    for i, hflip_label in enumerate(hflip_labels):\n","      if hflip_label == 1:\n","        images[i] = tf.hflip(images[i])\n","    images = self.unlabeled_augmentation(images)\n","    return images, hflip_labels"],"metadata":{"id":"5FvnCr2cluwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14075,"status":"ok","timestamp":1653757272241,"user":{"displayName":"John Long","userId":"00584768646093006387"},"user_tz":420},"id":"7SVm11zwnZ2m","outputId":"ddafc51a-1827-4ffc-a9cc-bab8d05527f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Load the model\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","model = torch.load(MODEL_PATH, map_location=\"cpu\")\n","model.eval()\n","model.zero_grad()\n","\n","# Put the model onto the device\n","model.to(device);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32855,"status":"ok","timestamp":1653757310648,"user":{"displayName":"John Long","userId":"00584768646093006387"},"user_tz":420},"id":"gTZAsiaFrvmT","outputId":"788056dc-4f34-431f-bcc9-6df051bdf2bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["(285, 4, 130, 750, 3)\n","(285, 1)\n"]}],"source":[" # Prepare data\n","x = np.load(X_PATH)\n","y = np.load(TRUE_LABELS_PATH)\n","\n","print(x.shape)\n","print(y.shape)\n","\n","tensor_x = torch.Tensor(x) \n","tensor_y = torch.Tensor(y).long()\n","\n","tensor_x = torch.swapaxes(tensor_x,2,4)\n","tensor_x = torch.swapaxes(tensor_x,3,4)\n","\n","from torch.utils.data import TensorDataset\n","\n","attribution_ds = TensorDataset(tensor_x) \n","attribution_dl = DataLoader(attribution_ds, BATCH_SIZE ,shuffle = False, pin_memory=True)\n","del x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69,"referenced_widgets":["12cd2cd5231945e4872b6b1391c72daf","f41726446ac54150aa37b6cab29370f4","1e354e928e0e4e36918fb6582ffe5f58","72007652945f4f31ad19081a25d3f6cc","8c1f33617af14ab39402c8716b69b363","3fdf18ffe03647538dbba492b99ed70d","21a21b93f5964b60844a922b8fd2c6f7","20010f859e50481cad8c7972ea2e21df","bb9e4242a6ce4e51b56b1eafe2deaa7f","fdf43d51de65438ea4ff8918ffd41ee6","e867e5c0ae7349fb9ace0e96ecff898c"]},"executionInfo":{"elapsed":10009,"status":"ok","timestamp":1653757346726,"user":{"displayName":"John Long","userId":"00584768646093006387"},"user_tz":420},"id":"FnI5Ao5kHdbo","outputId":"692085fe-1e60-460f-b4b3-942b2b6fb27c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/72 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12cd2cd5231945e4872b6b1391c72daf"}},"metadata":{}}],"source":["classifier_generated_labels = []\n","\n","for (images,) in tqdm(attribution_dl):\n","  # send images to the device\n","  images = images.to(device)\n","  # perform attributions\n","  for label in model(images):\n","    classifier_generated_labels.append(torch.argmax(label))\n","  # append attributions\n","  #classifier_generated_labels.append(label.cpu())\n","  # delete images, labels, and batch attributions from memory\n","  del images, label\n","  # If things were run on the GPU, empty the cache to prevent memory overloading\n","  if(BACKEND == Backend.GPU):\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqrTaowfLZim"},"outputs":[],"source":["np.save(CLASSIFIER_GENERATED_LABELS_PATH,\n","        torch.unsqueeze(torch.Tensor(classifier_generated_labels), 1).numpy())"]},{"cell_type":"code","source":["if(TEXT_LABELS_NEEDED):\n","  true_labels = np.squeeze(np.load(TRUE_LABELS_PATH)).astype(int)\n","  predicted_labels = np.squeeze(np.load(CLASSIFIER_GENERATED_LABELS_PATH)).astype(int)\n","  with open(TRUE_LABELS_TEXT_PATH,'a') as file:\n","    for idx, label in enumerate(true_labels):\n","      file.write(str(idx) + \" \" + str(label) + \"\\n\")\n","  with open(PREDICTED_LABELS_TEXT_PATH, 'a') as file:\n","    for idx, label in enumerate(predicted_labels):\n","      file.write(str(idx) + \" \" + str(label) + \"\\n\")"],"metadata":{"id":"NMIV0gPbuLy5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Label Generation","provenance":[],"authorship_tag":"ABX9TyPEqWI+lVbHCPMhqY8wPaBP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"12cd2cd5231945e4872b6b1391c72daf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f41726446ac54150aa37b6cab29370f4","IPY_MODEL_1e354e928e0e4e36918fb6582ffe5f58","IPY_MODEL_72007652945f4f31ad19081a25d3f6cc"],"layout":"IPY_MODEL_8c1f33617af14ab39402c8716b69b363"}},"f41726446ac54150aa37b6cab29370f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fdf18ffe03647538dbba492b99ed70d","placeholder":"​","style":"IPY_MODEL_21a21b93f5964b60844a922b8fd2c6f7","value":"100%"}},"1e354e928e0e4e36918fb6582ffe5f58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20010f859e50481cad8c7972ea2e21df","max":72,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb9e4242a6ce4e51b56b1eafe2deaa7f","value":72}},"72007652945f4f31ad19081a25d3f6cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdf43d51de65438ea4ff8918ffd41ee6","placeholder":"​","style":"IPY_MODEL_e867e5c0ae7349fb9ace0e96ecff898c","value":" 72/72 [00:10&lt;00:00,  7.67it/s]"}},"8c1f33617af14ab39402c8716b69b363":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fdf18ffe03647538dbba492b99ed70d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21a21b93f5964b60844a922b8fd2c6f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20010f859e50481cad8c7972ea2e21df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb9e4242a6ce4e51b56b1eafe2deaa7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdf43d51de65438ea4ff8918ffd41ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e867e5c0ae7349fb9ace0e96ecff898c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}