{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature Attribution Data Generation","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5dE9S3s998FgS4opGSAHg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"aabeb4e1478348d89ca6ab0fca90e285":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a8860a6f48c482ab4978811c4e01523","IPY_MODEL_fde22e388a184edd972ef9bf19330636","IPY_MODEL_3ce2f73eac6b4649979e63f4579f972f"],"layout":"IPY_MODEL_288241f3dbec4c06ba848f3c3a31441c"}},"3a8860a6f48c482ab4978811c4e01523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_088fccf373aa4c4fb437263679ecaac0","placeholder":"​","style":"IPY_MODEL_d5293a9592574d5cad11ac4430113582","value":"100%"}},"fde22e388a184edd972ef9bf19330636":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_debe542aac18490e831ad994dc4255b0","max":285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4fe26212fd44cb7af3aab7390290816","value":285}},"3ce2f73eac6b4649979e63f4579f972f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_605d03db7d344f37a313ec8d1cb8f73a","placeholder":"​","style":"IPY_MODEL_ca9f21f6c410444aae68cf64ab484a43","value":" 285/285 [02:35&lt;00:00,  1.78it/s]"}},"288241f3dbec4c06ba848f3c3a31441c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"088fccf373aa4c4fb437263679ecaac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5293a9592574d5cad11ac4430113582":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"debe542aac18490e831ad994dc4255b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4fe26212fd44cb7af3aab7390290816":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"605d03db7d344f37a313ec8d1cb8f73a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca9f21f6c410444aae68cf64ab484a43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Feature Attribution Data Generation\n","\n","This notebook is the main workhorse for the Feature Attribution pipeline, responsible for performing the feature attribution process that the \"Feature Attribution Image Generation\" and \"Feature Attribution Bokeh Generation\" notebooks will rely on, depending on what output you would like to see.\n","\n","## Known Incompatibilities/Computational Intensity Issues\n","\n","It should be noted that not all classification models support all attribution methods (or are practical with the standard computational resources provided by Colab/Colab Pro. Consider Colab Pro+ or downloading all necessary notebooks and running locally.)\n","\n","* Any ResNet-based model is incompatible with DeepLift over a known issue with repeated ReLU use (refer to the official Captum Github Issue here: https://github.com/pytorch/captum/issues/378)\n","* LRP (Layer-based Relevance Propagation) does not work with EfficientNet. Even after removal of augmentation layers, it seems to fail upon detecting SiLU layers.\n","* Methods such as Shapley Value Sampling and Occlusion take a great deal of time, this can be reduced by either:\n","  * Using a larger mask, in which case you sacrifice the granularity of the prediction for faster evaluation time\n","  * Using larger strides, in the case of Occlusion\n","* There is an option to *try* to run the process on Google's TPU (Tensor Processing Unit) from an attempt during development to see if it would be any faster but this fails.\n","* Integrated Gradients can be so memory intensive it ends up causing Colab to auto-terminate. This seems to be dependent on which model is being used.\n","\n","## Input \n","\n","### Required Arguments\n","* `NUMBER_TO_PROCESS` (integer or None. If integer, should NEVER EXCEED the total number of instances of data) - The number of data instances to perform attribution on. This is useful for debugging purposes or if you only want to obtain the attributions for the first n pieces of data. If this is set to None then all data is processed. \n","* `MODEL_PATH` (string) - A path to the saved classifier model, should saved as a *.pt file\n","* `X_PATH` (string) - A path to the dataset containing the images to be classified, should be saved in *.npy format\n","* `Y_PATH` (string) - A path to the TRUE labels of the dataset, should be stored as a *.npy file\n","& `ATTRIBUTIONS_PATH` (string)- A path to where the tensor containing all the attribution data should be saved, the resulting file with be saved as a *.pt file.\n","\n","### Optional Arguments\n","* `BATCH_SIZE` (integer) (default value of 1) - The number of data instances to load onto the selected backend for processing. This is set to 1 by default.\n","* `ATTRIBUTION_METHOD`  (AttributionMethods Enum) (default value of AttributionMethod.DECONVOLUTION) - Selects the method that the attribution should be done on. The ATTRIBUTION_METHOD variable can be set to any of the following:\n","  * `AttributionMethods.INTEGRATED_GRADIENTS` - If this is being used please considering modifying the values for better performance/accuracy of variables prefixed with (INTEGRATED_GRADIENTS)\n","  * `AttributionMethods.DEEPLIFT`\n","  * `AttributionMethods.SALIENCY_MAPS`\n","  * `AttributionMethods.INPUT_X_GRADIENT`\n","  * `AttributionMethods.GUIDED_BACKPROP`\n","  * `AttributionMethods.DECONVOLUTION`\n","  * `AttributionMethods.LRP` - Note: this has NOT been thoroughly tested and per the prior text, does not seem to work with EfficientNet. Its behavior with ResNet is unknown.\n","  * `AttributionMethods.OCCLUSION` - If this is being used please considering modifying the values for better performance/accuracy of variables prefixed with OCCLUSION\n","  * `AttributionMethods.SHAPLEY_VALUE_SAMPLING` - If this is being used please considering modifying the values for better performance/accuracy of variables prefixed with SHAPLEY_VALUE_SAMPLING\n","* `ATTRIBUTION_TARGET_MODE` (AttributionTargetMode Enum) - Determines if the attributions should be done against a single class target (specified by the Y_PATH variable) or against all classes (0, 1, and 2). This can be controlled by setting the `ATTRIBUTION_TARGET_MODE` variable to the following:\n","  * `AttributionTargetMode.SINGLE_CLASS`\n","  * `AttributionTargetMOde.ALL_CLASS`\n","* `BACKEND` (Backend Enum) - The device that the feature attribution will be done on. This  variable must be set equal to an Enum from the Backend class (ex: BACKEND = Backends.GPU). BACKEND can be set to the following:\n","  * `Backends.CPU`\n","  * `Backends.GPU`\n","  * `Backends.TPU`: WARNING the TPU option has NOT been thoroughly tested and is a byproduct of attempting to find faster methods for Feature Attribution. It is highly recommended to NOT use this setting. If you would like to test it, make sure that the commented lines in the library import Cell are uncommented to install the necessary dependencies as recommended by Google.\n","* `OCCLUSION_SLIDING_WINDOWS_SHAPES` (4 Integer tuple) (Default value of (1, 3, 26, 75) - The shape to Occlude. This should have dimensions of 1 - input tensor. The input tensor to the classifier is (1, 4, 3, 130, 750) so when creating the sliding window we drop the “1”. Thus, the window occupies 1 of the 4 images, covers all 3 color channels (we aren’t interested in occluding individual color channels), and covers a 26 x 75 subsection of the 130 x 750 image.\n","* `OCCLUSION_STRIDES` (4 Integer tuple) (Default value of (1, 3, 26, 75)) - This should follow similar dimensions to whatever was provided to OCCLUSION_SLIDING_WINDOWS_SHAPES. This specifies how much to slide the window by. The default arguments here do not overlap with the previous occlusion, so once 26 x 75 pixels have been accounted for, we immediately move to the next untouched 26 x 75 pixels. \n","* `OCCLUSION_PERTURBATIONS_PER_EVAL` (Integer) (Default value of 160) - how many occlusions should be passed to the classifier for processing. If there is sufficient GPU/System RAM this number should be made as large as possible to speed up processing.\n","* `SHAPLEY_VALUE_SAMPLING_MASK` (numpy array) - this should be set to a numpy array that has the same dimensions as a single instance of data, but which contains integers indicating which pixels are to be clumped together as a “hyperpixel” (for example, for a single instance of data which is 4 x 3 x 130 x 750, a 4 x 3 x 13 x 75 section of the array could be all 0’s, then the next section be all 1’s, indicating that those sections are to be treated as one large pixel).\n","* `SHAPLEY_VALUE_SAMPLING_N_SAMPLES` (Integer) (Default value of 10) - number of feature permutations tested, Captum defaults to 25 but in an earlier attempt to reduce the memory intensity this was reduced to 10.\n","* `SHAPLEY_VALUE_SAMPLING_PERTURBATIONS_PER_EVAL` (Integer) (Default of 80) - Allow for multiple ablations to be processed simultaneously when data is passed to the classifier. Captum defaults to 1 but 80 is provided from an earlier attempt to speed up the process while maintaining sufficient memory.\n","* `INTEGRATED_GRADIENTS_N_STEPS` (Integer) (Default value of 20, Captum recommends 50 by default) - number of steps to be used for approximation method  used by Integrated Gradients. 50 is the Captum default but to try and reduce the memory intensity of the attribution method (and avoid crashes) it is set to 20.\n","* `INTEGRATED_GRADIENTS_METHOD` (String) (Default value of “riemann_trapezoid”) - The integration method to be used for Integrated Gradients. By default, Captum always resorts to “gausslegendre” but “riemann_trapezoid” seems to be less memory intensive, in conjunction with a reduced number of steps for `INTEGRATED_GRADIENTS_N_STEPS`. Captum accepts the following arguments:\n","  * “riemann_right”\n","  * “riemann_left”\n","  * “riemann_middle”\n","  * “riemann_trapezoid”\n","  * “gausslegendre”\n","* `INTEGRATED_GRADIENTS_INTERNAL_BATCH_SIZE` (int) (Default value of None) - essentially take n_steps * example data points and divide that into chunks which are calculated sequentially by the classifier. The value must at least be equal to whatever the BATCH_SIZE option is set to IF THIS VARIABLE IS USED.\n","\n","## Output\n","\n","* A single tensor containing all the attribution data, outputted as a saved PyTorch tensor. The dimensions will vary depending on if attribution was done against a SINGLE class or ALL classes."],"metadata":{"id":"gNfWy8xwSjs1"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","\n","%matplotlib inline \n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import random\n","import math\n","import string\n","\n","\n","from skimage.filters import sobel\n","from skimage.color import rgb2gray\n","\n","# TPU support\n","## Insufficient testing has been done to see what methods support using the TPU but the ability to enable the TPU has been provided here.\n","## Ensure that the Runtime for the Colab notebook has \"TPU\" selected prior to use!\n","#!pip install -q cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm\n","\n","\n","# Install Captum for Feature Attribution\n","!pip install -q captum\n","\n","# Install tqdm for the progress bars\n","!pip install -q tqdm\n","from tqdm.auto import tqdm\n","from tqdm.contrib import tenumerate\n","\n","# Import Captum functions\n","from captum.attr import *\n","\n","# \n","from enum import Enum"],"metadata":{"id":"n5iNeFmJHjg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# THIS CODE ONLY APPLIES IF YOU ARE TRYING TO DO SHAPLEY VALUE SAMPLING\n","## This gives an example of how to build a mask for Shapley Value Sampling\n","\n","## a single instance of data has the shape is [4, 3, 130, 750]\n","## we want to build a mask that has the same dimensions\n","\n","# one unit matrix, all zeros, \n","unit_mat_26x75 = np.zeros((3,26,75), dtype=int)\n","\n","# # create a single row with 10 26x75 matrices\n","unit_row_26x750_list = []\n","\n","for i in range(10):\n","  unit_row_26x750_list.append(unit_mat_26x75 + i)\n","\n","unit_row_26x750 = np.concatenate((tuple(unit_row_26x750_list)), axis=2)\n","\n","unit_row_26x750.shape\n","\n","# create multiple rows to create one base matrix\n","# Should be 5 instances of rows\n","# need to increment the values of each row by 10\n","# [0, 30,..., ]\n","\n","unit_matrix_130x750_list = []\n","for i in np.arange(0,50,10):\n","  unit_matrix_130x750_list.append(unit_row_26x750 + i)\n","\n","unit_matrix_130x750 = np.concatenate(tuple(unit_matrix_130x750_list), axis=1)\n","\n","unit_matrix_130x750.shape\n","\n","# need 4 base matrices to create one whole mask\n","# want 4 x 3 x 130 x 750\n","\n","full_matrix_4x3x130x750_list = []\n","for i in np.arange(0, 200, 50):\n","  full_matrix_4x3x130x750_list.append(unit_matrix_130x750 + i)\n","\n","full_matrix_4x3x130x750 = np.zeros((4,3,130,750))\n","for i in range(len(full_matrix_4x3x130x750_list)):\n","  full_matrix_4x3x130x750[i] = full_matrix_4x3x130x750_list[i]\n","\n","full_matrix_4x3x130x750.shape\n","\n","shapley_feature_mask = full_matrix_4x3x130x750"],"metadata":{"id":"FOhmd1i-WmaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttributionMethods(Enum):\n","  INTEGRATED_GRADIENTS = 1\n","  DEEPLIFT = 2\n","  SALIENCY_MAPS = 3\n","  INPUT_X_GRADIENT = 4\n","  GUIDED_BACKPROP = 5\n","  DECONVOLUTION = 6\n","  LRP = 7\n","  OCCLUSION = 8\n","  SHAPLEY_VALUE_SAMPLING = 9\n","\n","class AttributionTargetMode(Enum):\n","  SINGLE_CLASS = 1 \n","  ALL_CLASS = 2\n","\n","class Backend(Enum):\n","  CPU = \"cpu\"\n","  GPU = \"cuda:0\"\n","  TPU = \"tpu\""],"metadata":{"id":"ooCHBBt-GKTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main Control Flow Variables\n","\n","## Method for Feature Attribution\n","ATTRIBUTION_METHOD = AttributionMethods.DECONVOLUTION\n","\n","## Targets to perform attribution against\n","### If SINGLE_CLASS then attribution is only done against whatever labels\n","### are supplied to the Y_PATH variable, if ALL_CLASS then attributions\n","### are done against all possible classes\n","ATTRIBUTION_TARGET_MODE = AttributionTargetMode.ALL_CLASS\n","\n","## Select the device for the feature attribution processing to be done on\n","BACKEND = Backend.GPU\n","## Controls how many data instances should be attributed in one go\n","BATCH_SIZE = 1\n","\n","# Number of pieces of data to process, useful for debugging if you \n","# have a large dataset but only want the first n to be attributed.\n","# if left equal to None, then ALL DATA will be processed\n","NUMBER_TO_PROCESS = None\n","\n","## File Paths\n","### Path to the saved model\n","MODEL_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/model1e-050.5.2022-05-22 12_13_10.pt\"\n","X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_cropped_b.npy\"\n","Y_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/predicted_labels.npy\"\n","ATTRIBUTIONS_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/Deconvolution All Class.pt\"\n","\n","## Attribtuion specific method options\n","\n","\n","## Occlusion\n","### Alternative working dimensions include:\n","### Strides: (1,3,10,10)\n","### Window Shape: (1,3,13,75)\n","\n","\n","OCCLUSION_SLIDING_WINDOW_SHAPES = (1,3,26,75)\n","OCCLUSION_STRIDES = (1,3,26,75)\n","OCCLUSION_PERTURBATIONS_PER_EVAL = 160\n","\n","SHAPLEY_VALUE_SAMPLING_FEATURE_MASK = full_matrix_4x3x130x750\n","SHAPLEY_VALUE_SAMPLING_N_SAMPLES = 10\n","SHAPLEY_VALUE_SAMPLING_PERTURBATIONS_PER_EVAL = 80\n","\n","INTEGRATED_GRADIENTS_N_STEPS = 20 # default is 50, may be too intensive for Colab to handle\n","INTEGRATED_GRADIENTS_METHOD = 'riemann_trapezoid'\n","INTEGRATED_GRADIENTS_INTERNAL_BATCH_SIZE = None\n","\n","\n","# Check that GPU is available, if it isn't then BACKEND will automatically toggle to CPU\n","if(BACKEND == Backend.GPU):\n","  if(torch.cuda.is_available()):\n","    print(\"GPU Selected, and confirmed available!\")\n","    device = torch.device(\"cuda:0\")\n","  else:\n","    print(\"GPU Selected, but not found! Switching to CPU for backend\")\n","    BACKEND = Backend.CPU\n","    device = torch.device(\"cpu\")\n","elif(BACKEND == Backend.TPU):\n","  assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n","  print(\"TPU Selected\")\n","  device = xm.xla_device()\n","elif(BACKEND == Backend.CPU):\n","  print(\"CPU Selected\")\n","  device = torch.device(\"cpu\")"],"metadata":{"id":"GApkxaFsISrV","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1653852286446,"user_tz":420,"elapsed":271,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"7a8877ba-211e-49df-8b87-0295afe8beac"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1d30aeb55500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Method for Feature Attribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mATTRIBUTION_METHOD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttributionMethods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDECONVOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Targets to perform attribution against\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AttributionMethods' is not defined"]}]},{"cell_type":"code","source":["#This function takes in a model and replaces inplace relu layers to an independent relu layer\n","def reluToInplaceFalse(model):\n","  for name, child in model.named_children():\n","    if isinstance(child, nn.ReLU):\n","      setattr(child, 'inplace', False)\n","    else:\n","      reluToInplaceFalse(child)"],"metadata":{"id":"KteNB-tzlAIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Model Class\n","\n","TARGET_WIDTH = 750\n","TARGET_HEIGHT = 130\n","from torchvision.transforms.transforms import RandomRotation, RandomAdjustSharpness, RandomGrayscale\n","import torchvision.transforms.functional as tf\n","\n","def init_weights(m):\n","  if isinstance(m, nn.Linear):\n","    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n","\n","\n","class Classifier(torch.nn.Module):\n","\n","  def __init__(self, backbone='resnet', multi_backbone = False, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH):\n","    super().__init__()\n","    self.multi_backbone = multi_backbone # Bool: Indicates if we use multibackbone\n","\n","    #In the following section we download the appropriate prettrained model\n","    if backbone == \"vgg19\":\n","      backbone = torchvision.models.vgg19(pretrained=True)\n","      self.out_channels = 25088\n","      \n","    elif backbone == \"resnet18\":\n","      backbone = torchvision.models.resnet18(pretrained=True)\n","      self.out_channels = 512\n","\n","    elif backbone == \"resnet50\":\n","      backbone = torchvision.models.resnet50(pretrained=True)\n","      self.out_channels = 2048\n","\n","    elif backbone == \"Efficientnet b1\":\n","      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n","      self.out_channels = 1280\n","\n","    elif backbone == \"Efficientnet b3\":\n","      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n","      self.out_channels = 1536\n","\n","    elif backbone == \"Efficientnet b5\":\n","      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n","      self.out_channels = 2048\n","\n","    elif backbone == \"Efficientnet b7\":\n","      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n","      self.out_channels = 2560\n","    else:\n","      raise ValueError(f'Invalid backbone \"{backbone}\"')\n","      \n","    # Disabling inplace ReLu becasuse GradCam doesn't work it enabled\n","    reluToInplaceFalse(backbone)\n","     \n","    modules = list(backbone.children())[:-1]\n","\n","    if self.multi_backbone: #We create the backbones and put them on the device\n","      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n","\n","    else:\n","      self.backbone =  nn.Sequential(*modules).to(device)\n","\n","    self.do_augmentation = do_augmentation\n","\n","    # Note: These are not all of the augmnetations performed, see custom_augmentation()\n","    self.unlabeled_augmentation = nn.Sequential(transforms.RandomVerticalFlip(0.5),\n","                                      transforms.RandomCrop(size=(target_height,target_width)),\n","                                      transforms.RandomRotation(10, interpolation=transforms.InterpolationMode.BILINEAR, fill=1),\n","                                      transforms.Normalize(0, 1)\n","    )\n","\n","    self.bottleneck_dim = 256\n","\n","    # This is the linear layer to compress each backbone\n","    self.fc_bb = nn.Sequential(nn.BatchNorm1d(self.out_channels),\n","                               nn.Dropout(dropout_rate),\n","                               nn.Linear(self.out_channels, self.bottleneck_dim),\n","                               nn.BatchNorm1d(self.bottleneck_dim),\n","                               nn.ReLU())\n","    self.fc_bb.apply(init_weights)\n","\n","    self.fc_hflip1 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip1.apply(init_weights)\n","\n","    self.fc_hflip2 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip2.apply(init_weights)\n","\n","    self.fc_hflip3 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip3.apply(init_weights)\n","\n","    self.fc_hflip4 = nn.Sequential(nn.Dropout(dropout_rate),\n","                                   nn.Linear(self.bottleneck_dim, 1))\n","    self.fc_hflip4.apply(init_weights)\n","\n","    #This is the final classification layer\n","    self.fc = nn.Sequential(nn.Dropout(dropout_rate),\n","                            nn.Linear(self.bottleneck_dim * 4, 3))\n","    self.fc.apply(init_weights)\n","\n","    # A softmax is applied in eval mode\n","    self.softmax = nn.Softmax(dim=1)              \n","     \n","  def forward(self, x):\n","    if self.do_augmentation and self.training:\n","      imgs, hflip_labels = map(list, zip(*[self.custom_augmentation(x[:,i]) for i in range(4)])) #list of 4 images\n","      hflip_labels = [torch.Tensor(hflip_label).float().unsqueeze(1) for hflip_label in hflip_labels]\n","    else:\n","      imgs = [x[:,i] for i in range(4)] #list of 4 images\n","      hflip_labels = None\n","    \n","    if self.multi_backbone:\n","      encodings = [self.fc_bb(self.backbone1(imgs[0]).flatten(1)), \n","                   self.fc_bb(self.backbone2(imgs[1]).flatten(1)),\n","                   self.fc_bb(self.backbone3(imgs[2]).flatten(1)),\n","                   self.fc_bb(self.backbone4(imgs[3]).flatten(1))]\n","    else:\n","      encodings = [self.fc_bb(self.backbone(img).flatten(1)) for img in imgs]\n","\n","    logits = self.fc(torch.cat(encodings,1))\n","    if self.training:\n","      # get hflip predictions\n","      hflip_preds = [self.fc_hflip1(encodings[0]),\n","                     self.fc_hflip2(encodings[1]),\n","                     self.fc_hflip3(encodings[2]),\n","                     self.fc_hflip4(encodings[3])]\n","      hflip_preds = torch.cat(hflip_preds, 1)\n","      hflip_labels = torch.cat(hflip_labels, 1).to(device)\n","      return logits, hflip_preds, hflip_labels\n","    else:\n","      return self.softmax(logits)\n","\n","  def custom_augmentation(self, images):\n","    hflip_labels = np.random.choice([0, 1], size = images.size(0))\n","    for i, hflip_label in enumerate(hflip_labels):\n","      if hflip_label == 1:\n","        images[i] = tf.hflip(images[i])\n","    images = self.unlabeled_augmentation(images)\n","    return images, hflip_labels"],"metadata":{"id":"NM8V2xEplBsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","model = torch.load(MODEL_PATH, map_location=\"cpu\")\n","model.eval()\n","model.zero_grad()\n","\n","# Remove the augmenation layers, LRP apparently does not like them ):\n","# FAILS: SiLU does not work with LRP\n","# Credit: \n","if(ATTRIBUTION_METHOD == AttributionMethods.LRP):\n","  model = nn.Sequential(*list(model.children())[1:])\n","\n","if(ATTRIBUTION_METHOD == AttributionMethods.SHAPLEY_VALUE_SAMPLING):\n","  shapley_feature_mask = torch.tensor(shapley_feature_mask).unsqueeze(0).to(device)\n","\n","# Put the model onto the device\n","model.to(device);"],"metadata":{"id":"qxlVTtuZH9pZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653774498295,"user_tz":420,"elapsed":6155,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"4c02a7ce-f539-49a7-d28c-484be9194372"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Prepare data\n","x = np.load(X_PATH)\n","y = np.load(Y_PATH)\n","\n","print(x.shape)\n","print(y.shape)\n","\n","tensor_x = torch.Tensor(x) \n","tensor_y = torch.Tensor(y).long()\n","\n","tensor_x = torch.swapaxes(tensor_x,2,4)\n","tensor_x = torch.swapaxes(tensor_x,3,4)\n","\n","print(tensor_x.shape)\n","print(tensor_y.shape)\n","from torch.utils.data import TensorDataset\n","\n","attribution_ds = TensorDataset(tensor_x ,tensor_y) \n","attribution_dl = DataLoader(attribution_ds, BATCH_SIZE ,shuffle = False)\n","del x,y"],"metadata":{"id":"l_K4hN62MxuJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653774516484,"user_tz":420,"elapsed":18192,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"4d641a34-d60b-4355-c97c-610a1b384268"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(285, 4, 130, 750, 3)\n","(285, 1)\n","torch.Size([285, 4, 3, 130, 750])\n","torch.Size([285, 1])\n"]}]},{"cell_type":"code","source":["## Depending on user selection, create the feature attribution object\n","\n","if (ATTRIBUTION_METHOD == AttributionMethods.INTEGRATED_GRADIENTS):\n","  print(\"Integrated Gradients Chosen\")\n","  attribution_obj = IntegratedGradients(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.DEEPLIFT):\n","  print(\"DeepLift Chosen\")\n","  attribution_obj = DeepLift(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.SALIENCY_MAPS):\n","  attribution_obj = Saliency(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.INPUT_X_GRADIENT):\n","  print(\"Input X Gradient Chosen\")\n","  attribution_obj = InputXGradient(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.GUIDED_BACKPROP):\n","  print(\"Guided Backpropagation Chosen\")\n","  attribution_obj = GuidedBackprop(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.DECONVOLUTION):\n","  print(\"Deconvolution Chosen\")\n","  attribution_obj = Deconvolution(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.LRP):\n","  print(\"LRP (Layer-wise Relevance Propagation)\")\n","  attribution_obj = LRP(model)\n","elif (ATTRIBUTION_METHOD == AttributionMethods.OCCLUSION):\n","  print(\"Occlusion Selected\")\n","  attribution_obj = Occlusion(model)\n","elif(ATTRIBUTION_METHOD == AttributionMethods.SHAPLEY_VALUE_SAMPLING):\n","  print(\"Shapley Value Sampling Selected\")\n","  attribution_obj = ShapleyValueSampling(model)"],"metadata":{"id":"gBYpEnUVAa7E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653774516485,"user_tz":420,"elapsed":12,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"f72b0454-ee41-4238-be37-562b83df9a2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deconvolution Chosen\n"]}]},{"cell_type":"code","source":["all_attributions = []\n","# should allow for a certain number to be executed\n","# Ex: 10 iterates\n","# should be an if-else\n","# if number provided, then call the loop that many times\n","\n","if(NUMBER_TO_PROCESS is not None):\n","  total_batches = NUMBER_TO_PROCESS - 1\n","else:\n","  total_batches = tensor_x.size(0)\n","\n","for iterations, (images, labels) in tenumerate(attribution_dl, total = total_batches):\n","  # send images to the device\n","  images = images.to(device)\n","  # perform attributions\n","  # If option is chosen to attribute to ALL targets, \n","  # then apply a for-loop to \n","  if(ATTRIBUTION_METHOD == AttributionMethods.INTEGRATED_GRADIENTS):\n","    if(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.SINGLE_CLASS):\n","      attributions = attribution_obj.attribute(images, \n","                                               target = labels.squeeze().to(device),\n","                                               method = INTEGRATED_GRADIENTS_METHOD,\n","                                               n_steps = INTEGRATED_GRADIENTS_N_STEPS,\n","                                               internal_batch_size = INTEGRATED_GRADIENTS_INTERNAL_BATCH_SIZE)\n","      all_attributions.append(attributions.cpu())     \n","    elif(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.ALL_CLASS):\n","      sub_attributions = []\n","      for i in range(3):\n","        sub_attributions = attribution_obj.attribute(images, \n","                                                     target = labels.squeeze().to(device),\n","                                                     method = INTEGRATED_GRADIENTS_METHOD,\n","                                                     n_steps = INTEGRATED_GRADIENTS_N_STEPS,\n","                                                     internal_batch_size = INTEGRATED_GRADIENTS_INTERNAL_BATCH_SIZE)\n","        all_attributions.append(sub_attributions)\n","  elif(ATTRIBUTION_METHOD == AttributionMethods.OCCLUSION):\n","    if(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.SINGLE_CLASS):\n","      attributions = attribution_obj.attribute(images, \n","                                              target=labels.squeeze().to(device), \n","                                              strides=OCCLUSION_STRIDES,\n","                                              sliding_window_shapes=OCCLUSION_SLIDING_WINDOW_SHAPES,\n","                                              perturbations_per_eval=OCCLUSION_PERTURBATIONS_PER_EVAL,\n","                                              show_progress=False)\n","      all_attributions.append(attributions.cpu())  \n","    elif(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.ALL_CLASS):\n","        sub_attributions = []\n","        for i in range(3): # classes are 0 1 and 2 respectively\n","          sub_attributions.append(attribution_obj.attribute(images, \n","                                                            target=i,\n","                                                            strides=OCCLUSION_STRIDES,\n","                                                            sliding_window_shapes=OCCLUSION_SLIDING_WINDOW_SHAPES,\n","                                                            perturbations_per_eval=OCCLUSION_PERTURBATIONS_PER_EVAL,\n","                                                            show_progress=False).squeeze().cpu())\n","        all_attributions.append(sub_attributions)\n","  elif(ATTRIBUTION_METHOD == AttributionMethods.SHAPLEY_VALUE_SAMPLING):\n","    if(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.SINGLE_CLASS):\n","      attributions = attribution_obj.attribute(images, \n","                                               target=labels.squeeze().to(device),\n","                                               feature_mask = shapley_feature_mask,\n","                                               n_samples = SHAPLEY_VALUE_SAMPLING_N_SAMPLES,\n","                                               perturbations_per_eval=SHAPLEY_VALUE_SAMPLING_PERTURBATIONS_PER_EVAL,\n","                                               show_progress=False)\n","      all_attributions.append(attributions.cpu())  \n","    elif(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.ALL_CLASS):\n","        sub_attributions = []\n","        for i in range(3): # classes are 0 1 and 2 respectively\n","          sub_attributions.append(attribution_obj.attribute(images, \n","                                                            target=labels.squeeze().to(device),\n","                                                            feature_mask = shapley_feature_mask,\n","                                                            n_samples = SHAPLEY_VALUE_SAMPLING_N_SAMPLES,\n","                                                            perturbations_per_eval=SHAPLEY_VALUE_SAMPLING_PERTURBATIONS_PER_EVAL,\n","                                                            show_progress=False))\n","        all_attributions.append(sub_attributions)\n","  else:\n","    if(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.SINGLE_CLASS):\n","      attributions = attribution_obj.attribute(images,\n","                                               target=labels.squeeze().to(device))\n","      all_attributions.append(attributions)\n","    elif(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.ALL_CLASS):\n","        sub_attributions = []\n","        for i in range(3): # classes are 0 1 and 2 respectively\n","          sub_attributions.append(attribution_obj.attribute(images, \n","                                                            target=i))\n","        all_attributions.append(sub_attributions)\n","\n","\n","  # delete images, labels, and batch attributions from memory\n","  del images, labels\n","  # If things were run on the GPU, empty the cache to prevent memory overloading\n","  if(BACKEND == Backend.GPU):\n","    torch.cuda.empty_cache()\n","  # prematurely break out of loop if an explicit amount of data was requested\n","  if(NUMBER_TO_PROCESS is not None and (iterations == NUMBER_TO_PROCESS - 1)):\n","    break"],"metadata":{"id":"H35Y0gMMNRL3","colab":{"base_uri":"https://localhost:8080/","height":140,"referenced_widgets":["aabeb4e1478348d89ca6ab0fca90e285","3a8860a6f48c482ab4978811c4e01523","fde22e388a184edd972ef9bf19330636","3ce2f73eac6b4649979e63f4579f972f","288241f3dbec4c06ba848f3c3a31441c","088fccf373aa4c4fb437263679ecaac0","d5293a9592574d5cad11ac4430113582","debe542aac18490e831ad994dc4255b0","e4fe26212fd44cb7af3aab7390290816","605d03db7d344f37a313ec8d1cb8f73a","ca9f21f6c410444aae68cf64ab484a43"]},"executionInfo":{"status":"ok","timestamp":1653774672113,"user_tz":420,"elapsed":155637,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"04b56f6a-7298-4fe3-bb25-944c828b3bd1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabeb4e1478348d89ca6ab0fca90e285"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/captum/_utils/gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n","  \"required_grads has been set automatically.\" % index\n","/usr/local/lib/python3.7/dist-packages/captum/attr/_core/guided_backprop_deconvnet.py:65: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n","  \"Setting backward hooks on ReLU activations.\"\n"]}]},{"cell_type":"code","source":["# final shape should be (285, 3, 4, 3, 130, 750)\n","# Credit to: https://discuss.pytorch.org/t/nested-lists-of-tensors-to-tensor/121449/2\n","if(ATTRIBUTION_TARGET_MODE == AttributionTargetMode.ALL_CLASS):\n","  all_attributions_tensor_size = [len(all_attributions), len(all_attributions[0])] + list(all_attributions[0][0].shape)\n","  all_attributions_tensor = torch.empty(all_attributions_tensor_size)\n","  for i in range(len(all_attributions)):\n","    for j in range(len(all_attributions[0])):\n","      all_attributions_tensor[i][j] = all_attributions[i][j]\n","  all_attributions_tensor = all_attributions_tensor.squeeze()\n","else:\n","  all_attributions_tensor = torch.cat(all_attributions, 0)\n","\n","del all_attributions"],"metadata":{"id":"AC419OYwmkCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the attribution tensors back to drive\n","\n","torch.save(all_attributions_tensor, ATTRIBUTIONS_PATH)"],"metadata":{"id":"SumdTXOzRsX1"},"execution_count":null,"outputs":[]}]}