{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature Attribution Image Generation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1O84_xu8PcLUo8uf7_A7ekoCjn9IkkB2s","authorship_tag":"ABX9TyOdBSrOs8oohgsD6EvG6h1R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"31b5d068fe9e4091bfe3b2a9fc3767b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af8ec50222044487b58cefb2905e7503","IPY_MODEL_1538e5fa3e9b48dc84fa449466f969a9","IPY_MODEL_364d0c7e3c2c4e48bb513aa2828f49af"],"layout":"IPY_MODEL_aba133f05e6446daa49f5e23e91c6a62"}},"af8ec50222044487b58cefb2905e7503":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3fedce4184b43f8bd8c55701c075922","placeholder":"​","style":"IPY_MODEL_2ffb0084c0354ecd8c45460f96be4c1a","value":"100%"}},"1538e5fa3e9b48dc84fa449466f969a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7279c4d6c8c4d9ab5109af771e26f30","max":285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9af45ffaa2e240fbb0b3a0fbe02f6619","value":285}},"364d0c7e3c2c4e48bb513aa2828f49af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2d63f14966c4e0890ee0b778f8b5e1d","placeholder":"​","style":"IPY_MODEL_8377a25f915c4f5cbf5d68125f2a986b","value":" 285/285 [16:38&lt;00:00,  3.62s/it]"}},"aba133f05e6446daa49f5e23e91c6a62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3fedce4184b43f8bd8c55701c075922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffb0084c0354ecd8c45460f96be4c1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7279c4d6c8c4d9ab5109af771e26f30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af45ffaa2e240fbb0b3a0fbe02f6619":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2d63f14966c4e0890ee0b778f8b5e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8377a25f915c4f5cbf5d68125f2a986b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Feature Attribution Image Generation\n","\n","This notebook allows you to generate images where the feature attribution data can be visualized (overlayed over the original image).\n","\n","The following image generation options are supported:\n","\n","## __Single Images__ \n","Out of a single instance of data (which contains 4 images of the fish) each of those images is turned into their own SEPERATE image.\n","\n","## __Confusion Matrices__ \n","NOTE: THIS REQUIRES YOU TO HAVE PERFORMED ATTRIBUTIONS AGAINST ALL CLASSES versus just a single class. Failure to do so results in the dimensions of the data being incompatible for this operation. Please refer to the \"Feature Attribution Data Generation\" notebook for how to do this. \n","\n","A Feature Attribution Confusion Matrix has the same underlying data instance's attributions against ALL possible classes visualized in one image. Each column is the class the attribution was done against (0, 1, or 2) while the row indicates the image (out of 4 from the underlying data) the attribution was done against.\n","\n","## Inputs\n","The settings that control the notebook are determined by a set of variables (all expressed as capital letters, with underscoring used for spaces ex: `ORIGINAL_IMAGES_PATH`). The values of these variables can be changed prior to execution of the notebook.\n","\n","### Required Arguments\n","The notebook requires the following information to be provided. \n","\n","## File Paths\n","* `ORIGINAL_IMAGES_PATH` (string) - A path to the original images saved as a numpy array `(*.npy)`\n","* `ATTRIBTUION_DATA_PATH` (string) - A path to the attribution data generated by the \"Feature Attribution Data Generation\" Notebook, saved as a PyTorch tensor `(*.pt)`\n","* `TRUE_LABELS_PATH` (string)  - A path to the True Labels of the dataset, saved as a numpy array `(*.npy)`\n","* `PREDICTED_LABELS_PATH` (string) - A path to the Predicted Labels of the dataset, saved as a numpy array `(*.npy)`\n","* `FINAL_IMAGES_FOLDER_PATH` (string) - A path to an EMPTY FOLDER where you would like the images to be stored\n","* `CONFUSION_MATRIX_ENABLED` (boolean) - Whether you would like single images or confusion matrices generated\n","* `ATTRIBUTION_DONE_AGAINST` (Enum, refer to `AttributionDoneAgainst` class options `TRUE_LABELS` and `PREDICTED_LABELS`). - This only applies when `CONFUSION_MARTRIX_ENABLED = False`, determines if the title of the image should indicate the attribution was done against a true label or a predicted label \n","\n","### Optional Arguments\n","The notebook has default options for these but they can be tweaked for custom results. Note that the following variables are passed DIRECTLY into a call to the `visualize_image_attr` function that Captum provides, meaning it should align with the information found in the Captum documentation: https://captum.ai/api/utilities.html#visualization. The most relevant parts have been summarized/taken straight from the documentation and provided below.\n","\n","* `METHOD` (string) (default value: \"blended_heat_map\") - the method for visualization attribution. They are:\n","  * \"heat_map\" - display a heatmap of attributions\n","  * \"blended_heat_map\" - put the heatmap over a greyscale version of the image\n","  * \"original_image\" - Just show the original image\n","  * \"masked_image\" - mask image (pixel-wise multiply) by normalized attribution values\n","  * \"alpha_scaling\" - set the alpha channel of each pixel to normalized attribution value\n","* `SIGN` (string) (default value: \"all\") - Determines which attribution values to show. The options for this method are:\n","  * \"positive\" - only display positive attributions\n","  * \"absolute_value\" - display the absolute value of all attributions\n","  * \"negative\" - only display negative attributions\n","  * \"all\" - display both positive and negative attributions. Note that if you set `METHOD` to \"masked_image\" or \"alpha_scaling\" the \"all\" option is NOT supported.\n","* `ALPHA_OVERLAY` (default value: 0.8) (float between 0 and 1) - controls the \"brightness\" or rather how prominently the zebrafish appears in the background. Higher Alpha values correspond to fainter background images.\n","* `SHOW_COLORBAR` (default value: True) (boolean) - Determines if a colorbar is added that shows a mapping between the color on the image (red/green) and its associated attribution value.\n","\n","## Outputs\n","* Confusion Matrices OR individual images at the location specified by `FINAL_IMAGES_FOLDER_PATH`. For a dataset with 285 instances, expect 285 x 4 individual images or 285 images of a confusion matrix. \n","  * If individual images are generated, the file will be named along the lines of: `Index: (main instance index), (subindex).png`. The main instance index allows you to locate the exact piece of data in the PyTorch tensor/dataset while the subindex refers to the specific image in the instance\n","  * If confusion matrices are generated, the file will only posses the main index as the file name: `Index:(main instance index).png`"],"metadata":{"id":"_UYK24EfHRNN"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKqluUfLGVHx","executionInfo":{"status":"ok","timestamp":1653775306455,"user_tz":420,"elapsed":8818,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"d5f905bd-1b02-48c5-dff6-83465b12ca21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"]}],"source":["# Load Dependencies\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","!pip install -q captum\n","!pip install tqdm\n","\n","# tqdm allows you to see the progress of the generation process as a \n","# \"loading\" bar, with \"it/s\" or iterations per second, giving you a rough\n","# idea of how long the process should take as its going\n","from tqdm.auto import tqdm\n","from tqdm.contrib import tenumerate\n","from captum.attr import visualization as viz"]},{"cell_type":"code","source":["# Fixed options for controlling the title of individual images,\n","# please refer to the Cell below for more information\n","\n","from enum import Enum\n","\n","class AttributionDoneAgainst(Enum):\n","  TRUE_LABELS = 1\n","  PREDICTED_LABELS = 2"],"metadata":{"id":"CTQufe7sQt5q","executionInfo":{"status":"ok","timestamp":1653775306457,"user_tz":420,"elapsed":45,"user":{"displayName":"John Long","userId":"00584768646093006387"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Control Variables \n","\n","## File Paths\n","\n","### Path to the original images used for feature attribution, must be a .npy file\n","ORIGINAL_IMAGES_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_cropped_b.npy\"\n","### Path to the attribution data generated from the \"Feature Attribution Data Generation\" Notebook,\n","### This should be a \".pt\" or saved PyTorch Tensor file\n","ATTRIBUTION_DATA_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/Deconvolution All Class.pt\"\n","### Path to the true labels of the dataset, should have the dimensions\n","### [1,x] where x is the number of pieces of data\n","TRUE_LABELS_PATH = \"/content/drive/Shareddrives/Exploding Gradients/y_b.npy\"\n","### Path to the predicted labels of the dataset, should have the dimensions\n","### [1,x] where x is the number of pieces of data\n","PREDICTED_LABELS_PATH =  \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/predicted_labels.npy\"\n","### Path to where you want to save your images\n","FINAL_IMAGES_FOLDER_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/Deconvolution Confusion Matrices\"\n","\n","## Denote if confusion matrix is desired.\n","## If set to False then only single images will be generated.\n","## Requires that feature attribution against ALL possible classes\n","## was performed in \"Feature Attribtuion Data Generation\" notebook, otherwise\n","## the dimensions of the data will not line up!\n","CONFUSION_MATRIX_ENABLED = True\n","\n","## Denote if attribution was done against the true labels or predicted labels\n","## This is ONLY USED WHEN CONFUSION_MATRIX_ENABLED = FALSE and single\n","## image output is desired. This will affect if the \n","ATTRIBTUION_DONE_AGAINST = AttributionDoneAgainst.PREDICTED_LABELS\n","\n","## Image Processing Options\n","\n","### These methods are passed straight to Captum's Visualization function.\n","### More information about the function can be found here: \n","### https://captum.ai/api/utilities.html\n","METHOD = \"blended_heat_map\"\n","SIGN = \"all\"\n","ALPHA_OVERLAY = 0.8\n","SHOW_COLORBAR = True"],"metadata":{"id":"3oGMb7_FGoQe","executionInfo":{"status":"ok","timestamp":1653775306458,"user_tz":420,"elapsed":37,"user":{"displayName":"John Long","userId":"00584768646093006387"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74_1H013HK3f","executionInfo":{"status":"ok","timestamp":1653775307550,"user_tz":420,"elapsed":1127,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"94587e3d-0c85-46f3-e8c8-1fde4751519c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Load original images from the .npy file\n","# and convert to Tensor, reshaping the dimensions as needed\n","images = np.load(ORIGINAL_IMAGES_PATH)\n","images_tensor = torch.Tensor(images) \n","images_tensor = torch.swapaxes(images_tensor,2,4)\n","images_tensor = torch.swapaxes(images_tensor,3,4)\n","\n","# Load attribution data\n","attributions_tensor = torch.load(ATTRIBUTION_DATA_PATH, map_location=torch.device('cpu'))\n","\n","# Load labels\n","true_labels = np.squeeze(np.load(TRUE_LABELS_PATH))\n","predicted_labels = np.squeeze(np.load(PREDICTED_LABELS_PATH))"],"metadata":{"id":"JgUY_so4HSKr","executionInfo":{"status":"ok","timestamp":1653775373834,"user_tz":420,"elapsed":66291,"user":{"displayName":"John Long","userId":"00584768646093006387"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# If the user chooses to generate a confusion matrix\n","if CONFUSION_MATRIX_ENABLED:\n","  # For each piece of data (an \"instance\"), iterate over its associated feature attribution tensor\n","  # (In this case, it's three tensors combined as one because the attribution was done against all\n","  # possible classes) and the image tensor. Because the SAME set of images were used for the \n","  # attribution, we will reuse those images for each attribution in the confusion matrix.\n","  for instance_idx, (multi_class_attribution_tensor, images) in tenumerate(zip(attributions_tensor, images_tensor),\n","                                                                           total = len(attributions_tensor)):\n","    # Create a plot with 4 rows, 3 columns\n","    # Each row will be for one image of the fish in the instance,\n","    # Each column will be for one class that attribution was done against\n","    main_fig, main_ax = plt.subplots(4,3, constrained_layout=True)\n","    plt.subplots_adjust(wspace = 0.11, bottom=0.05)\n","    # Constrained layout tries to minimize the whitespace in the plot\n","    #main_fig.constrained_layout()\n","    # Set the size of the plot\n","    main_fig.set_size_inches(10.5, 6, forward=True)\n","    # Create a subtitle\n","    main_fig.suptitle(\"Index {0}\".format(instance_idx), fontsize=16)\n","    # Iterate of each individual attribution tensor out of the combined three.\n","    # \"col\" gives the column index for the plot\n","    for col, single_class_attribution in enumerate(multi_class_attribution_tensor):\n","      # Out of each individual attribution tensor, iterate over the four subattrbituions\n","      # which target each image\n","      for row, (attribution, image) in enumerate(zip(single_class_attribution, images)):\n","        # Generate the title for the image, which includes its subindex,\n","        # The label, and whether or not the label attributed against was the \n","        # True and Predicted label, both, one of them, or none at all.\n","        subimage_name_title_prefix = \"Subindex: {0}, Label:{1}\".format(row, col)\n","        if(col == int(true_labels[instance_idx]) and col == int(predicted_labels[instance_idx])):\n","          subimage_label = \"True & Predicted\"\n","        elif(col == int(predicted_labels[instance_idx])):\n","          subimage_label = \"Predicted\"\n","        elif(col == int(true_labels[instance_idx])):\n","          subimage_label = \"True\"\n","        else:\n","          subimage_label = \"\"\n","\n","        subimage_name = subimage_name_title_prefix + \"\\n\" + subimage_label\n","        # The main title of the confusion matrix plot\n","        image_name = \"Index:{0}\".format(instance_idx)\n","        _ = viz.visualize_image_attr(attribution.permute(1,2,0).detach().numpy(), \n","                                    image.permute(1,2,0).detach().numpy(), \n","                                    method=METHOD, \n","                                    sign=SIGN, \n","                                    alpha_overlay=ALPHA_OVERLAY,\n","                                    show_colorbar=SHOW_COLORBAR, \n","                                    use_pyplot = False,\n","                                    plt_fig_axis = (main_fig, main_ax[row][col]),\n","                                    title=subimage_name)\n","    # We don't want the image to load in the notebook, especially if we're\n","    # producing a large number of confusion matrices, so we \"close\" it\n","    plt.close()\n","    # Save the confusion matrix image to the user specified location\n","    main_fig.savefig(FINAL_IMAGES_FOLDER_PATH + \"/\" + image_name, bbox_inches='tight')\n","# If the user only wants single images to be generated\n","else:\n","  # iterate over all instances, including the images and attributions associated with the instance\n","  for instance_idx, (images, attributions) in tenumerate(zip(images_tensor, attributions_tensor),\n","                                                         total = len(attributions_tensor)):\n","    # for each instance (which contains 4 images), iterate over each subinstance\n","    # (1 image, 1 piece of data for attribution)\n","    for subinstance_idx, (image, attribution) in enumerate(zip(images, \n","                                                               attributions)):\n","      # Indicate the Index and Subindex\n","      image_name_index = \"Index: {0}, {1}\".format(instance_idx, \n","                                                  subinstance_idx)\n","      # Indicate the True and Predicted Labels associated with the image\n","      image_name_labels = \" True Label: {0}, Predicted Label: {1}\".format(int(true_labels[instance_idx]), \n","                                                                          int(predicted_labels[instance_idx]))\n","      \n","      # If the attribution was done against true or predicted labels,\n","      # then the title should reflect that.\n","      if(ATTRIBTUION_DONE_AGAINST == AttributionDoneAgainst.TRUE_LABELS):\n","        image_name = image_name_index + \"\\n\" +  image_name_labels + \"\\n\"  + \" Attributed Against: True Label\"\n","      elif(ATTRIBTUION_DONE_AGAINST == AttributionDoneAgainst.PREDICTED_LABELS):\n","        image_name = image_name_index + \"\\n\" + image_name_labels + \"\\n\"  + \" Attributed Against: Predicted Label\"\n","\n","      # Call the visualization function provided by Captum. \n","      # Note that the \"visualize_image_attr\" function\n","      # will automatically create a plot if none are passed in. Thus, there is\n","      # no need to instantiate an empty plot like the Confusion Matrix handling\n","      # code above.\n","      fig, _ = viz.visualize_image_attr(attribution.permute(1,2,0).detach().numpy(), \n","                                        image.permute(1,2,0).detach().numpy(), \n","                                        method=METHOD, \n","                                        sign=SIGN, \n","                                        alpha_overlay=ALPHA_OVERLAY,\n","                                        show_colorbar=SHOW_COLORBAR, \n","                                        use_pyplot = False,\n","                                        title=image_name)\n","      # Save the figure to the user-specified directory.\n","      fig.savefig(FINAL_IMAGES_FOLDER_PATH + \"/\" + image_name_index, bbox_inches='tight')"],"metadata":{"id":"zBp3nrp4VZUI","executionInfo":{"status":"ok","timestamp":1653776369156,"user_tz":420,"elapsed":995360,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["31b5d068fe9e4091bfe3b2a9fc3767b3","af8ec50222044487b58cefb2905e7503","1538e5fa3e9b48dc84fa449466f969a9","364d0c7e3c2c4e48bb513aa2828f49af","aba133f05e6446daa49f5e23e91c6a62","c3fedce4184b43f8bd8c55701c075922","2ffb0084c0354ecd8c45460f96be4c1a","f7279c4d6c8c4d9ab5109af771e26f30","9af45ffaa2e240fbb0b3a0fbe02f6619","d2d63f14966c4e0890ee0b778f8b5e1d","8377a25f915c4f5cbf5d68125f2a986b"]},"outputId":"8ba271f4-1c8a-4878-e911-1607a1129b57"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b5d068fe9e4091bfe3b2a9fc3767b3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: This figure was using constrained_layout==True, but that is incompatible with subplots_adjust and or tight_layout: setting constrained_layout==False. \n","  del sys.path[0]\n"]}]}]}