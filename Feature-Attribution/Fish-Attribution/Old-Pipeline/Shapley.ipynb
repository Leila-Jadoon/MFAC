{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac44235-e31e-4aef-938f-32220eeaf41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8524297-d208-4193-9ee5-0c57d1933e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluToInplaceFalse(model):\n",
    "  for name, child in model.named_children():\n",
    "    if isinstance(child, nn.ReLU):\n",
    "      setattr(child, 'inplace', False)\n",
    "    else:\n",
    "      reluToInplaceFalse(child)\n",
    "\n",
    "from torchvision.transforms.transforms import RandomRotation\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, backbone='resnet', multi_backbone = True, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False):\n",
    "    super().__init__()\n",
    "    self.multi_backbone = multi_backbone\n",
    "\n",
    "    if backbone == \"vgg19\":\n",
    "      backbone = torchvision.models.vgg19(pretrained=True)\n",
    "      self.out_channels = 25088\n",
    "      \n",
    "    elif backbone == \"resnet18\":\n",
    "      backbone = torchvision.models.resnet18(pretrained=True)\n",
    "      self.out_channels = 512\n",
    "\n",
    "    elif backbone == \"resnet50\":\n",
    "      backbone = torchvision.models.resnet50(pretrained=True)\n",
    "      self.out_channels = 2048\n",
    "\n",
    "    elif backbone == \"Efficientnet b1\":\n",
    "      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n",
    "      self.out_channels = 1280\n",
    "\n",
    "    elif backbone == \"Efficientnet b3\":\n",
    "      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n",
    "      self.out_channels = 1536\n",
    "\n",
    "    elif backbone == \"Efficientnet b5\":\n",
    "      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n",
    "      self.out_channels = 2048\n",
    "\n",
    "    elif backbone == \"Efficientnet b7\":\n",
    "      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n",
    "      self.out_channels = 2560\n",
    "      \n",
    "    # Disabling inplace ReLu becasuse GradCam doesn't work it enabled\n",
    "    reluToInplaceFalse(backbone)\n",
    "     \n",
    "    modules = list(backbone.children())[:-1]\n",
    "    self.do_augmentation = do_augmentation\n",
    "\n",
    "    if self.do_augmentation:\n",
    "      self.augmentation = nn.Sequential(transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomVerticalFlip(),\n",
    "                                        transforms.RandomPerspective(0.2),\n",
    "                                        RandomRotation(20),\n",
    "                                        transforms.RandomAutocontrast())\n",
    "\n",
    "    if self.multi_backbone:\n",
    "      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
    "    else:\n",
    "      self.backbone =  nn.Sequential(*modules).to(device)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    self.fc1 = nn.Sequential(nn.Dropout(dropout_rate),\n",
    "                              nn.Linear(self.out_channels, 128),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(dropout_rate)) #TODO: Experiment with BN and Dropout\n",
    "\n",
    "    # 512 features in, 3 features out\n",
    "    self.fc = nn.Sequential(nn.Linear(512, 3))                  #TODO: L2 Regularization\n",
    "     \n",
    "  def forward(self, x, is_training = True):\n",
    "    if self.do_augmentation and is_training:\n",
    "      imgs = [self.augmentation(x[:,i]) for i in range(4)] #list of 4 images\n",
    "    else:\n",
    "      imgs = [x[:,i] for i in range(4)] #list of 4 images\n",
    "      #imgs = [x[i] for i in range(4)]\n",
    "\n",
    "    if self.multi_backbone:\n",
    "        # feed each image into a backbone\n",
    "      encodings = [self.fc1(torch.flatten(self.backbone1(imgs[0]),1)),\n",
    "                   self.fc1(torch.flatten(self.backbone2(imgs[1]),1)),\n",
    "                   self.fc1(torch.flatten(self.backbone3(imgs[2]),1)),\n",
    "                   self.fc1(torch.flatten(self.backbone4(imgs[3]),1))]\n",
    "    else:\n",
    "      encodings = [self.fc1(self.backbone(img).squeeze()) for img in imgs]\n",
    "\n",
    "    # modify to return an array with the largest encoding\n",
    "    \n",
    "    #raw_result = self.fc(torch.cat(encodings,1))\n",
    "    #fixed_result = torch.clone(raw_result)\n",
    "    #for idx, row in enumerate(raw_result):\n",
    "    #    ones = torch.ones(3, dtype=torch.long)\n",
    "    #    max_val = row.max()\n",
    "    #    fixed_result[idx] = torch.where(row < max_val, 0, ones)\n",
    "                  \n",
    "    #return fixed_result.type(torch.int64)\n",
    "    return self.fc(torch.cat(encodings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859d7ed8-ccea-41aa-bfe9-162bb5d0e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_b_mar_7_00_05.pt', map_location=\"cpu\")\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1737bc-586b-4efb-8502-8265362dfc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape:  torch.Size([256, 4, 3, 200, 1024]) x test shape:  torch.Size([29, 4, 3, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"x_train_b.npy\")\n",
    "y = np.load(\"y_train_b.npy\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tensor_x = torch.Tensor(x) \n",
    "tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "\n",
    "tensor_x = torch.swapaxes(tensor_x,2,4)\n",
    "tensor_x = torch.swapaxes(tensor_x,3,4)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(tensor_x,tensor_y,test_size=0.1)\n",
    "\n",
    "print(\"x train shape: \", x_train.shape,\"x test shape: \",x_test.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_ds = TensorDataset(x_train,nn.functional.one_hot(y_train,3)) \n",
    "test_ds = TensorDataset(x_test,nn.functional.one_hot(y_test,3)) \n",
    "\n",
    "del x_train, x_test, y_train, y_test\n",
    "\n",
    "train_dl = DataLoader(train_ds,4,shuffle = True)\n",
    "test_dl = DataLoader(test_ds,4,drop_last = True)\n",
    "exp_dl = DataLoader(test_ds,10,drop_last = True)\n",
    "\n",
    "del x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7184c701-db30-4990-ba2f-18931856154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances, labels = iter(exp_dl).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d01b4b-45b8-4aa7-822e-268c7b61eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "for t in labels.squeeze():\n",
    "  if torch.equal(t, torch.tensor([1,0,0])):\n",
    "    new_labels.append(0)\n",
    "  elif torch.equal(t, torch.tensor([0,1,0])):\n",
    "    new_labels.append(1)\n",
    "  elif torch.equal(t, torch.tensor([0,0,1])):\n",
    "    new_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9d5912-922d-494d-be2c-35137132f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 0, 1, 2, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ff8350-3a09-4d3f-aff0-c02200ed1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution for model\n",
    "from captum.attr import ShapleyValueSampling\n",
    "svs = ShapleyValueSampling(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbf05e-301d-49d8-94ca-cae57d9c169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attributions = svs.attribute(images[:50], target=labels[:50], feature_mask=feature_mask_14x1#4, n_samples = 10, show_progress = True)\n",
    "attributions = svs.attribute(instances, target=new_labels, n_samples = 5, show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c2b929-5dc9-41a1-ab89-dd4e05705478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature masks, check image dimensions\n",
    "instances, labels = iter(exp_dl).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ff1c04-1586-45a3-acb9-00ccef4062d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 3, 200, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4949286-0b6d-4d6d-8c22-51a2532760f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 200, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = instances[0]\n",
    "instance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06a1b63-0cd1-4011-beeb-8d203bf1162f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a8b75d-5c64-46a1-bf33-c909f7b9ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20x16 pixel basis blocks\n",
    "base_matrix = np.array([0] * 320).reshape(20,16)\n",
    "base_row_list = []\n",
    "for i in range(64):\n",
    "    base_row_list.append(base_matrix + i)\n",
    "\n",
    "base_row = np.hstack((tuple(base_row_list)))\n",
    "all_rows = []\n",
    "for i in range(10):\n",
    "    all_rows.append(base_row + (64 * i))\n",
    "\n",
    "one_channel_mat = np.vstack(tuple(all_rows))\n",
    "# now we need to create three instances of one_channel_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc6e872-3424-438e-a18b-1a49e8bcc927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_channel_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cd3ce19-1c16-47a5-88c4-3e71f6608ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3, 200, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe059755-3b8f-44f5-85f0-cc0e0e3127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = one_channel_mat \n",
    "a[1] = one_channel_mat\n",
    "a[2] = one_channel_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caddc968-02dc-40e7-a4d5-599fd61dad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((4, 3, 200, 1024))\n",
    "# should be distinct values\n",
    "b[0] = a\n",
    "b[1] = a\n",
    "b[2] = a\n",
    "b[3] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e85b8a1-cb38-49a0-a9ba-f738425d099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 200, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2ec41c-8c40-4283-a197-d48883c0be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mask_20x16 = torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3262e-5368-4b87-a9db-196aa01c1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = svs.attribute(instances, target=new_labels, feature_mask = feature_mask_20x16, n_samples = 5, show_progress = True, perturbations_per_eval = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
