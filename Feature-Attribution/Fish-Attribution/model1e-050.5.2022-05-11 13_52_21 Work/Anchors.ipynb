{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anchors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61Y85FEerLeR"
      },
      "outputs": [],
      "source": [
        "!pip -q install alibi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import alibi\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "Li2-igrYrvlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from skimage.filters import sobel\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77S0pDg6wIxJ",
        "outputId": "965bcab3-2455-4d34-b006-68097e6a8c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the classifier Class.\n",
        "from torchvision.transforms.transforms import RandomRotation, RandomAdjustSharpness, RandomGrayscale\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, backbone='resnet', multi_backbone = False, device =\"cuda:0\",dropout_rate = 0.2, do_augmentation = False):\n",
        "    super().__init__()\n",
        "    self.multi_backbone = multi_backbone # Bool: Indicates if we use multibackbone\n",
        "\n",
        "    #In the following secttion we download the appropriatee prettrained model\n",
        "    if backbone == \"vgg19\":\n",
        "      backbone = torchvision.models.vgg19(pretrained=True)\n",
        "      self.out_channels = 25088\n",
        "      \n",
        "    elif backbone == \"resnet18\":\n",
        "      backbone = torchvision.models.resnet18(pretrained=True)\n",
        "      self.out_channels = 512\n",
        "\n",
        "    elif backbone == \"resnet50\":\n",
        "      backbone = torchvision.models.resnet50(pretrained=True)\n",
        "      self.out_channels = 2048\n",
        "\n",
        "    elif backbone == \"Efficientnet b1\":\n",
        "      backbone = torchvision.models.efficientnet_b1(pretrained=True)\n",
        "      self.out_channels = 1280\n",
        "\n",
        "    elif backbone == \"Efficientnet b3\":\n",
        "      backbone = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "      self.out_channels = 1536\n",
        "\n",
        "    elif backbone == \"Efficientnet b5\":\n",
        "      backbone = torchvision.models.efficientnet_b5(pretrained=True)\n",
        "      self.out_channels = 2048\n",
        "\n",
        "    elif backbone == \"Efficientnet b7\":\n",
        "      backbone = torchvision.models.efficientnet_b7(pretrained=True)\n",
        "      self.out_channels = 2560\n",
        "     \n",
        "    modules = list(backbone.children())[:-1]\n",
        "    self.do_augmentation = do_augmentation\n",
        "\n",
        "    if self.multi_backbone: #We create the backbones and put them on the device\n",
        "      self.backbone1 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone2 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone3 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "      self.backbone4 = nn.Sequential(*copy.deepcopy(modules)).to(device)\n",
        "\n",
        "    else:\n",
        "      self.backbone =  nn.Sequential(*modules).to(device)\n",
        "\n",
        "\n",
        "    #This is the final classification layer\n",
        "    self.fc = nn.Sequential(nn.Dropout(dropout_rate),\n",
        "                            nn.Linear(self.out_channels * 4, 3))                \n",
        "     \n",
        "  def forward(self, x, is_training = True):\n",
        "    # Transform image range from (0, 1) to (-1, 1)\n",
        "\n",
        "    imgs = [x[:,i] for i in range(4)] #list of 4 images\n",
        "    \n",
        "\n",
        "    if self.multi_backbone:\n",
        "      encodings = [self.backbone1(imgs[0]).flatten(1), \n",
        "                   self.backbone2(imgs[1]).flatten(1),\n",
        "                   self.backbone3(imgs[2]).flatten(1),\n",
        "                   self.backbone4(imgs[3]).flatten(1)]\n",
        "    else:\n",
        "      encodings = [self.backbone(img).flatten(1) for img in imgs]\n",
        "\n",
        "    return self.fc(torch.cat(encodings,1))"
      ],
      "metadata": {
        "id": "JAFBMgamwEIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd3ebab7r3Il",
        "outputId": "0d9a3bf1-4087-48ec-f485-de8fb0dcb19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the classifier using the following weights\n",
        "# If your directory structure does not resemble the following, you may need to \n",
        "model = torch.load(\"drive/MyDrive/Fish Attribution/\" + 'model1e-050.5.2022-05-11 13_52_21.pt', map_location=\"cpu\")\n",
        "model.eval()\n",
        "model.zero_grad()"
      ],
      "metadata": {
        "id": "UOIzZd3KwqTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(X: np.ndarray) -> np.ndarray:\n",
        "    X = torch.as_tensor(X, device='cpu')\n",
        "    return model.forward(X).detach().numpy()"
      ],
      "metadata": {
        "id": "FFlgq86cvtgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the raw fish images\n",
        "# WARNING: The following WILL use a large amount of RAM\n",
        "import numpy as np\n",
        "\n",
        "X_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_cropped_b.npy\"\n",
        "Y_PATH= \"/content/drive/Shareddrives/Exploding Gradients/y_b.npy\"\n",
        "\n",
        "x = np.load(X_PATH)\n",
        "y = np.load(Y_PATH)\n",
        "\n",
        "# 285 instances, currently the shapes are off, see next cell for fix\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQFxK0nYrs6Y",
        "outputId": "52a58023-474d-4b4b-e9fc-1f636203674d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(285, 4, 130, 750, 3)\n",
            "(285, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conver to PyTorch tensors from raw numpy arrays and\n",
        "# fix the dimensions\n",
        "tensor_x = torch.Tensor(x) \n",
        "tensor_y = torch.Tensor(y).long()\n",
        "\n",
        "tensor_x = torch.swapaxes(tensor_x,2,4)\n",
        "tensor_x = torch.swapaxes(tensor_x,3,4)"
      ],
      "metadata": {
        "id": "BQ73xhI6sUz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch dataloader to feed data to the model\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "attribution_ds = TensorDataset(tensor_x ,tensor_y) \n",
        "\n",
        "# Each time this is called, 40 pieces of data (40x4 = 160) are provided\n",
        "# This was figured out in a bit of trial-and-error fashion because\n",
        "# you have a limited about of onboard GPU RAM. 40 seems to be the maximum\n",
        "# it's happy with and any Colab Pro instance goes into the red at this point\n",
        "attribution_dl = DataLoader(attribution_ds,1,shuffle = True)\n",
        "\n",
        "# Delete the raw numpy arrays from RAM, there's no need for them\n",
        "# now that they exist as PyTorch tensors\n",
        "del x,y"
      ],
      "metadata": {
        "id": "kqaKvugYr_Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = iter(attribution_dl).next()"
      ],
      "metadata": {
        "id": "3R9hhFu1vSJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9PgaG4Nvayy",
        "outputId": "e4873054-73c9-44aa-c4f9-e73c01effec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   4,   3, 130, 750])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3apjCn0WZ214",
        "outputId": "5ebb7c17-14b8-4266-a340-3e10f7ecf011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19444455,  0.46261984, -0.749903  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.load(\"drive/MyDrive/Fish Attribution\" + \"/13x25 Mask.npy\")"
      ],
      "metadata": {
        "id": "Twg4CeSuZP4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segmenter(image):\n",
        "  return mask"
      ],
      "metadata": {
        "id": "jZvyyqKtbNB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from alibi.explainers import AnchorImage\n",
        "\n",
        "explainer = AnchorImage(predictor, np.array(image.squeeze().shape), segmentation_fn=segmenter,\n",
        "                        images_background=None)"
      ],
      "metadata": {
        "id": "Zj7BaNVJr1M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation = explainer.explain(image.squeeze().detach().numpy(), threshold=.95, p_sample=.8, seed=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7h9fvcBWxq61",
        "outputId": "7be87a1b-cbf7-4997-dcf7-5bc3e65b8c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-58d773f3a25f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_image.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, image, p_sample, threshold, delta, tau, batch_size, coverage_samples, beam_size, stop_on_first, max_anchor_size, min_samples_start, n_covered_ex, binary_cache_size, cache_margin, verbose, verbose_every, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mverbose_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         )  # type: Any\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_base.py\u001b[0m in \u001b[0;36manchor_beam\u001b[0;34m(self, delta, epsilon, desired_confidence, beam_size, epsilon_stop, min_samples_start, max_anchor_size, stop_on_first, batch_size, coverage_samples, verbose, verbose_every, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;31m# sample by default 1 or min_samples_start more random value(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# mean = fraction of labels sampled data that equals the label of the instance to be explained, ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_base.py\u001b[0m in \u001b[0;36mdraw_samples\u001b[0;34m(self, anchors, batch_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0msample_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: List, Tuple, Tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         samples_iter = [self.sample_fcn((i, tuple(self.state['t_order'][anchor])), num_samples=batch_size)\n\u001b[0;32m--> 356\u001b[0;31m                         for i, anchor in enumerate(anchors)]\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mcovered_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovered_false\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditionals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0msample_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: List, Tuple, Tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         samples_iter = [self.sample_fcn((i, tuple(self.state['t_order'][anchor])), num_samples=batch_size)\n\u001b[0;32m--> 356\u001b[0;31m                         for i, anchor in enumerate(anchors)]\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mcovered_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovered_false\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditionals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_image.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, anchor, num_samples, compute_labels)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturbation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mcovered_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_covered_ex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_image.py\u001b[0m in \u001b[0;36mperturbation\u001b[0;34m(self, anchor, num_samples)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 fudged_image[segments == x] = [\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 ]\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi/explainers/anchor_image.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 fudged_image[segments == x] = [\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 ]\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFWjXFImZ-Nf",
        "outputId": "154e9a6a-b43a-4758-93be-8d74636ac999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3, 130, 750)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "AaEOZ__sd3VN",
        "outputId": "68d6bf94-a6c9-4612-dccd-d35ebd3beca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 3, 130, 750])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BFR443nnd4id"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}