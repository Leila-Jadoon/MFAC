{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature Attribution Bokeh Generation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMlfgtb8Vm+qBI9btDOyACr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5e7ba8d0c9674db0bc2342ea2667bb66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e2f4673f6ff4328b0020ab3eb21de25","IPY_MODEL_0bdd8f66712c4626967f92ed92667778","IPY_MODEL_2626dcea0f3a476eb3e5e21616709816"],"layout":"IPY_MODEL_b12a36969c7d4a989b4e1f0fba541624"}},"5e2f4673f6ff4328b0020ab3eb21de25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56a0ad7889ca43d880438439f30f228d","placeholder":"​","style":"IPY_MODEL_c7504a92694f42d78650dc72f046ec71","value":"Epochs completed: 100%| "}},"0bdd8f66712c4626967f92ed92667778":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b664a87a2204650b93011b6a97c8cf5","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47b3b70f67a64a09ab3c3a456724c31f","value":500}},"2626dcea0f3a476eb3e5e21616709816":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_722058265e114047a26fd0ae0f056f8c","placeholder":"​","style":"IPY_MODEL_33dce12be567425d8abf45ad96d45bb4","value":" 500/500 [00:03]"}},"b12a36969c7d4a989b4e1f0fba541624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56a0ad7889ca43d880438439f30f228d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7504a92694f42d78650dc72f046ec71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b664a87a2204650b93011b6a97c8cf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47b3b70f67a64a09ab3c3a456724c31f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"722058265e114047a26fd0ae0f056f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33dce12be567425d8abf45ad96d45bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Feature Attribution Bokeh Generation\n","\n","This notebook is responsible for taking the attribution and image data and providing an interative UMAP plot based on the raw attribution data. The plot itself is generated via Bokeh and lives in an HTML file.\n","\n","NOTE: This notebook only works with attributions that were done AGAINST A SINGLE CLASS. It will NOT WORK for multi-class attributions that are designed for generating confusion matrices.\n","\n","## Inputs\n","\n","The settings that control the notebook are determined by a set of variables (all expressed as capital letters, with underscoring used for spaces ex: `ORIGINAL_IMAGES_PATH`). The values of these variables can be changed prior to execution of the notebook.\n","\n","### Required Arguments\n","The notebook requires the following information to be provided. \n","\n","### File Paths\n","\n","* `TRUE_LABELS_PATH` (string) - Path to the true labels for the dataset, these should be stored in `*.npy` format and have the shape `[1,x]` where x is the number of instances of data\n","* `PREDICTED_LABELS_PATH` (string) - Path to the predicted labels generated from the \"Classifier Labels Generation\" notebook for the dataset, these should be stored in `*.npy` format and have the shape `[1,x]` where x is the number of instances of data\n","* `ATTRIBUTION_DATA_PATH` (string) - Path to the attribution data generated from the \"Feature Attribution Data Generation\" Notebook, this should be a \".pt\" or saved PyTorch Tensor file.\n","* `PLOT_PATH` (string) - Path to where you want the plot to be saved.\n","\n","### Optional Arguments\n","\n","The notebook provides default values for these values but they can be changed to new values.\n","\n","\n","#### Image Processing Options\n","\n","The notebook has default options for these but they can be tweaked for custom results. Note that the following variables are passed DIRECTLY into a call to the `visualize_image_attr` function that Captum provides, meaning it should align with the information found in the Captum documentation: https://captum.ai/api/utilities.html#visualization. The most relevant parts have been summarized/taken straight from the documentation and provided below.\n","\n","* `METHOD` (string) (default value: \"blended_heat_map\") - the method for visualization attribution. They are:\n","  * \"heat_map\" - display a heatmap of attributions\n","  * \"blended_heat_map\" - put the heatmap over a greyscale version of the image\n","  * \"original_image\" - Just show the original image\n","  * \"masked_image\" - mask image (pixel-wise multiply) by normalized attribution values\n","ng\n","  * \"alpha_scaling\" - set the alpha channel of each pixel to normalized attribution value\n","* `SIGN` (string) (default value: \"all\") - Determines which attribution values to show. The options for this method are:\n","  * \"positive\" - only display positive attributions\n","  * \"absolute_value\" - display the absolute value of all attributions\n","  * \"negative\" - only display negative attributions\n","  * \"all\" - display both positive and negative attributions. Note that if you set `METHOD` to \"masked_image\" or \"alpha_scaling\" the \"all\" option is NOT supported.\n","* `ALPHA_OVERLAY` (float between 0 and 1) (default value: 0.8) - controls the \"brightness\" or rather how prominently the zebrafish appears in the background. Higher Alpha values correspond to fainter background images.\n","* `SHOW_COLORBAR` (boolean)  (default value: True)- Determines if a colorbar is added that shows a mapping between the color on the image (red/green) and its associated attribution value.\n","\n","#### UMAP options\n","\n","These are passed to UMAP upon instantiation of a UMAP object and control UMAP's behavior. The following is summarized/taken from https://umap-learn.readthedocs.io/en/latest/api.html:\n","\n","* `N_NEIGHBOURS` (int) (default value: 20) - number of neighbouring sample points \n","* `MIN_DIST` (float) (default value: 0.1) - the effective minimum distance between embedded points\n","* `VERBOSE` (Boolean) (default value: True) - allows you to enable/disable verbose output from UMAP when it's constructing the data\n","\n","#### Plot Options\n","\n","These options target the Bokeh Plot.\n","\n","* `PLOT_TITLE` (string) (defalut value: \"Deconvolution against Predicted Labels\") - The title the UMAP plot will have\n","* `PLOT_SUB_IMAGE_WIDTH_PX` (string) (default value: \"256px\") - the width of each fish image upon being displayed when the user hovers on a point.\n","* `PLOT_SUB_IMAGE_HEIGHT_PX` (string) (default value: \"90px\") - the height of each fish image upon being displayed when the user hovers on a point.\n","\n","## Outputs\n","\n","* An HTML file at the path specified by `PLOT_PATH` that can be opened in any browser, containing an interactive UMAP plot of the feature attribution data."],"metadata":{"id":"uiHAqXl6ZiBr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrmNget6gD8r","executionInfo":{"status":"ok","timestamp":1653779267772,"user_tz":420,"elapsed":28173,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a6f5d39b-d716-41fd-9a89-28f3e31d8158"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.4 MB 11.8 MB/s \n","\u001b[K     |████████████████████████████████| 88 kB 5.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 33.7 MB/s \n","\u001b[?25h  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Import dependencies\n","import torch \n","\n","from io import BytesIO\n","import base64\n","from PIL import Image\n","\n","from bokeh import plotting, palettes\n","from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","# Install Captum\n","!pip install -q captum\n","# We just need the visualization library, the actual \n","# attribution itself is done in a separate notebook\n","# See: \"Feature Attribution Data Generation\"\n","from captum.attr import visualization as viz\n","\n","# Install and import UMAP\n","!pip install -q umap-learn\n","import umap\n"]},{"cell_type":"code","source":["# Control Variables\n","\n","### File Paths\n","#### Path to the original dataset, should be saved as .npy file\n","ORIGINAL_IMAGES_PATH = \"/content/drive/Shareddrives/Exploding Gradients/X_cropped_b.npy\"\n","#### Path to the true labels of the dataset, should be save as .npy file\n","TRUE_LABELS_PATH = \"/content/drive/Shareddrives/Exploding Gradients/y_b.npy\"\n","#### Path to the classifier generated labels, should be saved as .npy file (use \"Labels Generation\" notebook if you don't have this file!)\n","PREDICTED_LABELS_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/predicted_labels.npy\"\n","#### Path to the attribution data\n","ATTRIBUTION_DATA_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/Deconvolution Single Class.pt\"\n","#### Path to where the plot should be stored\n","PLOT_PATH = \"/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/deconvolution.html\"\n","\n","## Image Processing Options\n","### Method the attributions should be visualized as\n","METHOD = \"blended_heat_map\"\n","### The signs of the attribution to visualize\n","SIGN = \"all\"\n","### Controls how prominently the background image shows, \n","### 1 means the background has full prominence/brightness while 0 means\n","### the background is not visible at all\n","ALPHA_OVERLAY = 0.8\n","### Decides whether or not to include the colorbar for each image, \n","### indicating the color associated with each attribution value on the\n","### image.\n","SHOW_COLORBAR = True\n","\n","## UMAP options\n","### Number of neighbours used for evaluation\n","N_NEIGHBOURS = 20\n","### Minimum distance between points, should range from 0 to 1 as a float\n","MIN_DIST = 0.5\n","### Decides if you want to display the progress as UMAP is crunching numbers\n","VERBOSE = True\n","\n","## Plot Options\n","### Title of the Plot\n","PLOT_TITLE = \"Deconvolution against Predicted Labels\"\n","### Image Width in pixels, expressed as a string (this is injected into the\n","### final HTML that bokeh uses)\n","PLOT_SUB_IMAGE_WIDTH_PX = \"256px\"\n","### Image Height expressed in pixels, \n","### expressed as a string (this is injected into the\n","### final HTML that bokeh uses\n","PLOT_SUB_IMAGE_HEIGHT_PX = \"90px\""],"metadata":{"id":"scCun_lrCjH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load all necessary data\n","\n","import torch\n","\n","## Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","## Load Images along with True and Predicted labels\n","images = np.load(ORIGINAL_IMAGES_PATH)\n","y_true = np.load(TRUE_LABELS_PATH)\n","y_predicted = np.load(PREDICTED_LABELS_PATH)\n","\n","## Convert the labels from numpy arrays to tensors\n","y_true_tensor = torch.Tensor(y_true).long()\n","y_predicted_tensor = torch.Tensor(y_predicted).long()\n","\n","## Reshape tensors to proper dimensions\n","images_tensor = torch.Tensor(images) \n","images_tensor = torch.swapaxes(images_tensor,2,4)\n","images_tensor = torch.swapaxes(images_tensor,3,4)\n","\n","# Get the attribution data\n","attributions_tensor = torch.load(ATTRIBUTION_DATA_PATH, map_location=torch.device('cpu'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2-txkQtggjJ","executionInfo":{"status":"ok","timestamp":1653779421315,"user_tz":420,"elapsed":153564,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"a50cdbeb-aeea-4688-c8d2-3fe494c0b790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Once the data has been extracted from the numpy arrays and converted to\n","# tensor form, just delete the original arrays (saves RAM)\n","del images, y_true, y_predicted "],"metadata":{"id":"H-aT2HOcU66f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## This is a heavily modified helper function (originally taken from this source:\n","# https://tonio73.github.io/data-science/cnn/CnnVsDense-Part2-Visualization.html\n","# which also provided the code below that has been slightly modified, and\n","# https://zapaishchykova.medium.com/the-meaningful-visualization-of-clusters-2a666be0f460\n","\n","# This function is applied to convert each cluster of images and attributions to\n","# the format that Bokeh can use to view the images\n","def embeddableImage(image_cluster, attribution_cluster):\n","    # an \"image_cluster\" should have the following dimensions: [4, 3, 130, 370]\n","    # as should the \"attribution_cluster\" (equivalent to one piece of data)\n","    encoded_subimages = []\n","    # For each image and attribution associated with the image, pass the data\n","    # to Captum visualization to get th eimage\n","    for sub_image, sub_attribution in zip(image_cluster, attribution_cluster):\n","      fig, _ = viz.visualize_image_attr(sub_attribution.transpose(1,2,0), \n","                                                  sub_image.transpose(1,2,0), \n","                                                  method=METHOD, \n","                                                  sign=SIGN,\n","                                                  show_colorbar=SHOW_COLORBAR,\n","                                                  use_pyplot=False);\n","      # encode the data into a format that Bokeh understands by saving the\n","      # image to a buffer instead of a file\n","      buffer = BytesIO()\n","      fig.savefig(buffer, format='png', bbox_inches='tight')\n","      encoded_subimages.append('data:image/png;base64,' + base64.b64encode(buffer.getvalue()).decode())\n","    return encoded_subimages"],"metadata":{"id":"Hd7o9Y9_gRkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# umapPlot is solely responsbile for generating the plot itself \n","# It accepts the following arguments:\n","## embedding - refers to the data produced by applying UMAP, should be an array\n","## x - the raw images used\n","## y_true - the true labels\n","## y_predicted - predicted labels \n","## attributions - the attribution data\n","## title - a string that indicates what the plot title is\n","## x and attributions MUST have the same dimensions\n","def umapPlot(embedding, x, y_true, y_predicted, attributions, title=''):\n","    \"\"\" Plot the embedding of X and y with popovers using Bokeh \"\"\"\n","    \n","    # Create a two-column dataframe from the embedding data\n","    df = pd.DataFrame(embedding, columns=('x', 'y'))\n","    # for each image should be able to apply the embeddable image function\n","    # list of lists [rows x columns], x instances with 4 columns\n","    sub_images = np.array(list(map(embeddableImage, x, attributions)))\n","\n","    # Start storing the subimages into the dataframe with their own index\n","    for i in range(4):\n","      df['image'+str(i+1)] = sub_images[:,i]\n","    # Convert the predicted and true labels to strings\n","    df['true_class'] = [str(label) for label in y_true]\n","    df['predicted_class'] = [str(label) for label in y_predicted]\n","    # Create the indices for the data so a user can index back and obtain them\n","    df['index'] = list(range(len(y_true)))\n","\n","    datasource = ColumnDataSource(df)\n","\n","    colorMapping = CategoricalColorMapper(factors=np.arange(10).astype(np.str), palette=palettes.Spectral10)\n","\n","    plotFigure = plotting.figure(\n","        title=title,\n","        plot_width=600,\n","        plot_height=600,\n","        tools=('pan, wheel_zoom, reset')\n","    )\n","\n","    # Whenever the user hovers on a point with their mouse, show \n","    # the 4 images of the fish in the data instance simultaneously, along with\n","    # The True and Predicted class, as well as the data index number\n","    tooltip = \"\"\"\n","        <div>\n","            <div>\n","                <img src='@image1' style='float: left; width:{WIDTH}; height:{HEIGHT}; margin: 5px 5px 5px 5px'/>\n","            </div>\n","            <div>\n","                <img src='@image2' style='float: left; width:{WIDTH}; height:{HEIGHT}; margin: 5px 5px 5px 5px'/>\n","            </div>\n","            <div>\n","                <img src='@image3' style='float: left; width:{WIDTH}; height:{HEIGHT}; margin: 5px 5px 5px 5px'/>\n","            </div>\n","            <div>\n","                <img src='@image4' style='float: left; width:{WIDTH}; height:{HEIGHT}; margin: 5px 5px 5px 5px'/>\n","            </div>\n","            <div>\n","                <span style='font-size: 16px; color: #224499'>True Class:</span>\n","                <span style='font-size: 18px'>@true_class</span>\n","            </div>\n","            <div>\n","                <span style='font-size: 16px; color: #224499'>Predicted Class:</span>\n","                <span style='font-size: 18px'>@predicted_class</span>\n","            </div>\n","            <div>\n","                <span style='font-size: 16px; color: #224499'>Index:</span>\n","                <span style='font-size: 18px'>@index</span>\n","            </div>\n","        </div>\n","        \"\"\".format(WIDTH=PLOT_SUB_IMAGE_WIDTH_PX,\n","                   HEIGHT=PLOT_SUB_IMAGE_HEIGHT_PX)\n","        # Inject the proper Pixel Widht and Height values specified by the suer\n","    plotFigure.add_tools(HoverTool(tooltips=tooltip))\n","\n","    # The actual coordinates for the circles/points on the plot are taken\n","    # from the embedding that UMAP generates (see how the embedding variable)\n","    # is converted into a panda's dataframe earlier\n","    plotFigure.circle(\n","        'x', 'y',\n","        source=datasource,\n","        # The color of each point should be by whatever the PREDICTED label\n","        # is\n","        color=dict(field='predicted_class', transform=colorMapping),\n","        line_alpha=0.6, fill_alpha=0.6, size=8\n","    )\n","    \n","    # The original code would force the plot to be shown at this point,\n","    # this should be AVOIDED. Any attempt at rendering the plot \n","    # in Colab will CAUSE IT TO CRASH\n","    return plotFigure\n"],"metadata":{"id":"MbrehztwgeXm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the reducer is an instance of a UMAP object that will do the heavy lifting,\n","# giving us the x,y coordinates of interest from the original data\n","reducerFish = umap.UMAP(n_neighbors = 20,\n","                        min_dist=0.5,\n","                        verbose = True)\n","# There are 285 pieces of data, reshape the entire dataset such that it's just a series of flat tensors\n","# (4x3x130x750 = 1170000). Otherwise UMAP won't accept the data.\n","embeddingFish = reducerFish.fit_transform(torch.reshape(attributions_tensor, (285, 1170000)).detach())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210,"referenced_widgets":["5e7ba8d0c9674db0bc2342ea2667bb66","5e2f4673f6ff4328b0020ab3eb21de25","0bdd8f66712c4626967f92ed92667778","2626dcea0f3a476eb3e5e21616709816","b12a36969c7d4a989b4e1f0fba541624","56a0ad7889ca43d880438439f30f228d","c7504a92694f42d78650dc72f046ec71","7b664a87a2204650b93011b6a97c8cf5","47b3b70f67a64a09ab3c3a456724c31f","722058265e114047a26fd0ae0f056f8c","33dce12be567425d8abf45ad96d45bb4"]},"id":"wqli1fkNjRZb","executionInfo":{"status":"ok","timestamp":1653779448740,"user_tz":420,"elapsed":27440,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"468c3f92-de2f-4209-a05c-75b320b2f944"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["UMAP(min_dist=0.5, n_neighbors=20, verbose=True)\n","Sat May 28 23:10:20 2022 Construct fuzzy simplicial set\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n","  warnings.warn(problem)\n"]},{"output_type":"stream","name":"stdout","text":["Sat May 28 23:10:37 2022 Finding Nearest Neighbors\n","Sat May 28 23:10:40 2022 Finished Nearest Neighbor Search\n","Sat May 28 23:10:42 2022 Construct embedding\n"]},{"output_type":"display_data","data":{"text/plain":["Epochs completed:   0%|            0/500 [00:00]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7ba8d0c9674db0bc2342ea2667bb66"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Sat May 28 23:10:46 2022 Finished embedding\n"]}]},{"cell_type":"code","source":["# Feed all relevant data to create the UMAP plot\n","fig = umapPlot(embeddingFish, \n","               images_tensor.numpy(), \n","               y_true_tensor.squeeze().numpy(),\n","               y_predicted_tensor.squeeze().numpy(),\n","               attributions_tensor.detach().numpy(),\n","               title=PLOT_TITLE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqZ_YM2sj1le","executionInfo":{"status":"ok","timestamp":1653779750233,"user_tz":420,"elapsed":301506,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"a6709090-fba0-4551-8b72-4a9f7d49af21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["285\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]}]},{"cell_type":"code","source":["from bokeh.plotting import output_file, save"],"metadata":{"id":"59BQmDlFQQP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_file(PLOT_PATH)\n","save(fig)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":57},"id":"sUNL58_dZtLL","executionInfo":{"status":"ok","timestamp":1653779751236,"user_tz":420,"elapsed":1020,"user":{"displayName":"John Long","userId":"00584768646093006387"}},"outputId":"36a05e58-c141-4207-8284-ef510e40cb4c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Fish Attribution/model1e-050.5.2022-05-22 12:13:10.pt Work/deconvolution.html'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]}]}